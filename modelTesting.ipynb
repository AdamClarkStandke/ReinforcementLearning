{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aCStandke/ReinforcementLearning/blob/main/modelTesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R2FNp7qQfzB3"
      },
      "outputs": [],
      "source": [
        "# ignore warning messages because they are annoying lol\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!pip install -q stable-baselines3[extra] # reinforcement learning library\n",
        "!pip install -q optuna; # hyperparameter library\n",
        "!pip install -q --upgrade importlib-metadata==4.13.0 # for backwards compatibility issue\n",
        "!pip install -q empyrical # financial calculation library\n",
        "!pip install -q yfinance # yahoo finance for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qyt-HqL-f2iV"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import gym \n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import json\n",
        "import datetime as dt\n",
        "import optuna\n",
        "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.utils import constant_fn\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.env_util import DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_checker import VecCheckNan, check_env\n",
        "from stable_baselines3.common.callbacks import BaseCallback, CallbackList, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.logger import configure\n",
        "from stable_baselines3.common.callbacks import ProgressBarCallback\n",
        "from empyrical import sortino_ratio, omega_ratio\n",
        "import sqlite3\n",
        "from sqlite3 import Error\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "import collections\n",
        "import datetime\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import os\n",
        "import csv\n",
        "from csv import DictWriter\n",
        "from matplotlib import style\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8LepWiJiJqB"
      },
      "outputs": [],
      "source": [
        "'getting training and test data'\n",
        "T = 'IYT' # ticker for agent to trade in\n",
        "train_start = \"2004-01-01\" \n",
        "train_end = \"2017-12-12\"   \n",
        "test_start = \"2018-01-01\"\n",
        "test_end = \"2019-01-01\"\n",
        "train_data = yf.download(tickers=T, start=train_start, end=train_end)\n",
        "test_data =  yf.download(tickers=T, start=test_start, end=test_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rulHF3qy1XJE"
      },
      "outputs": [],
      "source": [
        "print(f'TRAINING DATA SIZE: {train_data.shape}')\n",
        "print(f'TEST DATA SIZE: {test_data.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dJiMYPDIAj3y"
      },
      "outputs": [],
      "source": [
        "'Environment Parmaters'\n",
        "# stock environment parameters\n",
        "MAX_ACCOUNT_BALANCE = 2147483647 \n",
        "MAX_NUM_SHARES = 2147483647\n",
        "MAX_SHARE_PRICE = 4294967295\n",
        "LOOKBACK_WINDOW_SIZE = 30 # trading window, default is 30 frames \n",
        "MAX_STEPS = 1e6 # num of iterations, default is 1 mil\n",
        "INITIAL_ACCOUNT_BALANCE = 10000 # starting balance and networth\n",
        "# default percentage of stock price trading agent pays broker when \n",
        "# buying/selling, default is 1% (i.e. not very reasonable)\n",
        "DA_COMMISION = 0.01\n",
        "# One month t-bill risk free rate, defualt is 1% (i.e. not cool)\n",
        "R_FREE_RATE = 0.01 \n",
        "# validation frequency, defualt is 1000\n",
        "FEQ = 1000 \n",
        "# the number of concurrent environments for training  \n",
        "ENV = 3\n",
        "MODEL = T\n",
        "\n",
        "# Trading Enviornment\n",
        "class StockTradingEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, data, title, reward_func='ERR', r=False, random=True, volumes=True):\n",
        "        super(StockTradingEnv, self).__init__()\n",
        "        self.data = data\n",
        "        self.random_ofs_on_reset = random\n",
        "        self.reward_func = reward_func\n",
        "        self.bars_count = LOOKBACK_WINDOW_SIZE\n",
        "        self.commission = DA_COMMISION\n",
        "        self.risk_free =  R_FREE_RATE\n",
        "        self.title = title\n",
        "        self.volumes = volumes\n",
        "        self._render_ja = r\n",
        "        \n",
        "        # Actions of the format Buy x%, Sell x%, Hold x%\n",
        "        # the action space is 3 x 10 = 30 that agent can execute\n",
        "        self.action_space = spaces.MultiDiscrete([3, 10])\n",
        "\n",
        "        # Prices contains the OHCL values for the last five prices the \n",
        "        # observation space is 6 x 10 = 60 that agent can observe\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=self.shape, dtype=np.float32)\n",
        "        \n",
        "        # log transaction cost calc for excess return reward\n",
        "        self.trans = np.log((1-self.commission)/(1+self.commission))\n",
        "\n",
        "        # setting random seed\n",
        "        self.seed()\n",
        "\n",
        "        # creating header for text output\n",
        "        if self._render_ja == True:\n",
        "          header = \"Time, Networth, Balence, SharesHeld\"\n",
        "          with open(self.title, 'a') as f:\n",
        "            f.write(header + \"\\n\")\n",
        "            f.close\n",
        "\n",
        "    def reset(self):\n",
        "      # random offset portion \n",
        "      bars = self.bars_count\n",
        "      if self.random_ofs_on_reset:\n",
        "        offset = self.np_random.choice(self.data.high.shape[0]-bars*10)+bars\n",
        "      else:\n",
        "        offset = bars\n",
        "      self._reset(offset)\n",
        "      return self._next_observation()\n",
        "\n",
        "    def _reset(self, offset):\n",
        "      self.trades = []\n",
        "      self.aav_buy = []\n",
        "      self.aav_sell = []\n",
        "      self.aav_total = []\n",
        "      self.roi_buy = []\n",
        "      self.roi_sell = []\n",
        "      self.r_t = []\n",
        "      self.r_f = []\n",
        "      self.balance = INITIAL_ACCOUNT_BALANCE\n",
        "      self.netWorth = INITIAL_ACCOUNT_BALANCE\n",
        "      self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
        "      self.shares_held  = 0\n",
        "      self.N = 0\n",
        "      self._offset = offset\n",
        "      # setting account history portion\n",
        "      self.account_history = np.repeat([[self.netWorth],\n",
        "                                        [self.balance],\n",
        "                                        [self.shares_held]], self.bars_count, axis=1)\n",
        "\n",
        "    # shape of observation space is 1D\n",
        "    @property\n",
        "    def shape(self):\n",
        "      if self.volumes:\n",
        "        return (8*(3*self.bars_count),)\n",
        "      else:\n",
        "        return (7*(3*self.bars_count),)\n",
        "\n",
        "    def _next_observation(self):\n",
        "      res = np.zeros(shape=self.shape, dtype=np.float32)\n",
        "      shift = 0\n",
        "      for bar_idx in range(-self.bars_count+1, 1):\n",
        "        res[shift] = self.data.open[self._offset + bar_idx]\n",
        "        shift += 1\n",
        "        res[shift] = self.data.high[self._offset + bar_idx]\n",
        "        shift += 1\n",
        "        res[shift] = self.data.low[self._offset + bar_idx]\n",
        "        shift += 1\n",
        "        res[shift] = self.data.close[self._offset + bar_idx]\n",
        "        shift += 1\n",
        "        if self.volumes:\n",
        "          res[shift] = self._prices.volume[self._offset + bar_idx]\n",
        "          shift += 1\n",
        "      for bar_idx in range(-self.bars_count, 0):\n",
        "        res[shift]=self.account_history[0][bar_idx]\n",
        "        shift+=1\n",
        "      for bar_idx in range(-self.bars_count, 0):\n",
        "        res[shift]=self.account_history[1][bar_idx]\n",
        "        shift+=1\n",
        "      for bar_idx in range(-self.bars_count, 0):\n",
        "        res[shift]=self.account_history[2][bar_idx]\n",
        "        shift+=1\n",
        "      return res\n",
        "\n",
        "       \n",
        "    def _take_action(self, action):\n",
        "      reward = 0\n",
        "      current_price = self._cur_close()\n",
        "      action_type = action[0]\n",
        "      amount = action[1]/10\n",
        "\n",
        "      shares_bought = 0\n",
        "      shares_sold = 0\n",
        "      additional_cost = 0\n",
        "      sales = 0\n",
        "\n",
        "      if action_type < 1 and int(self.balance) > 0 and action[1] > 0:\n",
        "        # Buy amount % of balance in shares\n",
        "        total_possible = self.balance / (current_price * (1+self.commission))\n",
        "        shares_bought = total_possible * amount\n",
        "        additional_cost = shares_bought * current_price * (1+self.commission)\n",
        "        self.balance -= additional_cost\n",
        "        self.shares_held += shares_bought\n",
        "        self.N += 1\n",
        "        # trading history\n",
        "        if shares_bought > 0:\n",
        "          self.trades.append({'step': self._offset, 'shares': shares_bought, \n",
        "                              'total': additional_cost, 'type': \"buy\"})\n",
        "      if action_type < 2 and int(self.shares_held) > 0 and action[1] > 0:\n",
        "        # Sell amount % of shares held\n",
        "        shares_sold = self.shares_held * amount  \n",
        "        sales = shares_sold * current_price * (1 - self.commission)\n",
        "        self.balance += sales\n",
        "        self.shares_held -= shares_sold\n",
        "        self.N += 1\n",
        "        # trading history\n",
        "        if shares_sold > 0:\n",
        "          self.trades.append({'step': self._offset, 'shares': -shares_sold, \n",
        "                              'total': shares_sold * current_price, 'type': \"sell\"})\n",
        "\n",
        "      \n",
        "      self.netWorth = self.balance + self.shares_held * current_price\n",
        "        \n",
        "\n",
        "      # updating account history\n",
        "      self.account_history = np.append(self.account_history, [[self.netWorth],\n",
        "                                                              [self.balance],\n",
        "                                                              [self.shares_held]],axis=1)\n",
        "      \n",
        "      # reward Calculations based off networth history\n",
        "      returns = self.account_history[0][-self.bars_count:]\n",
        "      # calcualtion for ratio based rewards\n",
        "      r = np.diff(returns)\n",
        "      # sortinoRewardRatio\n",
        "      if self.reward_func == 'sortinoRewardRatio':\n",
        "        ratio = sortino_ratio(r, period=\"daily\") # default period daily\n",
        "        reward= ratio \n",
        "        if self.netWorth > self.max_net_worth:\n",
        "          self.max_net_worth = self.netWorth\n",
        "          reward *= 100\n",
        "      # Omega Ratio\n",
        "      elif self.reward_func == 'omegaRewardRatio':\n",
        "        ratio = omega_ratio(r) \n",
        "        reward= ratio\n",
        "        if self.netWorth > self.max_net_worth:\n",
        "          self.max_net_worth = self.netWorth\n",
        "          reward *= 100\n",
        "      # Excess Return reward\n",
        "      elif self.reward_func == 'ERR':\n",
        "        current_price = self._cur_close()\n",
        "        last_price = self._past_close()\n",
        "        com_return = None\n",
        "        buy_hold_return = None\n",
        "        if action_type == 0:\n",
        "          self.r_t.append((np.log(current_price)-np.log(last_price)))\n",
        "        if action_type == 1:\n",
        "          self.r_f.append(((1+self.risk_free)*1))\n",
        "        if action_type == 2:\n",
        "          reward = np.sum(self.r_t)+self.trans\n",
        "        if action_type == 0 or action_type == 1:\n",
        "          reward = ((np.sum(self.r_t)+np.sum(self.r_f)+self.N*self.trans)-(np.sum(self.r_t)+self.trans))\n",
        "        if self.netWorth > self.max_net_worth:\n",
        "          self.max_net_worth = self.netWorth\n",
        "          reward *= 100\n",
        "        reward = np.float32(reward)\n",
        "      return reward if abs(reward) != np.inf and not np.isnan(reward) else 0\n",
        "\n",
        "      \n",
        "    def _cur_close(self):\n",
        "      \"\"\"\n",
        "      Calculate real close price for the current bar\n",
        "      \"\"\"\n",
        "      return self.data.real_close[self._offset]\n",
        "\n",
        "    def _past_close(self):\n",
        "      \"\"\"\n",
        "      Calculate the past one day close  \n",
        "      \"\"\"\n",
        "      return self.data.real_close[(self._offset-1)] \n",
        "\n",
        "    def step(self, action):\n",
        "      # Execute one time step within the environment\n",
        "      reward = self._take_action(action)\n",
        "      self._offset += 1\n",
        "      if self._offset >= self.data.close.shape[0]-1 or int(self.netWorth)<1 or int(self.netWorth)>=MAX_ACCOUNT_BALANCE:\n",
        "        done=True\n",
        "      else:\n",
        "        done=False\n",
        "      obs = self._next_observation()\n",
        "      info = {\"Net Worth\":self.netWorth, \"reward\": reward, 'Balance': self.balance}\n",
        "      return obs, reward, done, info\n",
        "\n",
        "    def _render_to_file(self):\n",
        "      with open(self.title, 'a') as f:\n",
        "        f.write(f'{self.data.date[self._offset]},{self.netWorth},{self.balance},{self.shares_held}\\n')\n",
        "        f.close()\n",
        " \n",
        "    def render(self, mode='file', title=\"Agent's Trading Screen\", **kwargs):\n",
        "      # Render the environment to the screen\n",
        "      if mode == 'file':\n",
        "        if self._render_ja == True:\n",
        "          self._render_to_file()\n",
        "\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "      self.np_random, seed1 = seeding.np_random(seed)\n",
        "      seed2 = seeding.hash_seed(seed1+1) % 2**33\n",
        "      return [seed1, seed2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "szUR1sYHHEVl"
      },
      "outputs": [],
      "source": [
        "# create a log-differenced series (i.e. lag one) to create stationary input\n",
        "# as detailed by Jason in his post \n",
        "# https://machinelearningmastery.com/remove-trends-seasonality-difference-transform-python/\n",
        "\n",
        "def difference(dataset, interval=1):\n",
        "\tdiff = list()\n",
        "\tfor i in range(interval, len(dataset)):\n",
        "\t\tvalue = np.log(dataset[i]) - np.log(dataset[i - interval])\n",
        "\t\tdiff.append(value)\n",
        "\treturn diff\n",
        "\n",
        "\n",
        "# training data\n",
        "train_data['Date']=train_data.index\n",
        "data=train_data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "# OHLCV data stationary  \n",
        "o = np.array(difference(data['Open'], 1))\n",
        "rh = np.array(difference(data['High'], 1))\n",
        "rl = np.array(difference(data['Low'], 1))\n",
        "rc = np.array(difference(data['Close'], 1))\n",
        "vol = np.array(difference(data['Volume'], 1))\n",
        "\n",
        "# # OHLCV data non-stationary  \n",
        "# o = data['Open']\n",
        "# rh = data['High']\n",
        "# rl = data['Low']\n",
        "# rc = data['Close']\n",
        "# vol = data['Volume'].values\n",
        "\n",
        "# year data of year-month-day form\n",
        "dt = data['Date'].array\n",
        "\n",
        "\n",
        "Train_Data = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume', 'real_open',  'real_close', 'real_high', 'real_low', 'real_vol'])\n",
        "train = Train_Data(date=dt,high=rh, low=rl, close=rc, open=o, volume=vol, real_open=data['Open'].values, real_close=data['Close'].values, real_high=data['High'].values, real_low=data['Low'].values, real_vol=data['Volume'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1TdubrWOfamD"
      },
      "outputs": [],
      "source": [
        "# Testing data\n",
        "test_data['Date']=test_data.index\n",
        "data_two=test_data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "# OHLCV data stationary  \n",
        "diff_o = np.array(difference(data_two['Open'], 1))\n",
        "diff_h = np.array(difference(data_two['High'], 1))\n",
        "diff_l = np.array(difference(data_two['Low'], 1))\n",
        "diff_c = np.array(difference(data_two['Close'], 1))\n",
        "vols   = np.array(difference(data_two['Volume'], 1))\n",
        "\n",
        "# # OHLCV data non-stationary  \n",
        "# diff_o = data_two['Open']\n",
        "# diff_h = data_two['High']\n",
        "# diff_l = data_two['Low']\n",
        "# diff_c = data_two['Close']\n",
        "# vols = data_two['Volume'].values\n",
        "\n",
        "# year data of year-month-day form\n",
        "date = data_two['Date'].array\n",
        "\n",
        "Test_Data = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume', 'real_open', 'real_close', 'real_high', 'real_low', 'real_vol'])\n",
        "test = Test_Data(date=date,high=diff_h, low=diff_l, close=diff_c, open=diff_o, volume=vols, real_open=data_two['Open'].values, real_close=data_two['Close'].values, real_high=data_two['High'].values, real_low=data_two['Low'].values, real_vol=data_two['Volume'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1hEB-iRM4GLz"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(model,env,n_eval_episodes=1,deterministic=False):\n",
        "    episode_rewards, networths, episode_balances = [], [], []\n",
        "    for i in range(n_eval_episodes):\n",
        "        done, state = False, None\n",
        "        episode_reward = 0.0\n",
        "        episode_length = 0\n",
        "        episode_balance = 0\n",
        "        networth = 0\n",
        "        obs = env.reset()\n",
        "        while not done:\n",
        "            action, state = model.predict(obs, state=state, deterministic=deterministic)\n",
        "            new_obs, reward, done, _info = env.step(action)\n",
        "            obs = new_obs\n",
        "            episode_reward += reward\n",
        "            episode_balance += _info[0]['Balance']\n",
        "            networth += _info[0]['Net Worth']\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_balances.append(episode_balance)\n",
        "        networths.append(networth)\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    mean_networth = np.mean(networths)\n",
        "    mean_balance = np.mean(episode_balances)\n",
        "    return mean_reward, _info[0]['Net Worth'], _info[0]['Balance']\n",
        "\n",
        "\n",
        "class TensorboardCallback(BaseCallback):\n",
        "    def __init__(self, env, eval,  verbose=1):\n",
        "        super(TensorboardCallback, self).__init__(verbose)\n",
        "        self.eval= eval\n",
        "        self.env = env\n",
        "\n",
        "    def _on_step(self):\n",
        "      if (self.num_timesteps % self.eval == 0):\n",
        "        m, net, bal = evaluate_policy(self.model, self.env)\n",
        "        self.logger.record('Last Networth Value', net)\n",
        "        self.logger.record('Last Balance Value', bal)\n",
        "      return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAJYlkEqIwcK"
      },
      "outputs": [],
      "source": [
        "'Rewards'\n",
        "# reward = 'sortinoRewardRatio'\n",
        "# reward = 'omegaRewardRatio'\n",
        "reward = 'ERR' # default reward function\n",
        "\n",
        "# create evaluation env \n",
        "eval_env = DummyVecEnv([lambda: StockTradingEnv(test, f'{MODEL}.txt',reward_func=reward,random=False, volumes=False)])\n",
        "# create training envs (can be 1 or determined by ENV variable, defualt is 3)\n",
        "envs =  DummyVecEnv([lambda: StockTradingEnv(train, f'{MODEL}.txt', reward_func=reward,random=True, volumes=False) for _ in range(0,ENV)])\n",
        "\n",
        "# callback list for tracking purposes \n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=f'{MODEL}',\n",
        "                             log_path=f'{MODEL}_log', eval_freq=FEQ,\n",
        "                             deterministic=False, render=False, n_eval_episodes=1)\n",
        "callback = CallbackList([TensorboardCallback(eval_env, FEQ), eval_callback])\n",
        "\n",
        "# optional additional keyword parameters to pass to PPO agent \n",
        "policy_kwargs = dict(net_arch=[128, dict(vf=[64, 32], pi=[64, 32])])\n",
        "# Initializing PPO agent\n",
        "model = PPO('MlpPolicy', envs,  verbose=0, tensorboard_log=f\"/content/tensorboard\", policy_kwargs=policy_kwargs)\n",
        "\n",
        "# check to make sure no erros in the env, such as observation space errors or nans\n",
        "check_env(StockTradingEnv(train, f'{MODEL}.txt', random=True, volumes=False))\n",
        "VecCheckNan(envs, raise_exception=True, check_inf=True)\n",
        "\n",
        "# Agent training\n",
        "model.learn(total_timesteps=MAX_STEPS, callback=callback, progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvrZ0gN0khs0"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/tensorboard/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXu6Nsgjkv95"
      },
      "outputs": [],
      "source": [
        "model = PPO.load(f'{MODEL}/best_model.zip')\n",
        "env = StockTradingEnv(test, f'{MODEL}.txt', r=True, random=False, volumes=False)\n",
        "obs = env.reset()\n",
        "for i in range(len(test.date)):\n",
        "  action, _states = model.predict(obs, deterministic=False)\n",
        "  obs, rewards, done, info = env.step(action)\n",
        "  env.render()\n",
        "  if done:\n",
        "    break\n",
        "df = pd.read_csv(f'{MODEL}.txt')\n",
        "df[' SharesHeld']=df[' SharesHeld']*100\n",
        "x = np.arange(data_two['Close'].size)\n",
        "fit = np.polyfit(x, data_two['Close'], deg=2)\n",
        "fit_function = np.poly1d(fit)\n",
        "y = fit_function(x)\n",
        "\n",
        "for i, row in enumerate(data_two[\"Date\"]):\n",
        "  p = re.compile(\" 00:00:00\")\n",
        "  datetime = p.split(str(data_two[\"Date\"][i]))[0]\n",
        "  data_two.iloc[i, 0] = datetime\n",
        "\n",
        "for i, row in enumerate(df[\"Time\"]):\n",
        "  p = re.compile(\" 00:00:00\")\n",
        "  datetime = p.split(str(df[\"Time\"][i]))[0]\n",
        "  df.iloc[i, 0] = datetime\n",
        "\n",
        "# drawing the interactive figure\n",
        "fig = make_subplots(\n",
        "    rows=3, cols=1,\n",
        "    shared_xaxes=True, \n",
        "    vertical_spacing = 0.01, \n",
        "    specs =[[{\"type\": \"candlestick\"}],\n",
        "           [{\"type\": \"scatter\"}],\n",
        "           [{\"type\": \"scatter\"}]],\n",
        "    subplot_titles=['Candlestick Trend', 'Regression Trend', \"Agent's Trading Behavior\"],\n",
        "    y_title = \"USD\",\n",
        "    x_title = 'Date' \n",
        "    \n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x = df[\"Time\"],\n",
        "        y = df[' Networth'],\n",
        "        mode = \"lines\", \n",
        "        name = \"NetWorth\"\n",
        "    ), \n",
        "    row=3, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x = df[\"Time\"],\n",
        "        y = df[' Balence'],\n",
        "        mode = \"lines\", \n",
        "        name = \"Balance\"\n",
        "    ), \n",
        "    row=3, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x = df[\"Time\"],\n",
        "        y = df[' SharesHeld'],\n",
        "        mode = \"lines\", \n",
        "        name = \"SharesHeld\"\n",
        "    ), \n",
        "    row=3, col=1\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x = data_two[\"Date\"],\n",
        "        y = data_two['Close'],\n",
        "        mode = \"lines\", \n",
        "        name = \"Real Close\"\n",
        "    ), \n",
        "    row=2, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x = data_two[\"Date\"],\n",
        "        y = y,\n",
        "        mode = \"lines\", \n",
        "        name = \"Tend Line\"\n",
        "    ), \n",
        "    row=2, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Candlestick(\n",
        "        x = data_two[\"Date\"],\n",
        "        open = data_two[\"Open\"],\n",
        "        high = data_two[\"High\"], \n",
        "        low = data_two[\"Low\"], \n",
        "        close = data_two[\"Close\"]\n",
        "    ), \n",
        "    row=1, col=1\n",
        ")\n",
        "fig.update_layout(\n",
        "    font_family=\"Times New Roman\",\n",
        "    font_color=\"Black\",\n",
        "    title_font_family=\"Courier New, monospace\",\n",
        "    title_font_color=\"Black\" \n",
        ")\n",
        "fig.update_layout(height=800, width=1500, title_text=f\"{T} from {test_start} to {test_end}\")\n",
        "fig.update_layout(xaxis_rangeslider_visible=False)\n",
        "fig.update_layout(hovermode='x unified')\n",
        "fig.show()\n",
        "!rm /content/*.txt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "14PTov59y5BsmHQNkGSVeWkMeD2L6FVWz",
      "authorship_tag": "ABX9TyOoDjHB1AhkzFpISA/KjnfZ",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}