{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReinforementLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1QAI7aXfmQsXetAiOYxngwdC3825iP1MB",
      "authorship_tag": "ABX9TyMGvo1n6tDwdQdMnqr5iQxL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aCStandke/ReinforcementLearning/blob/main/ReinforementLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Importing the OpenAI Gym environment for Super Mario Bros\n",
        "\n",
        "That plays on the Nintendo entertainment System(NES) through a nes-py emulator.\n",
        "\n",
        "By default, gym_super_mario_bros \n",
        "environments use the full NES action space of 256 discrete actions. To contstrain this, gym_super_mario_bros.actions provides three actions lists (RIGHT_ONLY, SIMPLE_MOVEMENT, and COMPLEX_MOVEMENT) for the nes_py.wrappers.JoypadSpace wrapper. See gym_super_mario_bros/actions.py for a breakdown of the legal actions in each of these three lists.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Citation: \n",
        "@misc{gym-super-mario-bros,\n",
        "  author = {Christian Kauten},\n",
        "  howpublished = {GitHub},\n",
        "  title = {{S}uper {M}ario {B}ros for {O}pen{AI} {G}ym},\n",
        "  URL = {https://github.com/Kautenja/gym-super-mario-bros},\n",
        "  year = {2018},\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "6tkuNyB6NhbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install gym-super-mario-bros==7.3.0\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "from pyvirtualdisplay import Display"
      ],
      "metadata": {
        "id": "BfMQPc-JS2dV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e33af25-f86b-4a44-df6f-15b7bebcfc96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym-super-mario-bros==7.3.0 in /usr/local/lib/python3.7/dist-packages (7.3.0)\n",
            "Requirement already satisfied: nes-py>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from gym-super-mario-bros==7.3.0) (8.1.8)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.21.6)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,271 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n",
            "Fetched 784 kB in 0s (5,661 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155501 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bwrjfZCCP48v"
      },
      "outputs": [],
      "source": [
        "# Installing the required libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy\n",
        "from IPython import display as ipythondisplay\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Installing Gym an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# Installing NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Installing Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the Mario enviornment\n",
        "\n",
        "The mario environment is intialized using the method make(). \n",
        "\n"
      ],
      "metadata": {
        "id": "fupAs_PpQoc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Super Mario environment\n",
        "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")"
      ],
      "metadata": {
        "id": "9OhG8jj-iB4m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the Environment\n",
        "\n",
        "Since alot of the environmental information is not necessary(ie., color), Wrappers are used to preprocess the environment data before sending it to the agent/Mario. \n",
        "\n",
        "\n",
        "\n",
        "1.   **SkipFrame Class:**\n",
        "  *   This wrapper inherits from the gym.Wrapper and implements the step() function. Because consecutive frames don’t vary much, we can skip n-intermediate frames without losing much information. The n-th frame aggregates rewards accumulated over each skipped frame.\n",
        "\n",
        "2.   **FrameStack Class:**\n",
        "  *   Is a wrapper that allows us to squash consecutive frames of the environment into a single observation point to feed to our learning model. This way, we can identify if Mario was landing or jumping based on the direction of his movement in the previous several frames. \n",
        "\n",
        "3.   **GrayScaleObservation and ResizeObservation Classes**\n",
        "  *   These wrappers inherit from the gym.ObservationWrapper to first transform an RGB image to grayscale so that one color channel is used. Then the image is downsampled for each observation into a square image\n",
        "\n",
        "4.  **JoypadSpace**\n",
        " *   Because the orginal Mario action space consists of 256 discrete actions, the number of actions that Mario can take is limited by setting the JoypadSpace wrapper to RIGHT_ONLY\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "80Lc38GfUETs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # permute [H, W, C] array to [C, H, W] tensor\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# Apply Wrappers to environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = FrameStack(env, num_stack=4)\n",
        "# Using the RIGHT_ONLY action list for Mario in the environment\n",
        "env = JoypadSpace(env, RIGHT_ONLY)"
      ],
      "metadata": {
        "id": "t_mc5CCkiKAQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Mario Agent\n",
        "\n",
        "The Mario Agent should be able to do the following things:\n",
        "\n",
        "1.   **ACT:** Mario should be able to take the most optimal action policy based on the current state of the environment\n",
        "2.   **Remember:** Mario should be able to remember his past actions, to update his action policy. In this case Experience/Memory for Mario =(current state, current action, reward, next state). This is implemented through caching and later recalling his experiences\n",
        "3.   **Learn:** Mario should learn a better action policy overtime \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Act\n",
        "\n",
        "For any given state, Mario can choose to do the most optimal action(called exploit) or a random action(called explore). When Mario chooses to randomly explore this chance is determined by self.exploration_rate; however, when Mario chooses to exploit, Mario relies on MarioNet, which is a DDQN algorithm to get the most optimal action."
      ],
      "metadata": {
        "id": "TnzE9IibYMYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir, pretrained):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "        self.pretrained = pretrained\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "\n",
        "        # Mario's DNN to predict the most optimal action when doing exploit\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        # load the weights if pretraining is on\n",
        "        if self.pretrained:\n",
        "          self.net.load_state_dict(torch.load(\"model.pth\"))\n",
        "        # use gpu if availiable\n",
        "        if self.use_cuda:\n",
        "            self.net = self.net.to(device=\"cuda\")\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99\n",
        "        self.exploration_rate_min = 0.01\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        # Given a state, shoose an epsilon-greedy action\n",
        "        # and update value of step\n",
        "\n",
        "        # Inputs:\n",
        "        # state: A single observation of the current state, dim\n",
        "        # is (state_dim)\n",
        "        # Outputs:\n",
        "        # action: an int representing with action Mario will take\n",
        "\n",
        "        # EXPLORE\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # EXPLOIT\n",
        "        else:\n",
        "            state = state.__array__()\n",
        "            if self.use_cuda:\n",
        "                state = torch.tensor(state).cuda()\n",
        "            else:\n",
        "                state = torch.tensor(state)\n",
        "            state = state.unsqueeze(0)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return action_idx\n"
      ],
      "metadata": {
        "id": "o4EU-jNwihsB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mario's Memory\n",
        "\n",
        "\n",
        "\n",
        "1.   **Cache function:** Each time Mario performs an action, he stores the experience to his memory. His experience includes the current state, action performed, reward from the action, the next state, and whether the game is done.\n",
        "2.   **Recall function:** Mario randomly samples a batch of experiences from his memory, and uses that to learn the game. \n",
        "\n",
        "Note: Made the batch size to 64 rather than 32 in the tutorial for Mario to have more access to past experiences to help him in the environment \n",
        "\n"
      ],
      "metadata": {
        "id": "S_BDKzcggXo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):  # subclassing for continuity\n",
        "    def __init__(self, state_dim, action_dim, save_dir, pretrained):\n",
        "        super().__init__(state_dim, action_dim, save_dir, pretrained)\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.batch_size = 64\n",
        "       \n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        Inputs:\n",
        "        state (LazyFrame),\n",
        "        next_state (LazyFrame),\n",
        "        action (int),\n",
        "        reward (float),\n",
        "        done(bool))\n",
        "        \"\"\"\n",
        "        state = state.__array__()\n",
        "        next_state = next_state.__array__()\n",
        "\n",
        "        if self.pretrained:\n",
        "            if self.use_cuda:\n",
        "              state = torch.load(\"STATE_MEM.pth\", map_location=torch.device(\"cuda\"))\n",
        "              action = torch.load(\"ACTION_MEM.pth\",map_location=torch.device(\"cuda\"))\n",
        "              reward = torch.load(\"REWARD_MEM.pth\",map_location=torch.device(\"cuda\"))\n",
        "              next_state = torch.load(\"NEXT_State.pth\",map_location=torch.device(\"cuda\"))\n",
        "              done = torch.load(\"DONE.pth\", map_location=torch.device(\"cuda\"))\n",
        "            else:\n",
        "              state = torch.load(\"STATE_MEM.pth\")\n",
        "              action = torch.load(\"ACTION_MEM.pth\")\n",
        "              reward = torch.load(\"REWARD_MEM.pth\")\n",
        "              next_state = torch.load(\"NEXT_State.pth\")\n",
        "              done = torch.load(\"DONE.pth\")\n",
        "        else:\n",
        "          if self.use_cuda:\n",
        "            state = torch.tensor(state).cuda()\n",
        "            next_state = torch.tensor(next_state).cuda()\n",
        "            action = torch.tensor([action]).cuda()\n",
        "            reward = torch.tensor([reward]).cuda()\n",
        "            done = torch.tensor([done]).cuda()\n",
        "          else:\n",
        "            state = torch.tensor(state)\n",
        "            next_state = torch.tensor(next_state)\n",
        "            action = torch.tensor([action])\n",
        "            reward = torch.tensor([reward])\n",
        "            done = torch.tensor([done])\n",
        "\n",
        "        self.memory.append((state, next_state, action, reward, done,))\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        Retrieve a batch of experiences from memory\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6ktYybJloZx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning\n",
        "\n",
        "Mario's Action policy for solving sequential decision problems (ie., the machine brain, lol) is the DDQN algorithm as detailed in https://arxiv.org/pdf/1509.06461.pdf[1]\n",
        "\n",
        "\n",
        "---\n",
        "# Double DQN Algorithm\n",
        "The idea of Double Q-learning is to reduce overestimations\n",
        "by decomposing the max operation in the target into action\n",
        "selection and action evaluation[1]. The algorithm evaluates the greedy policy according to the online network and uses the target network to estimate its value[1]. \n",
        "\n",
        "![Screenshot 2022-04-22 3.02.36 PM.png](data:image/png;base64,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)\n",
        "\n",
        "The weights of the second network contain the weights of the target network ***Θₜ-***for the evaluation of the current greedy policy. The update to the target network stays unchanged from\n",
        "DQN, and remains a periodic copy of the online network[1].\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ys61BIuuh9Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MarioNet(nn.Module):\n",
        "    \"\"\"mini cnn structure\n",
        "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "       \n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.online = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )\n",
        "\n",
        "        self.target = copy.deepcopy(self.online)\n",
        "\n",
        "        # Q_target parameters are frozen.\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)"
      ],
      "metadata": {
        "id": "anTD0kHplt0t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TD Estimate & TD Target\n",
        "\n",
        "Two values are involved in learning:\n",
        "**TD Estimate** - the predicted optimal 𝑸* for a given state *s*\n",
        "\n",
        "![Screenshot 2022-04-22 3.37.47 PM.png](data:image/png;base64,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)\n",
        "\n",
        "**TD Target** - aggregation of current reward and the estimated 𝑸* in the next state *s'* \n",
        "\n",
        "\n",
        "![Screenshot 2022-04-22 3.37.59 PM.png](data:image/png;base64,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)\n",
        "\n",
        "![Screenshot 2022-04-22 3.38.06 PM.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMUAAAAnCAYAAAC/tIx5AAAAAXNSR0IArs4c6QAADI5JREFUeJztnHtQlNUbx7/LAqahwArkKitEAiGMCboICToxAQlMpYKEjmlyExGny4wM8YcNNpnOOA1NTCJTKZA4KEHiAGIW1gbuAlLIRVhRQZZi2WUFhIW9nd8fDu/PvXHZxUv1fv7inPOc5znnnfPd9znnvMoghBDQ0NBQWDztAdDQPGvQoqCh0YEWBQ2NDrQoaGh0oEVBQ6MDLQoaGh1oUdDQ6ECLgoZGB1oUNDQ60KKgodGBFgXNY6ewsBAZGRlm+1EqlQgPD4dQKJyDURmHFsU/mJKSErS0tDyV2BqNBnw+f0a2ly5dwpIlS8yO2dTUhP7+frDZbABAbW2t2T4NYWlqR7VajZqaGhj7npDBYIDJZILL5eL5558HALS3t6O5uZnqM2ljbW0NX19fcDgcU4fzr0KlUqGyshL19fUYHx+Hk5MToqOj4erqCuDhgkxLS8OyZcsgkUhQUlKCQ4cOPdExZmdnY/369dPaKRQK3LhxA0eOHDE7pkAgQEREBGxsbAAAMpkMeXl5SExMNNv3o5j8prh69SqysrLQ1tYGsViMqqoqpKamoq2tDVKpFDdu3EBqaiqGhoaoPvPnz8fExAQOHTqE7u5uODo6wt7eHmKxGHFxcfjiiy/mZFL/ZFpaWvD222+jv78fH3/8MY4dO4aAgADs2LEDAoEAAGBhYYF9+/aBx+Ph999/R1JS0hMd408//YTx8XH4+/tPa/vnn39izZo1cHZ2NjuuQCDAtm3bqHJkZCTa29tx7do1s31rQUwkIyODdHV1UeWsrCzC5XKJWq2m6t577z2i0Wi0+l24cIF4eHiQ/v5+rXqhUEi8vLwIj8czdUhPlMHBQXL+/Pk59dnY2Ej8/PzIb7/9ptf2+eefk/DwcKJSqYharSZffvklyc/PJzU1NeTkyZNzOo7p2Lx5MxGJRDOyzcnJIdXV1WbHVCgUJCYmRq++paWFREdHm+3/UUx6UyiVSgCAm5sbVcfn88HlcmFh8X+Xjo6OYDAYWn0FAgFcXV3h5OSkVb9ixQo4OzujsrLSlCE9ceRyOfr7++fM3+DgINLS0hAXF4egoCC9dh8fH9y5cwf37t2DhYUF0tLS4OnpCTc3tzlPH6aivr4e1tbWWLp06YzsX375ZYSEhJgdVy6X48MPP9Sr9/b2xuDgIDo7O82OMYlJewqlUom9e/dSZalUCqFQqPVqA4Bdu3bp9eXz+QgICDDolxACiURiypBMoq2tDQUFBVi/fj0cHR1RVVUFFxcX7N69e07jKBQKnDp1Co2NjQgLC8PWrVsBABKJBHfu3AGXy0VOTg5GRkawZ8+eKX2JxWJqbzGT9GWu4fF48PPzM9gmFotx+vRpjI+PY8WKFYiLi5u1IAghKC8vR2trK5YtWwZPT0+4u7uDxWIZXTd+fn6ora2Fh4fHrOdjCJNEsWDBAixfvpwqT55CrFu3Tstu5cqVWmWxWIzu7m4cOHBAz+fo6Ch6e3sRGBg443Hk5eWht7d3Wrv3338f9vb2evWFhYXYvn07YmNjsXfvXnC5XGRkZGDnzp1gMplT+iQz/AeLarUaiYmJIITAzs4OR44cgYODAzZu3IiCggK88847UCqVKCsrQ3BwMFgslkE/HR0dAAAHB4cZxX1cdHZ2YuPGjXr1CoUCe/bsQV5eHubNm4eoqChs2rQJdnZ2s/Kfnp6OBQsW4JNPPkFpaSneffddFBYWGn0uAMDhcNDV1TXruRjD5NOnRxEIBLC1tZ1WqcbEM9mm0WiwYcMGqq6iogLe3t5wcXEx6C8+Pn7K06/J1E03hQMevt3Wrl2LW7duwd7eHqmpqWhoaEB6erpBQchkMkxMTFDlgYEBjI6O4u+//9aye+GFF7TilZeXIyoqCjExMQAeLqrs7GxwuVzI5XKw2Wy0t7fjwYMHWL16tcG5AA+fMZvNxosvvmjU5kkgk8mwaNEivXqhUIi7d++iubkZK1euRHJy8qwFUVJSgp9//hlXr14FALDZbFhbW2PVqlVT9rO1tX366ZMufD4f/v7+Bhefrp2rqyscHR312i5evAgOh4PXXnuNqvvmm2+Qm5tr1N+j+5fZsnjxYmzZsgWZmZkIDAwEk8nEunXrDAoWAKqrqyEWi6ny8PAwOjo6cO7cOS27nTt3ai2GydRsEg8PDzCZTJw9exaxsbEA/r9HM3ZCIxQK0dTUhMzMTK1n/PXXXyMlJWWWM58dujHUajWsrKz07Nzc3ODs7IwDBw7A2toax48fn3Ws77//XusIv6GhAa+88grmzZs3ZT8rKysoFIpZxzOG2aKQSCS4ffs24uLiprUVCAQG80Iej4fLly+jqKgITCYTfX19OH/+PEQiEcrKypCQkGDQX25uLnp6eqaN+8EHHxhNOwQCAZKTk6f1MbmAJxGJRPjxxx+xb9++KfsZ+gHgcDior6+n9g8cDgcMBoN664hEInR1dSE4OBgMBgOffvopgoKCsGPHDsrHyMgIKioqHqsoDMWwtbXF8PCwlp1CocDly5dRWlqKmzdvIicnB2fOnEFYWNis4nV2duLNN9+kyvX19fD390dfXx8WLVpE3U/oMjw8jIULF84q1lSYLYq6ujoA02/6+vr60N3djbS0NKpOo9GgoqICx44dQ3Z2Nnx8fAAAS5cuha+vLwYHB40KAsCMFvNU9Pf3o6en54lvWO3s7BAeHk6V7e3tER4ejqqqKmzYsAFnzpxBYGAgioqKcOvWLSiVSpw4cYJK6xoaGvDDDz/A0tISFRUViIiIQF9fH6qrq2FjYwNbW1uEhoZCIBCgoaEB7u7uuHv3LrZu3QoWiwWZTIaqqipoNBo4OTkhNDQUExMTKC0thUajwUsvvQQmk6kXAwCWLFmidxhy8eJFZGVlISgoCL6+vggICMCDBw8Mzl0ikWD//v3Yv3+/3ikbm82mfkS6urpw/fp1JCQk4Ntvv8VHH31k9HlKpVKtPa65mJx/HD16FCkpKTh+/DisrKxw+PBhpKSkQC6X69kePHiQWtyFhYVITExEQkICoqOj0dDQgLNnz+qdUkymZI+TlpYWuLi4zOkDnQlisVhvvocPH4aNjQ2SkpKohZuXl4fly5ejoKBAa5+zdu1asFgsJCUlISIiAjKZDOnp6YiNjYWNjQ2uXLkC4OHp2uRnIEKhEBKJBHK5HCkpKYiMjISnpyeKi4tBCEFycjJWrVqFzZs3o66uTi/GJP7+/mhubtYau5+fH7hcLi5cuIBjx46htbXV6IWiTCZDV1cXysrK9NoOHjyI4uJinDx5EufOnQOXy0VlZSUWL16M+fPnG32ekxeEc8ac3nrMITExMWRgYIBcu3btscVQKpXk/v37JvXt7e0lOTk5JvXdvXu30bbOzk5y5coV0tTURKKioohSqSQKhYKcOHFCy27btm1kYGCAEPLwguzo0aPU35OXiiqVirz++utEpVJR/YqKikhmZib1d25uLmlsbCShoaGkuLiYlJSUkOHhYb0Yk0ilUhIcHEwmJia06lUqFbl37x4ZGhqadv4qlYpkZWUZbJPL5UQsFhNCCNFoNOSvv/6a0tf9+/dJaGio1hzN5Zn9IHDhwoX49ddfoVarH1sMS0tL2NramtSXzWZj+/bts+7X29tLbSQN4e7ujpCQEKxevRpDQ0MIDw9HSEgIvLy8KJuxsTGMjo7CwcEBfD4fIpGI+qWsq6uDl5cXWltbcfv2bbi6umq9ZcRiMZWm8ng8+Pv7Y2xsDGvWrEFMTAy2bNmC8fFxvRiTsFgsvPHGGygvL9caN5PJhLOzs8GTKV1++eUXo3cOzz33HJVCMRiMaT8kLCoqQmxs7LRH6LPhmRVFXl4ewsLC8Oqrrz7toRjEwsJi1keOANDc3GzwxtoQUVFRUCqV2LVrl9ZRtUqlAovFQn5+PpydnbFp0ybweDycOnUKY2NjqKmpAYfDwR9//KH30V5kZCSuX7+O06dPo7a2Fj4+PuByuRgdHcWlS5eQn5+Pmzdv6sV4lLS0NFRVVWl91zYb+Hw+QkNDTer7KH19fWhvb5/zy9ZnNn36tyKRSPRSD1OQy+VafsbGxohSqSRqtZqMjY0RQggZHx/XSyt6enoIIYR0dHSQ+Ph4ql6tVhOpVDpljEcRiUTku+++M3se5vDVV1+ZnP5OBYMQ+v+S/a8wMjKCt956C5999hmKi4sRHx8Pb2/vpz2sZw5aFP8xhoeHIZVKweFwYGk5J3e3/zpoUdDQ6PDMbrRpaJ4WtChoaHSgRUFDowMtChoaHWhR0NDoQIuChkYHWhQ0NDrQoqCh0eF/cADrBAY5HSQAAAAASUVORK5CYII=)"
      ],
      "metadata": {
        "id": "kMr7IQmoxIOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir, pretrained):\n",
        "        super().__init__(state_dim, action_dim, save_dir, pretrained)\n",
        "        self.gamma = 0.9\n",
        " \n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
      ],
      "metadata": {
        "id": "W31cwaj4l34Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updating Double DQN Model \n",
        "\n",
        "As Mario samples inputs from his replay buffer, we compute *TDₜ* and *TDₑ* and backpropagate this loss down Qₒₗᵢₙₑ to update its parameters *θₒₗᵢₙₑ* (α is the learning rate)  \n",
        "​\n",
        "![Screenshot 2022-04-22 4.18.50 PM.png](data:image/png;base64,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)\n",
        "\n",
        "theta_target weights does not get updated during backpropogation, instead rather weights from online are assigned to the weights to the target\n",
        "![Screenshot 2022-04-22 4.23.55 PM.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "j2iHHwP_0asM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir, pretrained):\n",
        "        super().__init__(state_dim, action_dim, save_dir, pretrained)\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())"
      ],
      "metadata": {
        "id": "zGNufdN60Lr6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Mario Agent!\n",
        "\n",
        "Mario was trained for 695 episodes before finishing eary due to completing the game and the Double DQN model was saved along with the state, reward, actions, and done"
      ],
      "metadata": {
        "id": "XTaUkaAH9v1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir, pretrained):\n",
        "        super().__init__(state_dim, action_dim, save_dir, pretrained)\n",
        "        self.burnin = 1e4  # min. experiences before training\n",
        "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ],
      "metadata": {
        "id": "hGrzDCy4mjcI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(training_mode, pretrained, num_episodes=1):\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  print(f\"Using CUDA: {use_cuda}\")\n",
        "  print()\n",
        "\n",
        "  save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "  save_dir.mkdir(parents=True)\n",
        "  mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir, pretrained=pretrained)\n",
        "\n",
        "  \n",
        "  total_rewards =[]\n",
        "  for e in tqdm(range(num_episodes)):\n",
        "   \n",
        "\n",
        "    if not training_mode:\n",
        "        cv2_imshow(env.render(mode='rgb_array'))\n",
        "        cv2.waitKey(0)\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    STATE = state\n",
        "    ACTION = None\n",
        "    REWARD = None\n",
        "    NEXT_STATE = None\n",
        "    DONE = None\n",
        "    # Play the game!\n",
        "    while True:\n",
        "          action = mario.act(state)\n",
        "          # Agent performs action\n",
        "          next_state, reward, done, info = env.step(action)\n",
        "          total_reward += reward\n",
        "          STATE = state\n",
        "          ACTION = action\n",
        "          REWARD = reward\n",
        "          NEXT_STATE = next_state\n",
        "          DONE = done\n",
        "          if training_mode:\n",
        "            # Remember\n",
        "            mario.cache(state, next_state, action, reward, done)\n",
        "            # Learn\n",
        "            q, loss = mario.learn()\n",
        "          # Update state\n",
        "          state = next_state\n",
        "          \n",
        "          if not training_mode:\n",
        "            cv2_imshow(env.render(mode='rgb_array')) \n",
        "          # Check if end of game\n",
        "          if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    total_rewards.append(total_reward)\n",
        "    if e != 0 and e % 100 == 0:\n",
        "      print(\"Episode {} score = {}, average score = {}\".format(e + 1, total_rewards[-1], np.mean(total_rewards)))\n",
        "\n",
        "  print(\"Episode {} score = {}, average score = {}\".format(e + 1, total_rewards[-1], np.mean(total_rewards)))\n",
        "  if training_mode:\n",
        "    torch.save(mario.net.state_dict(), \"model.pth\")\n",
        "    torch.save(STATE,  \"STATE_MEM.pth\")\n",
        "    torch.save(ACTION, \"ACTION_MEM.pth\")\n",
        "    torch.save(REWARD, \"REWARD_MEM.pth\")\n",
        "    torch.save(NEXT_STATE, \"NEXT_State.pth\")\n",
        "    torch.save(DONE,   \"DONE.pth\")\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  env.close()\n"
      ],
      "metadata": {
        "id": "Amq6AoQDmrgp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runing the game and then testing the game\n",
        "run(training_mode=True, pretrained=False, num_episodes=10000)\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NeOcPPok-lE",
        "outputId": "660c6e03-d36b-4af7-831d-e62aec50b7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA: False\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n",
            "  1%|          | 101/10000 [01:43<2:05:55,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 101 score = 232.0, average score = 264.6237623762376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 201/10000 [07:47<5:55:54,  2.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 201 score = 251.0, average score = 277.318407960199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 301/10000 [11:58<5:26:07,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 301 score = 252.0, average score = 274.7076411960133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 401/10000 [15:45<5:35:13,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 401 score = 252.0, average score = 270.8304239401496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 501/10000 [23:19<26:29:49, 10.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 501 score = 247.0, average score = 296.750499001996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 601/10000 [41:14<27:51:52, 10.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 601 score = 637.0, average score = 354.6289517470882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 695/10000 [53:27<26:24:21, 10.22s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # For Testing\n",
        "# run(training_mode=False, pretrained=True, num_episodes=1)"
      ],
      "metadata": {
        "id": "_mRP7QpZ_LIt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}