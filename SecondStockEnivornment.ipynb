{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aCStandke/ReinforcementLearning/blob/main/SecondStockEnivornment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TBwoqXizywK"
      },
      "source": [
        "# Second Stock Trading Environment\n",
        "\n",
        "\n",
        "  This second stock environment is based on Adam King's article as found here:[Create custom gym environments from scratch — A stock market example](https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e). Similar to the first stock trading environment based on Maxim Lapan's implementation as found in chapter eight of his book [Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition](https://www.amazon.com/Deep-Reinforcement-Learning-Hands-optimization/dp/1838826998), the agent is trading in the environment of the [SPY ETF](https://www.etf.com/SPY?L=1) except in this trading environment the agent is taking continuous actions, rather than discrete actions and is tasked with managing a [trading account](https://www.investopedia.com/terms/t/tradingaccount.asp#:~:text=A%20trading%20account%20is%20an,margin%20requirements%20set%20by%20FINRA.).  \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "  In the first trading environment, the agent's reward is based on relative price movement, however in this trading environment the agent's reward is based on managing its trading account. As Adam King details the agent can take two actions: 1) either buying or selling the SPY ETF (none: eventhough this decision is discrete in nature, it is being modeled as a continous action by making values less than 1.0 as buy actions and values greater than 1.0 and less than 2.0 as sell actions);  and 2) by what percentage of the SPY ETF's shares to buy or sell, which ranges in the continous interval of [0-1] (i.e. 0% to 100%).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHO1Q4dS9tF0"
      },
      "outputs": [],
      "source": [
        "# ignore warning messages because they are annoying lol\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGbbfGz0KsDS"
      },
      "source": [
        "# Installing Necessary Package for Training the Trading Agent\n",
        "\n",
        "To train the Trading Agent the package [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/index.html) was used. As stated in the docs: \n",
        "> Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines. And steems from the paper [Stable-Baselines3: Reliable Reinforcement Learning Implementations](https://jmlr.org/papers/volume22/20-1364/20-1364.pdf)\n",
        "The algorithms in this package will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\n",
        "\n",
        "Because in this environment the Agent will be executing continous actions, two different RL algorithms were chosen. The first one was  Deep Deterministic Policy Gradient(DDPG) and the second one was Soft Actor Critic (SAC).  \n",
        "\n",
        "---\n",
        "### Deep Deterministic Policy Gradient(DDPG):\n",
        "\n",
        "As stated by the authors of [CONTINUOUS CONTROL WITH DEEP REINFORCEMENT\n",
        "LEARNING](https://arxiv.org/pdf/1509.02971.pdf):\n",
        "\n",
        "> While DQN solves problems with high-dimensional observation spaces, it can only handle discrete and low-dimensional action spaces. Many tasks of interest, most notably physical control tasks, have continuous (real valued) and high dimensional action spaces. DQN cannot be straightforwardly applied to continuous domains since it relies on a finding the action that maximizes the\n",
        "action-value function, which in the continuous valued case requires an iterative optimization process at every step. In this work we present a model-free, off-policy actor-critic algorithm using deep function approximators that can learn policies in high-dimensional, continuous action spaces. Here we combine the actor-critic approach with insights from the recent success of Deep Q Network [DQN](https://arxiv.org/pdf/1312.5602.pdf) \n",
        "\n",
        "**The key take aways from the algorithm are the following:** \n",
        "\n",
        "1.   Uses an actor-critic approach based on the [DPG algorithm](http://proceedings.mlr.press/v32/silver14.pdf) \n",
        "2.   A replay buffer is used to store transitions for sampling, since it is an off-policy algorithm\n",
        "3.   A copy of the actor and critic networks,  $µ_{\\theta}(s|θ)$ and $Q_{\\phi}(s,a|{\\phi})$ respectively, are used for calculating the target values. The weights of these target networks are then updated by having them slowly track the learned networks\n",
        "4.   Batch normalization is used \n",
        "5.   An exploration policy is used by adding noise sampled from a noise process *N* to the actor policy  $µ_{\\theta}(s_t)= µ(s_t |{\\theta}, µ_t) + N$ \n",
        "\n",
        "*N* can be chosen to suit the environment, I used the Ornstein Uhlenbeck Noise as provided by Stable-Baselines\n",
        "\n",
        "---\n",
        "### Soft Actor Critic (SAC):\n",
        "\n",
        "As stated by the authors of [Soft Actor-Critic:\n",
        "Off-Policy Maximum Entropy Deep Reinforcement\n",
        "Learning with a Stochastic Actor](https://arxiv.org/pdf/1801.01290.pdf):\n",
        "\n",
        "> In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting\n",
        "as randomly as possible.\n",
        "\n",
        "**The key take aways from the algorithm are the following:** \n",
        "\n",
        "1.   Uses a maximum entropy objective rather than  the standard maximum\n",
        "expected reward objective \n",
        "2.   Uses a soft policy iteration, which is a general algorithm for learning optimal maximum entropy policies that alternate between policy evaluation and policy improvement \n",
        "3.   SAC concurrently learns a policy  $\\pi_{\\theta}$ and two Q-functions  $Q_{\\phi_1}$,  $Q_{\\phi_2}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqZJWIuzLFDb",
        "outputId": "adb864da-d38d-43f6-91d4-db02d1ae4ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Requirement already satisfied: gym==0.21 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.21.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.5.0)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.12.1+cu113)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: ale-py==0.7.4 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.7.4)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.4.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.8.0)\n",
            "Requirement already satisfied: protobuf~=3.19.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.19.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (4.12.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4->stable-baselines3[extra]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4->stable-baselines3[extra]) (3.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.47.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2022.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HDwRKzPLJWG"
      },
      "source": [
        "# Installing the Necessary Packages for Visualizing the Trading Agent's Envirnoment on Google Colab Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT_p1bNSDnJ_",
        "outputId": "261737ed-e185-4182-871b-9ce8d97ef956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mpl_finance in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mpl_finance) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mpl_finance) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mpl_finance) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (0.2.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.64.0)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (2.9.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageio_ffmpeg in /usr/local/lib/python3.7/dist-packages (0.4.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install mpl_finance #used for plotting the candelstick graph\n",
        "!pip install moviepy #\n",
        "!pip install imageio_ffmpeg #\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1 #used to create a display for vm\n",
        "!apt-get install x11-utils > /dev/null 2>&1 #\n",
        "!pip install pyglet==v1.3.2 > /dev/null 2>&1 #\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1 #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoLhA3_b_XeK"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib import style\n",
        "import json\n",
        "import datetime as dt\n",
        "from stable_baselines3 import SAC, DDPG\n",
        "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "import collections\n",
        "import datetime\n",
        "from mpl_finance import candlestick_ochl as candlestick\n",
        "import math\n",
        "import os\n",
        "import moviepy.video.io.ImageSequenceClip\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB9FxIN_AQC4",
        "outputId": "5168f204-bf43-4d2f-9a4d-78dd53883686"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fc2c7395650>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Stock Environment Parameters\n",
        "MAX_ACCOUNT_BALANCE = 2147483647\n",
        "MAX_NUM_SHARES = 2147483647\n",
        "TRADING_DAYS = 5\n",
        "DEFAULT_COMMISSION_PERC = 0.01\n",
        "INITIAL_ACCOUNT_BALANCE = 10000\n",
        "\n",
        "# Visualization Parameters\n",
        "style.use('dark_background')\n",
        "VOLUME_CHART_HEIGHT = 0.20\n",
        "UP_COLOR = '#27A59A'\n",
        "DOWN_COLOR = '#EF534F'\n",
        "UP_TEXT_COLOR = '#73D3CC'\n",
        "DOWN_TEXT_COLOR = '#DC2C27'\n",
        "LOOKBACK_WINDOW_SIZE = 10\n",
        "\n",
        "# Video Parameters\n",
        "image_folder='/content/frames/'\n",
        "!mkdir '/content/frames'\n",
        "fps=1\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y-VGjdGWxke"
      },
      "source": [
        " ## Creating visualization for Stock/ETF Environment\n",
        " As detalied by Adam King in his article titled\n",
        " [Rendering elegant stock trading agents using Matplotlib \n",
        " and Gym](https://towardsdatascience.com/visualizing-stock-trading-agents-using-matplotlib-and-gym-584c992bc6d4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2xK5ujxWX7i"
      },
      "outputs": [],
      "source": [
        "def date2num(date):\n",
        "  converter = mdates.strpdate2num('%Y-%m-%d')\n",
        "  return converter(date)\n",
        "\n",
        "class StockTradingGraph:\n",
        "  \"\"\"A stock trading visualization using matplotlib made to render \n",
        "    OpenAI gym environments\"\"\"\n",
        "  def __init__(self, df, title=None):\n",
        "    self.df = df\n",
        "    self.net_worths = np.zeros(len(df.date))\n",
        "    self.frame_cnt = 0\n",
        "    # Create a figure on screen and set the title\n",
        "    fig = plt.figure()\n",
        "    fig.suptitle(title)\n",
        "    # Create top subplot for net worth axis\n",
        "    self.net_worth_ax = plt.subplot2grid((6, 1), (0, 0), rowspan=2,     \n",
        "      colspan=1)\n",
        "  \n",
        "    # Create bottom subplot for shared price/volume axis\n",
        "    self.price_ax = plt.subplot2grid((6, 1), (2, 0), rowspan=8, \n",
        "      colspan=1, sharex=self.net_worth_ax)\n",
        "    # Create a new axis for volume which shares its x-axis with price\n",
        "    self.volume_ax = self.price_ax.twinx()\n",
        "    # Add padding to make graph easier to view\n",
        "    plt.subplots_adjust(left=0.11, bottom=0.24, right=0.90, \n",
        "      top=0.90, wspace=0.2, hspace=0)\n",
        "    \n",
        "  \n",
        "  def render(self, current_step, net_worth, trades, window_size=40):\n",
        "    self.net_worths[current_step] = net_worth\n",
        "    window_start = max(current_step - window_size, 0)\n",
        "    step_range = range(window_start, current_step + 1)\n",
        "    # Format dates as timestamps, necessary for candlestick graph\n",
        "    dates = np.array([date2num(x) for x in self.df.date[step_range]])\n",
        "    \n",
        "\n",
        "    self._render_net_worth(current_step, net_worth, step_range, dates)\n",
        "    self._render_price(current_step, net_worth, dates, step_range)\n",
        "    self._render_volume(current_step, net_worth, dates, step_range)\n",
        "    self._render_trades(current_step, trades, step_range)\n",
        "        \n",
        "    # Format the date ticks to be more easily read\n",
        "    self.price_ax.set_xticklabels(self.df.date[step_range], rotation=45,\n",
        "                                      horizontalalignment='right')\n",
        "        \n",
        "    # Hide duplicate net worth date labels\n",
        "    plt.setp(self.net_worth_ax.get_xticklabels(), visible=False)\n",
        "    plt.savefig(f'/content/frames/frame_{self.frame_cnt}.png')\n",
        "    self.frame_cnt +=1\n",
        "\n",
        "  \n",
        "  def _render_net_worth(self, current_step, net_worth, step_range, dates):\n",
        "    # Clear the frame rendered last step\n",
        "    self.net_worth_ax.clear()\n",
        "\n",
        "    # Plot net worths\n",
        "    self.net_worth_ax.plot_date(dates, self.net_worths[step_range],\n",
        "                                '-', label='Net Worth')\n",
        "\n",
        "    # Show legend, which uses the label we defined for the plot above\n",
        "    self.net_worth_ax.legend()\n",
        "    legend = self.net_worth_ax.legend(loc=2, ncol=2, prop={'size': 8})\n",
        "    legend.get_frame().set_alpha(0.4)\n",
        "\n",
        "    last_date = date2num(self.df.date[current_step])\n",
        "    last_net_worth = self.net_worths[current_step]\n",
        "\n",
        "    # Annotate the current net worth on the net worth graph\n",
        "    self.net_worth_ax.annotate('{0:.2f}'.format(net_worth), (last_date, last_net_worth),\n",
        "                                   xytext=(last_date, last_net_worth),\n",
        "                                   bbox=dict(boxstyle='round',\n",
        "                                             fc='w', ec='k', lw=1),\n",
        "                                   color=\"black\",\n",
        "                                   fontsize=\"small\")\n",
        "\n",
        "    # Add space above and below min/max net worth\n",
        "    self.net_worth_ax.set_ylim(\n",
        "            min(self.net_worths[np.nonzero(self.net_worths)]) / 1.25, max(self.net_worths) * 1.25)\n",
        "    \n",
        "\n",
        "  def _render_price(self, current_step, net_worth, dates, step_range):\n",
        "        self.price_ax.clear()\n",
        "\n",
        "        # Format data for OHCL candlestick graph\n",
        "        candlesticks = zip(dates,\n",
        "                           self.df.open[step_range], self._cur_close(step_range),\n",
        "                           self._cur_high(step_range), self._cur_low(step_range))\n",
        "\n",
        "        # Plot price using candlestick graph from mpl_finance\n",
        "        candlestick(self.price_ax, candlesticks, width=1,\n",
        "                    colorup=UP_COLOR, colordown=DOWN_COLOR)\n",
        "\n",
        "        last_date = date2num(self.df.date[current_step])\n",
        "        last_close = self._cur_close(current_step)\n",
        "        last_high = self._cur_high(current_step)\n",
        "\n",
        "        # Print the current price to the price axis\n",
        "        self.price_ax.annotate('{0:.2f}'.format(last_close), (last_date, last_close),\n",
        "                               xytext=(last_date, last_high),\n",
        "                               bbox=dict(boxstyle='round',\n",
        "                                         fc='w', ec='k', lw=1),\n",
        "                               color=\"black\",\n",
        "                               fontsize=\"small\")\n",
        "\n",
        "        # Shift price axis up to give volume chart space\n",
        "        ylim = self.price_ax.get_ylim()\n",
        "        self.price_ax.set_ylim(ylim[0] - (ylim[1] - ylim[0])\n",
        "                               * VOLUME_CHART_HEIGHT, ylim[1])\n",
        "        \n",
        "  def _render_volume(self, current_step, net_worth, dates, step_range):\n",
        "        self.volume_ax.clear()\n",
        "\n",
        "        volume = np.array(self.df.volume[step_range])\n",
        "\n",
        "        pos = self.df.open[step_range] - \\\n",
        "            self._cur_close([step_range]) < 0\n",
        "        neg = self.df.open[step_range] - \\\n",
        "            self._cur_close([step_range]) > 0\n",
        "\n",
        "        # Color volume bars based on price direction on that date\n",
        "        self.volume_ax.bar(dates[pos], volume[pos], color=UP_COLOR,\n",
        "                           alpha=0.4, width=0.5, align='center')\n",
        "        self.volume_ax.bar(dates[neg], volume[neg], color=DOWN_COLOR,\n",
        "                           alpha=0.4, width=0.5, align='center')\n",
        "\n",
        "        # Cap volume axis height below price chart and hide ticks\n",
        "        self.volume_ax.set_ylim(0, max(volume) / VOLUME_CHART_HEIGHT)\n",
        "        self.volume_ax.yaxis.set_ticks([])\n",
        "\n",
        "  def _render_trades(self, current_step, trades, step_range):\n",
        "    for trade in trades:\n",
        "      if trade['step'] in step_range:\n",
        "        date = date2num(self.df.date[trade['step']])\n",
        "        high = self._cur_high(trade['step'])\n",
        "        low = self._cur_low(trade['step'])\n",
        "         \n",
        "        if trade['type'] == 'buy':\n",
        "          high_low = low\n",
        "          color = UP_TEXT_COLOR\n",
        "        else:\n",
        "          high_low = high\n",
        "          color = DOWN_TEXT_COLOR\n",
        "        \n",
        "        total = '{0:.2f}'.format(trade['total'])\n",
        "        \n",
        "        # Print the current price to the price axis\n",
        "        self.price_ax.annotate(f'${total}', (date, high_low),\n",
        "                             xytext=(date, high_low),\n",
        "                             color=color,\n",
        "                             fontsize=8,\n",
        "                             arrowprops=(dict(color=color)))\n",
        "                \n",
        "  def _cur_close(self, offset):\n",
        "      \"\"\"\n",
        "      Calculate real close price for the current bar\n",
        "      \"\"\"\n",
        "      open = self.df.open[offset]\n",
        "      rel_close = self.df.close[offset]\n",
        "      return open * (1.0 + rel_close)\n",
        "\n",
        "  def _cur_high(self, offset):\n",
        "      \"\"\"\n",
        "      Calculate real close price for the current bar\n",
        "      \"\"\"\n",
        "      open = self.df.open[offset]\n",
        "      rel_high = self.df.high[offset]\n",
        "      return open * (1.0 + rel_high)\n",
        "\n",
        "  def _cur_low(self, offset):\n",
        "      \"\"\"\n",
        "      Calculate real close price for the current bar\n",
        "      \"\"\"\n",
        "      open = self.df.open[offset]\n",
        "      rel_low = self.df.low[offset]\n",
        "      return open * (1.0 + rel_low)\n",
        "\n",
        "  def close(self):\n",
        "    plt.close()\n",
        "              \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJiMYPDIAj3y"
      },
      "outputs": [],
      "source": [
        "# Stock/ETF Trading Enviornment\n",
        "class StockTradingEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, data, random_ofs_on_reset=True, commission_prec=DEFAULT_COMMISSION_PERC):\n",
        "        super(StockTradingEnv, self).__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.random_ofs_on_reset = random_ofs_on_reset\n",
        "        self.bars_count = TRADING_DAYS\n",
        "        self.commission_perc = commission_prec\n",
        "        self.visualization = None \n",
        "\n",
        "        # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([-1, 0]), high=np.array([1, 1]), dtype=np.int32)\n",
        "\n",
        "        # Prices contains the OHCL values for the last five prices\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=1, shape=self.shape)\n",
        "        \n",
        "        self.random_ofs_on_reset = random_ofs_on_reset\n",
        "        self.seed()\n",
        "\n",
        "    def reset(self):\n",
        "      bars = self.bars_count\n",
        "      if self.random_ofs_on_reset:\n",
        "        offset = self.np_random.choice(self.data.high.shape[0]-bars*10)+bars\n",
        "      else:\n",
        "        offset = bars\n",
        "      self._reset(offset)\n",
        "      return self._next_observation()  \n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "      return (4*self.bars_count+4, )\n",
        "\n",
        "    def _next_observation(self):\n",
        "        # Get the stock data points for the last 5 days and scale to between 0-1\n",
        "        res = np.ndarray(shape=self.shape, dtype=np.float32)\n",
        "        shift = 0\n",
        "        for bar_idx in range(-self.bars_count+1, 1):\n",
        "          res[shift] = self.data.high[self._offset + bar_idx]\n",
        "          shift += 1\n",
        "          res[shift] = self.data.low[self._offset + bar_idx]\n",
        "          shift += 1\n",
        "          res[shift] = self.data.close[self._offset + bar_idx]\n",
        "          shift += 1\n",
        "          res[shift] = self.data.volume[self._offset + bar_idx]\n",
        "          shift += 1\n",
        "\n",
        "        # Append additional data and scale each value to between 0-1\n",
        "        res[shift] = self.balance / MAX_ACCOUNT_BALANCE\n",
        "        shift += 1\n",
        "        res[shift] = self.shares_held / MAX_NUM_SHARES\n",
        "        shift += 1\n",
        "        res[shift] = self.total_shares_sold / MAX_NUM_SHARES\n",
        "        shift += 1\n",
        "        res[shift] = self.total_shares_bought/MAX_NUM_SHARES\n",
        "      \n",
        "        return res\n",
        "\n",
        "    def _take_action(self, action):\n",
        "\n",
        "        reward = 0.0\n",
        "        current_price = self._cur_close()\n",
        "        action_type = int(action[0])\n",
        "        amount = action[1]\n",
        "\n",
        "        if action_type < 0:\n",
        "            total_possible = int(self.balance / current_price)\n",
        "            total_possible = abs(total_possible)\n",
        "            shares_bought = int(total_possible * amount)\n",
        "            additional_cost = (shares_bought * current_price) \n",
        "\n",
        "            # balance calculation \n",
        "            self.balance-=additional_cost\n",
        "         \n",
        "\n",
        "            self.shares_held += shares_bought\n",
        "            self.total_shares_bought += shares_bought\n",
        "\n",
        "            \n",
        "            \n",
        "            # visualization portion\n",
        "            if shares_bought > 0:\n",
        "              self.trades.append({'step': self._offset, 'shares': shares_bought, \n",
        "                                  'total': additional_cost, 'type': \"buy\"})\n",
        "\n",
        "        else:\n",
        "            shares_sold = int(self.shares_held * amount)\n",
        "            gain = (shares_sold * current_price) \n",
        "            \n",
        "            # balance calculation \n",
        "            self.balance += gain \n",
        "\n",
        "            self.shares_held -= shares_sold\n",
        "            self.total_shares_sold += shares_sold\n",
        "\n",
        "            \n",
        "\n",
        "            # visualization portion\n",
        "            if shares_sold > 0:\n",
        "              self.trades.append({'step': self._offset, 'shares': shares_sold, \n",
        "                                  'total': gain, 'type': \"sell\"})\n",
        "            \n",
        "        self.net_worth = self.balance + self.shares_held * current_price\n",
        "\n",
        "        if self.net_worth > self.max_net_worth:\n",
        "          self.max_net_worth = self.net_worth\n",
        "          reward += 1000\n",
        "        else:\n",
        "          reward -= self.commission_perc\n",
        "\n",
        "        \n",
        "        self._offset += 1\n",
        "\n",
        "        return reward \n",
        "\n",
        "    def _cur_close(self):\n",
        "      \"\"\"\n",
        "      Calculate real close price for the current bar\n",
        "      \"\"\"\n",
        "      open = self.data.open[self._offset]\n",
        "      rel_close = self.data.close[self._offset]\n",
        "      return open * (1.0 + rel_close)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Execute one time step within the environment\n",
        "        reward = self._take_action(action)\n",
        "\n",
        "        \n",
        "        if self._offset >= self.data.close.shape[0]-1 or \\\n",
        "        self.net_worth >= MAX_ACCOUNT_BALANCE \\\n",
        "        or self.net_worth <=0:\n",
        "          done=True\n",
        "        else:\n",
        "          done=False\n",
        "\n",
        "\n",
        "        obs = self._next_observation()\n",
        "\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "    def _reset(self, offset):\n",
        "        # Reset the state of the environment to an initial state\n",
        "        self.balance = INITIAL_ACCOUNT_BALANCE\n",
        "        self.net_worth = INITIAL_ACCOUNT_BALANCE\n",
        "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
        "        self.shares_held = 0\n",
        "        self.total_shares_sold = 0\n",
        "        self.total_sales_value = 0\n",
        "        self.total_shares_bought = 0\n",
        "        self._offset = offset\n",
        "        self.trades = []\n",
        "\n",
        "\n",
        "    def _render_to_file(self, filename='render.txt'):\n",
        "      f = open(filename, 'a+')\n",
        "      f.write(f\"Step: {self._offset}\\n\")\n",
        "      f.write(f\"Balence: {self.balance}\\n\")\n",
        "      f.write(f\"Amount Held: {self.shares_held}\\n\") \n",
        "      f.write(f\"Amount Sold: {self.total_shares_sold}\\n)\")\n",
        "      f.write(f\"Amount Bought: {self.total_shares_bought}\\n\")\n",
        "      #add some more\n",
        "      f.close()\n",
        "\n",
        "    def render(self, mode='file', title=\"Agent's Trading Screen\", **kwargs):\n",
        "      # Render the environment to the screen\n",
        "      if mode == 'file':\n",
        "        self._render_to_file(kwargs.get('filename', 'render.txt'))\n",
        "      elif mode == 'live':\n",
        "        if self.visualization == None:\n",
        "          self.visualization = StockTradingGraph(self.data, title)\n",
        "        \n",
        "        self.visualization.render(self._offset, self.net_worth,\n",
        "                                    self.trades, window_size=LOOKBACK_WINDOW_SIZE)\n",
        "\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "      self.np_random, seed1 = seeding.np_random(seed)\n",
        "      seed2 = seeding.hash_seed(seed1+1) % 2**33\n",
        "      return [seed1, seed2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szUR1sYHHEVl"
      },
      "outputs": [],
      "source": [
        "# Training data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/StockMarketData/archive/Data/ETFs/spy.us.txt')\n",
        "df = df.sort_values('Date')\n",
        "data=df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "# year data of year-month-day form\n",
        "dt = data['Date'].array\n",
        "# calculating relative prices \n",
        "rh = (data['High'].values-data['Open'].values)/data['Open'].values\n",
        "rl = (data['Low'].values-data['Open'].values)/data['Open'].values\n",
        "rc = (data['Close'].values-data['Open'].values)/data['Open'].values\n",
        "o = data['Open'].values\n",
        "# volumne data\n",
        "vol = data['Volume'].values\n",
        "\n",
        "Data = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume'])\n",
        "data=Data(date=dt,high=rh, low=rl, close=rc, open=o, volume=vol)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TdubrWOfamD"
      },
      "outputs": [],
      "source": [
        "# Testing data\n",
        "test = pd.read_csv('/content/drive/MyDrive/Datasets/StockMarketData/test.csv')\n",
        "t_df = test.sort_values('Date')\n",
        "data_two=t_df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "# year data of year-month-day form\n",
        "dt = data_two['Date'].array\n",
        "# calculating relative prices \n",
        "rh = (data_two['High'].values-data_two['Open'].values)/data_two['Open'].values\n",
        "rl = (data_two['Low'].values-data_two['Open'].values)/data_two['Open'].values\n",
        "rc = (data_two['Close'].values-data_two['Open'].values)/data_two['Open'].values\n",
        "o = data_two['Open'].values\n",
        "# volumne data\n",
        "vol = data_two['Volume'].values\n",
        "\n",
        "Data_two = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume'])\n",
        "test_data=Data_two(date=dt,high=rh, low=rl, close=rc, open=o, volume=vol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg2HNywdyLaO"
      },
      "source": [
        "# Training and Validation Portion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAJYlkEqIwcK"
      },
      "outputs": [],
      "source": [
        "# choosing between the two RL algorithms of DDPG or SAC\n",
        "MODEL = \"SAC\"\n",
        "# number of learning steps to train RL model\n",
        "MAX_STEPS = 1e4\n",
        "\n",
        "# create evaluation env that takes in test data for validation\n",
        "eval_env = StockTradingEnv(test_data, random_ofs_on_reset=True)\n",
        "# use deterministic actions for evaluation callback\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=f'/content/drive/MyDrive/RLmodels/best{MODEL}/',\n",
        "                             log_path='/content/drive/MyDrive/RLmodels/logs/', eval_freq=500,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "# create training env that takes in training data for training\n",
        "env = StockTradingEnv(data, random_ofs_on_reset=False)\n",
        "if MODEL == \"DDPG\":\n",
        "  # The noise objects for DDPG\n",
        "  n_actions = env.action_space.shape[-1]\n",
        "  action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "  model = DDPG(\"MlpPolicy\", env, action_noise=action_noise)\n",
        "else:\n",
        "  model = SAC(\"MlpPolicy\", env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6sMSFIuhLzx",
        "outputId": "515749ba-07f4-45c9-ae1d-7a3adc893d5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=500, episode_reward=93195.34 +/- 24051.91\n",
            "Episode length: 558.80 +/- 165.92\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1000, episode_reward=73596.62 +/- 51722.54\n",
            "Episode length: 412.00 +/- 267.05\n",
            "Eval num_timesteps=1500, episode_reward=97594.77 +/- 56412.15\n",
            "Episode length: 620.20 +/- 336.69\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2000, episode_reward=138393.21 +/- 25717.23\n",
            "Episode length: 817.00 +/- 165.47\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2500, episode_reward=95395.06 +/- 50120.34\n",
            "Episode length: 589.60 +/- 243.96\n",
            "Eval num_timesteps=3000, episode_reward=73195.52 +/- 58840.97\n",
            "Episode length: 521.40 +/- 324.40\n",
            "Eval num_timesteps=3500, episode_reward=122593.94 +/- 47866.83\n",
            "Episode length: 728.20 +/- 278.29\n",
            "Eval num_timesteps=4000, episode_reward=65196.10 +/- 55722.70\n",
            "Episode length: 455.60 +/- 372.44\n",
            "Eval num_timesteps=4500, episode_reward=86995.45 +/- 47123.05\n",
            "Episode length: 542.00 +/- 290.96\n",
            "Eval num_timesteps=5000, episode_reward=77996.06 +/- 47550.17\n",
            "Episode length: 471.60 +/- 239.59\n",
            "Eval num_timesteps=5500, episode_reward=72196.13 +/- 50702.79\n",
            "Episode length: 459.20 +/- 239.31\n",
            "Eval num_timesteps=6000, episode_reward=83995.74 +/- 67450.85\n",
            "Episode length: 510.40 +/- 353.61\n",
            "Eval num_timesteps=6500, episode_reward=100594.49 +/- 54640.48\n",
            "Episode length: 651.20 +/- 301.98\n",
            "Eval num_timesteps=7000, episode_reward=58397.04 +/- 44439.06\n",
            "Episode length: 354.60 +/- 283.70\n",
            "Eval num_timesteps=7500, episode_reward=100194.52 +/- 47559.50\n",
            "Episode length: 648.20 +/- 330.25\n",
            "Eval num_timesteps=8000, episode_reward=100194.34 +/- 44912.52\n",
            "Episode length: 666.00 +/- 304.45\n",
            "Eval num_timesteps=8500, episode_reward=87194.21 +/- 71582.44\n",
            "Episode length: 666.20 +/- 435.30\n",
            "Eval num_timesteps=9000, episode_reward=92994.10 +/- 58611.01\n",
            "Episode length: 682.80 +/- 371.88\n",
            "Eval num_timesteps=9500, episode_reward=82395.04 +/- 70723.07\n",
            "Episode length: 578.00 +/- 426.02\n",
            "Eval num_timesteps=10000, episode_reward=101794.52 +/- 68493.56\n",
            "Episode length: 649.60 +/- 359.73\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.sac.sac.SAC at 0x7fc23132ea50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model.learn(total_timesteps=MAX_STEPS, callback=eval_callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS1VdLn8yHB_"
      },
      "source": [
        "# Prediction and Rendering Environment Portion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpOBmCDTKyEB"
      },
      "outputs": [],
      "source": [
        "seed = np.random.randint(len(test_data.date))\n",
        "\n",
        "if MODEL == \"DDPG\":\n",
        "  model = DDPG.load(\"/content/drive/MyDrive/RLmodels/bestDDPG/best_model.zip\")\n",
        "  obs = eval_env.reset()\n",
        "  for i in range(seed):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, done, info = eval_env.step(action)\n",
        "    eval_env.render()\n",
        "    if done:\n",
        "      break\n",
        "else:\n",
        "  model = SAC.load(\"/content/drive/MyDrive/RLmodels/bestSAC/best_model.zip\")\n",
        "  obs = eval_env.reset()\n",
        "  for i in range(seed):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, done, info = eval_env.step(action)\n",
        "    eval_env.render()\n",
        "    if done:\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# taken from https://stackoverflow.com/questions/44947505/how-to-make-a-movie-out-of-images-in-python\n",
        "image_files = [os.path.join(image_folder,img)\n",
        "               for img in os.listdir(image_folder)\n",
        "               if img.endswith(\".png\")]\n",
        "clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(image_files, fps=fps)\n",
        "clip.write_videofile('agent_trading.mp4')"
      ],
      "metadata": {
        "id": "Hp59hzzGC-vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lHH0sY5lhLQ"
      },
      "outputs": [],
      "source": [
        "# taken from https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=8nj5sjsk15IT\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('agent_trading.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Vr_aCGPlvbP"
      },
      "outputs": [],
      "source": [
        "show_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpIyXz52nYs4"
      },
      "outputs": [],
      "source": [
        "!rm -r frames/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1BnRqmo4sw9dhUsiP_WO4HGPM9M7Cryi7",
      "authorship_tag": "ABX9TyNLjeerepQ3/KBWzhu9vzaK",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}