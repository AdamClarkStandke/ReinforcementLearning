{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aCStandke/ReinforcementLearning/blob/main/ThirdStockEnivornment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TBwoqXizywK"
      },
      "source": [
        "# Third Stock Trading Environment\n",
        "\n",
        "\n",
        "  This third stock trading environment is based on Adam King's article as found here:[Creating Bitcoin trading bots don’t lose money](https://medium.com/towards-data-science/creating-bitcoin-trading-bots-that-dont-lose-money-2e7165fb0b29). Similar to the first stock trading environment based on Maxim Lapan's implementation as found in chapter eight of his book [Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition](https://www.amazon.com/Deep-Reinforcement-Learning-Hands-optimization/dp/1838826998), the agent is trading in the environment of the [SPY ETF](https://www.etf.com/SPY?L=1) except in this trading environment the agent is tasked with two discrete actions of not only buying, selling or holding shares but also tasked with determining the amount to buy/sell ranging from 1 to 100 (which will be converted into pecentage form i.e. 1/100=1%, 100/100=100%) based on its trading account/balance [trading account](https://www.investopedia.com/terms/t/tradingaccount.asp#:~:text=A%20trading%20account%20is%20an,margin%20requirements%20set%20by%20FINRA.).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OHO1Q4dS9tF0"
      },
      "outputs": [],
      "source": [
        "# ignore warning messages because they are annoying lol\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGbbfGz0KsDS"
      },
      "source": [
        "# Installing Necessary Package for Training the Trading Agent\n",
        "\n",
        "To train the Trading Agent the package [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/index.html) was used. As stated in the docs: \n",
        "> Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines. And steems from the paper [Stable-Baselines3: Reliable Reinforcement Learning Implementations](https://jmlr.org/papers/volume22/20-1364/20-1364.pdf)\n",
        "The algorithms in this package will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\n",
        "\n",
        "---\n",
        "## Proximal Policy Optimization(PPO):\n",
        "\n",
        "Because in this environment the Agent will be executing continous actions, the Proximal Policy Optimization(PPO) algorithm was chosen. As detailed by the authors [PPO](https://arxiv.org/pdf/1707.06347.pdf)\n",
        "\n",
        "\n",
        "> We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically).\n",
        "\n",
        "\n",
        "PPO uses the following novel objective function:\n",
        "\n",
        "$L^{CLIP}(θ)=\\hat{E}_t[min(r_{t}(θ)\\hat{A}_t,clip(r_{t}(θ), 1-ϵ, 1+ϵ)\\hat{A}_t]$\n",
        "\n",
        "*  $\\theta$ is the policy parameter\n",
        "*  $\\hat{E}_t$ denotes the empirical expectation over timesteps\n",
        "*  $r_{t}$ is the ratio of the probability under the new and old policies, respectively\n",
        "*  $\\hat{A}_t$ is the estimated advantage at time t\n",
        "*  $\\epsilon$ is the clipping hyperparameter, usually 0.1 or 0.2\n",
        "\n",
        "\n",
        "As detailed by the authors [openAI](https://openai.com/blog/openai-baselines-ppo/#ppo)\n",
        "\n",
        "\n",
        "> This objective implements a way to do a Trust Region update which is compatible with Stochastic Gradient Descent, and simplifies the algorithm by removing the KL penalty and need to make adaptive updates. In tests, this algorithm has displayed the best performance on continuous control tasks and almost matches ACER’s performance on Atari, despite being far simpler to implement\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqZJWIuzLFDb"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3[extra]\n",
        "!pip install empyrical\n",
        "!pip install optuna\n",
        "!pip install --upgrade importlib-metadata==4.13.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AoLhA3_b_XeK"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import gym \n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import datetime as dt\n",
        "import optuna\n",
        "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.utils import constant_fn\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.env_util import DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_checker import VecCheckNan, check_env\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from empyrical import sortino_ratio, calmar_ratio, omega_ratio\n",
        "import sqlite3\n",
        "from sqlite3 import Error\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "import datetime\n",
        "from sklearn import preprocessing\n",
        "import math\n",
        "import os\n",
        "import csv\n",
        "from csv import DictWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Third Stock Environment\n",
        "\n"
      ],
      "metadata": {
        "id": "rB2onijSZMkg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OB9FxIN_AQC4"
      },
      "outputs": [],
      "source": [
        "# stock environment parameters\n",
        "MAX_ACCOUNT_BALANCE = 2147483647\n",
        "MAX_NUM_SHARES = 2147483647\n",
        "MAX_SHARE_PRICE = 4294967295\n",
        "LOOKBACK_WINDOW_SIZE = 10\n",
        "MAX_STEPS = 20000\n",
        "INITIAL_ACCOUNT_BALANCE = 10000\n",
        "# default percentage of stock price trading agent pays broker when \n",
        "# buying/selling, default is 0.1% (i.e. very reasonable)\n",
        "DA_COMMISION = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dJiMYPDIAj3y"
      },
      "outputs": [],
      "source": [
        "# Stock/ETF Trading Enviornment\n",
        "class StockTradingEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, data, reward_func='BalenceReward', random=True):\n",
        "        super(StockTradingEnv, self).__init__()\n",
        "        self.data = data\n",
        "        self.scale = preprocessing.MinMaxScaler()\n",
        "        self.random_ofs_on_reset = random\n",
        "        self.reward_func = reward_func\n",
        "        self.bars_count = LOOKBACK_WINDOW_SIZE\n",
        "        self.commission = DA_COMMISION\n",
        "        self.hold= False\n",
        "\n",
        "        # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
        "        self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([3, 1]), dtype=np.float32)\n",
        "\n",
        "        # Prices contains the OHCL values for the last five prices the state \n",
        "        # space is 12 dim i.e. 6 x 2 = 12\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=1, shape=self.shape, dtype=np.float32)\n",
        "        \n",
        "        self.seed()\n",
        "\n",
        "    def reset(self):\n",
        "      # random offset portion \n",
        "      bars = self.bars_count\n",
        "      if self.random_ofs_on_reset:\n",
        "        offset = self.np_random.choice(self.data.high.shape[0]-bars*10)+bars\n",
        "      else:\n",
        "        offset = bars\n",
        "      self._reset(offset)\n",
        "      return self._next_observation()\n",
        "\n",
        "    def _reset(self, offset):\n",
        "      self.trades = []\n",
        "      self.balance = INITIAL_ACCOUNT_BALANCE\n",
        "      self.netWorth = INITIAL_ACCOUNT_BALANCE\n",
        "      self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
        "      self.standkeMaxBenchShares = 0\n",
        "      self.shares_held  = 0\n",
        "      self._offset = offset\n",
        "      # setting account history portion\n",
        "      self.account_history = np.repeat([[self.netWorth/MAX_ACCOUNT_BALANCE]], LOOKBACK_WINDOW_SIZE, axis=1)\n",
        "\n",
        "    # shape of observation space is 2D\n",
        "    @property\n",
        "    def shape(self):\n",
        "      return (6, self.bars_count)\n",
        "\n",
        "    def _next_observation(self):\n",
        "      res = np.zeros(shape=(6, self.bars_count), dtype=np.float32)\n",
        "      ofs = self.bars_count-1\n",
        "      res[0] = self.data.volume[self._offset-ofs:self._offset+1]\n",
        "      res[1] = self.data.high[self._offset-ofs:self._offset+1]\n",
        "      res[2] = self.data.low[self._offset-ofs:self._offset+1]\n",
        "      res[3] = self.data.open[self._offset-ofs:self._offset+1]\n",
        "      res[4] = self.account_history[0][-self.bars_count:]\n",
        "      res[5] = self.data.close[self._offset-ofs:self._offset+1]\n",
        "      res = np.float32(res)\n",
        "      return res\n",
        "       \n",
        "    def _take_action(self, action):\n",
        "      reward = 0\n",
        "      current_price = self._cur_close()\n",
        "      action_type = action[0]\n",
        "      amount = action[1]\n",
        "      \n",
        "      shares_bought = 0\n",
        "      shares_sold = 0\n",
        "      additional_cost = 0\n",
        "      sales = 0\n",
        "\n",
        "\n",
        "      if action_type < 1 :\n",
        "        # Buy amount % of balance in shares\n",
        "        total_possible = self.balance / (current_price * (1+self.commission))\n",
        "        shares_bought = total_possible * amount\n",
        "        additional_cost = shares_bought * current_price * (1+self.commission)\n",
        "        self.balance -= additional_cost\n",
        "        self.standkeMaxBenchShares += shares_bought\n",
        "        self.shares_held += shares_bought\n",
        "        \n",
        "        \n",
        "        # visualization portion\n",
        "        if shares_bought > 0:\n",
        "          self.trades.append({'step': self._offset, 'shares': shares_bought, \n",
        "                              'total': additional_cost, 'type': \"buy\"})\n",
        "          \n",
        "          \n",
        "      elif action_type < 2:\n",
        "        # Sell amount % of shares held\n",
        "        shares_sold = self.shares_held * amount  \n",
        "        sales = shares_sold * current_price * (1 - self.commission)\n",
        "        self.balance += sales\n",
        "        self.standkeMaxBenchShares -= shares_sold\n",
        "        self.shares_held -= shares_sold\n",
        "        \n",
        "\n",
        "        # visualization portion\n",
        "        if shares_sold > 0:\n",
        "          self.trades.append({'step': self._offset, 'shares': -shares_sold, \n",
        "                                  'total': shares_sold * current_price, 'type': \"sell\"})  \n",
        "          \n",
        "      \n",
        "      self.netWorth = self.balance + self.shares_held * current_price\n",
        "      \n",
        "      if self.netWorth > self.max_net_worth:\n",
        "        self.max_net_worth = self.netWorth\n",
        "\n",
        "      # updating account history\n",
        "      self.account_history = np.append(self.account_history, [[self.netWorth/MAX_ACCOUNT_BALANCE]], axis=1)\n",
        "      # reward Calculations\n",
        "      returns = self.account_history[0][-self.bars_count:]\n",
        "      if self.reward_func == 'BalenceReward':\n",
        "        delay_modifier = (self._offset / MAX_STEPS)\n",
        "        reward = self.balance * delay_modifier\n",
        "      elif self.reward_func == 'sortinoRewardRatio':\n",
        "        ratio = sortino_ratio(returns, period=\"daily\")\n",
        "        reward= ratio * self.balance\n",
        "      elif self.reward_func == 'calmarRewardRatio':\n",
        "        ratio = calmar_ratio(returns, period=\"daily\")\n",
        "        reward= ratio * self.balance\n",
        "      elif self.reward_func == 'omegaRewardRatio':\n",
        "        ratio = omega_ratio(returns,  annualization=self.bars_count)\n",
        "        reward= ratio * self.balance\n",
        "      elif self.reward_func == 'StandkeCurrentValueReward':\n",
        "        prev_net = returns[-2]\n",
        "        current_net = returns[-1]\n",
        "        ratio = current_net-prev_net\n",
        "        reward = ratio * self.balance\n",
        "      elif self.reward_func == 'StandkeSmallDrawDownReward':\n",
        "        mx = np.max(returns)\n",
        "        mi = np.min(returns)\n",
        "        ratio = round(abs(mx-mi/mx), 1) \n",
        "        reward = ratio * self.balance\n",
        "      elif self.reward_func == 'StandkeSumofDifferenceReward':\n",
        "        ratio = np.sum(np.diff(returns))\n",
        "        reward = ratio * self.balance\n",
        "      else:\n",
        "        ratio = np.mean(returns)\n",
        "        reward = ratio * self.balance\n",
        "      return reward if abs(reward) != np.inf and not np.isnan(reward) else 0\n",
        "\n",
        "      \n",
        "    def _cur_close(self):\n",
        "      \"\"\"\n",
        "      Calculate real close price for the current bar\n",
        "      \"\"\"\n",
        "      return self.data.real_close[self._offset]\n",
        "\n",
        "    def step(self, action):\n",
        "      # Execute one time step within the environment\n",
        "      reward = self._take_action(action)\n",
        "    \n",
        "      self._offset += 1\n",
        "\n",
        "      if self._offset >= self.data.close.shape[0]-1 or self.netWorth <= 0 or self.netWorth>=MAX_ACCOUNT_BALANCE:\n",
        "        done=True\n",
        "      else:\n",
        "        done=False\n",
        "  \n",
        "      obs = self._next_observation()\n",
        "\n",
        "      info = {\"Net Worth\":self.netWorth, \"reward\": reward}\n",
        "      \n",
        "      return obs, reward, done, info\n",
        "\n",
        "    def _render_to_file(self, filename='results.csv'):\n",
        "      csv_columns = ['Date','Net_Worth','Balence', 'StandkeMaxBenchShares']\n",
        "      dict_data = {'Date':self.data.date[self._offset], 'Net_Worth':self.netWorth, 'Balence':self.balance, 'StandkeMaxBenchShares':self.standkeMaxBenchShares}\n",
        "      with open(filename, 'a+', newline='') as f:\n",
        "        writer = DictWriter(f, fieldnames=csv_columns)\n",
        "        writer.writerow(dict_data)\n",
        "        f.close()\n",
        " \n",
        "    def render(self, mode='file', title=\"Agent's Trading Screen\", **kwargs):\n",
        "      # Render the environment to the screen\n",
        "      if mode == 'file':\n",
        "        self._render_to_file()\n",
        "\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "      self.np_random, seed1 = seeding.np_random(seed)\n",
        "      seed2 = seeding.hash_seed(seed1+1) % 2**33\n",
        "      return [seed1, seed2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "1.   First the data is made [stationary](https://machinelearningmastery.com/remove-trends-seasonality-difference-transform-python/) to remove any trends or seasonality associated with the time series data\n",
        "2.   Then the price data is converted into releative prices to model  the relative change rather than absolute change \n",
        "3. Lastly the data is normalized using sklearn's [min-max scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) so as to fit within the environment's observation space of [0,1]\n"
      ],
      "metadata": {
        "id": "-wu_j-g5YIA4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "szUR1sYHHEVl"
      },
      "outputs": [],
      "source": [
        "# using sklearn's min-max scaler for the relative high and low\n",
        "x=preprocessing.MinMaxScaler()\n",
        "\n",
        "# create a differenced series as done in step 1 (see link for more info)\n",
        "def difference(dataset, interval=1):\n",
        "\tdiff = list()\n",
        "\tfor i in range(interval, len(dataset)):\n",
        "\t\tvalue = np.log(dataset[i]) - np.log(dataset[i - interval])\n",
        "\t\tdiff.append(value)\n",
        "\treturn diff\n",
        " \n",
        "# training data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/StockMarketData/archive/Data/ETFs/spy.us.txt')\n",
        "df = df.sort_values('Date')\n",
        "data=df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "# making OHLC data stationary before calculating relative and normalizing \n",
        "diff_o = np.array(difference(data['Open'], 1))\n",
        "diff_h = np.array(difference(data['High'], 1))\n",
        "diff_l = np.array(difference(data['Low'], 1))\n",
        "diff_c = np.array(difference(data['Close'], 1))\n",
        "# volumne data\n",
        "vol = data['Volume'].values/MAX_NUM_SHARES\n",
        "# year data of year-month-day form\n",
        "dt = data['Date'].array\n",
        "# calculating relative prices and normalizing data\n",
        "o =  (diff_o-diff_l)/(diff_h-diff_l)\n",
        "o =  x.fit_transform(o.reshape(-1,1)).reshape(-1)\n",
        "rc = (diff_c-diff_l)/(diff_h-diff_l)\n",
        "rc = x.fit_transform(rc.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "rh = x.fit_transform(diff_h.reshape(-1,1)).reshape(-1)\n",
        "rl = x.fit_transform(diff_l.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "Train_Data = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume', 'real_open',  'real_close', 'real_high', 'real_low', 'real_vol'])\n",
        "train = Train_Data(date=dt,high=rh, low=rl, close=rc, open=o, volume=vol, real_open=data['Open'].values, real_close=data['Close'].values, real_high=data['High'].values, real_low=data['Low'].values, real_vol=data['Volume'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1TdubrWOfamD"
      },
      "outputs": [],
      "source": [
        "# Testing data\n",
        "test = pd.read_csv('/content/drive/MyDrive/Datasets/StockMarketData/test.csv')\n",
        "t_df = test.sort_values('Date')\n",
        "data_two=t_df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "# making OHLC data stationary before calculating relative and normalizing \n",
        "diff_o = np.array(difference(data_two['Open'], 1))\n",
        "diff_h = np.array(difference(data_two['High'], 1))\n",
        "diff_l = np.array(difference(data_two['Low'], 1))\n",
        "diff_c = np.array(difference(data_two['Close'], 1))\n",
        "# volumne data\n",
        "vol = data_two['Volume'].values/MAX_NUM_SHARES\n",
        "# year data of year-month-day form\n",
        "dt = data_two['Date'].array\n",
        "# calculating relative prices and normalizing data\n",
        "o =  (diff_o-diff_l)/(diff_h-diff_l)\n",
        "o =  x.fit_transform(o.reshape(-1,1)).reshape(-1)\n",
        "rc = (diff_c-diff_l)/(diff_h-diff_l)\n",
        "rc = x.fit_transform(rc.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "rh = x.fit_transform(diff_h.reshape(-1,1)).reshape(-1)\n",
        "rl = x.fit_transform(diff_l.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "Test_Data = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume', 'real_open', 'real_close', 'real_high', 'real_low', 'real_vol'])\n",
        "test = Test_Data(date=dt,high=rh, low=rl, close=rc, open=o, volume=vol, real_open=data['Open'].values, real_close=data_two['Close'].values, real_high=data_two['High'].values, real_low=data_two['Low'].values, real_vol=data['Volume'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Seperate Policy/Value Network Class using a 1D CNN Feature extractor\n",
        "\n",
        "Stable-baselines3's lists the following blog on PPO [37 implementation details of PPO](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) which breaks down the differnt implementations of PPO. Furthermore, as the authors of [WHAT MATTERS FOR ON-POLICY DEEP ACTORCRITIC METHODS? A LARGE-SCALE STUDY](https://openreview.net/pdf?id=nIAxjsniDzg) detail: \n",
        "\n",
        "\n",
        "> Separate value and policy networks (C47) appear to lead to better performance on our out of five environments (Fig. 15). To avoid analyzing the other choices based on bad models, we thus focus for the rest of this experiment only on agents with separate value and policy networks. Regarding network sizes, the optimal width of the policy MLP depends on the complexity of the environment (Fig. 18) and too low or too high values can cause significant drop in performance while for the value function there seems to be no downside in using wider networks (Fig. 21). Moreover, on some environments it is beneficial to make the value network wider than the policy one, e.g. on HalfCheetah the best results are achieved with 16 − 32 units per layer in the policy network and 256\n",
        "in the value network. Two hidden layers appear to work well for policy (Fig. 22) and value networks (Fig. 20) in all tested environments. As for activation functions, we observe that tanh activations perform best and relu worst\n",
        "\n",
        "With this statement in mind, I decided to implement a seperate-network architecture for PPO. \n",
        "\n",
        "## 1D CNN Feature Extractor\n",
        "\n",
        "I decided to use two seperate 1D CNN Feature Extractors for the policy and value network. I decided upon the following architecture for the  Policy feature extractor: \n",
        "\n",
        "```\n",
        "self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input, 32, kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(32, 64, kernel_size=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.Flatten(),\n",
        "```\n",
        "And I decided upon the following architecture for the Value feature extractor:\n",
        "```\n",
        "self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input,128,kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 256, kernel_size=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "```\n",
        "I decided on the following architecure for the Policy Network: \n",
        "\n",
        "## Policy Network\n",
        "\n",
        "```\n",
        "  self.policy_net = nn.Sequential(\n",
        "            layer_init(nn.Linear(feature_dim, 32)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(32, last_layer_dim_pi), std=0.01),\n",
        "            nn.Tanh(),  \n",
        "        )\n",
        "```\n",
        "\n",
        "## Value Network \n",
        "\n",
        "And I decided on the following architecure for the Value Network: \n",
        "\n",
        "```\n",
        "self.value_net = nn.Sequential(\n",
        "           layer_init(nn.Linear(256, 256)),\n",
        "           nn.Tanh(),\n",
        "           layer_init(nn.Linear(256, 128)),\n",
        "           nn.Tanh(),\n",
        "           layer_init(nn.Linear(128,32)),\n",
        "           nn.Tanh(),\n",
        "           layer_init(nn.Linear(32,last_layer_dim_vf)),\n",
        "           nn.Tanh(),\n",
        "           )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_-5jadJfRbDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    th.nn.init.orthogonal_(layer.weight, std)\n",
        "    th.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class StandkePolicyExtractor(BaseFeaturesExtractor):\n",
        "  def __init__(self, observation_space=gym.spaces.Box, features_dim=128):\n",
        "        super(StandkePolicyExtractor, self).__init__(observation_space, features_dim)\n",
        "        input = observation_space.shape[0]\n",
        "        # Feature Extractor\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input,16,kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(16, 32, kernel_size=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(\n",
        "                th.as_tensor(observation_space.sample()[None]).float()\n",
        "            ).shape[1]\n",
        "  \n",
        "  def forward(self, observations):\n",
        "    return self.cnn(observations)\n",
        "\n",
        "class StandkeValueExtractor(BaseFeaturesExtractor):\n",
        "  def __init__(self, observation_space=gym.spaces.Box, features_dim=128):\n",
        "        super(StandkeValueExtractor, self).__init__(observation_space, features_dim)\n",
        "        input = observation_space.shape[0]\n",
        "        # Feature Extractor\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input,128,kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 256, kernel_size=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(\n",
        "                th.as_tensor(observation_space.sample()[None]).float()\n",
        "            ).shape[1]\n",
        "\n",
        "        \n",
        "  def forward(self, observations):\n",
        "    return self.cnn(observations)\n",
        "\n",
        "\n",
        "\n",
        "class StandkeNetwork(nn.Module):\n",
        "  def __init__(self,feature_dim=32, last_layer_dim_pi=2, last_layer_dim_vf=1):\n",
        "        super(StandkeNetwork, self).__init__()\n",
        "\n",
        "        # IMPORTANT:\n",
        "        # Save output dimensions, used to create the distributions\n",
        "        self.latent_dim_pi = last_layer_dim_pi\n",
        "        self.latent_dim_vf = last_layer_dim_vf\n",
        "\n",
        "         # Policy Network\n",
        "        self.policy_net = nn.Sequential(\n",
        "            layer_init(nn.Linear(feature_dim, 32)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(32, last_layer_dim_pi), std=0.01),\n",
        "            nn.Tanh(),  \n",
        "        )\n",
        "\n",
        "\n",
        "         # Value Network\n",
        "        self.value_net = nn.Sequential(\n",
        "           layer_init(nn.Linear(256, 256)),\n",
        "           nn.Tanh(),\n",
        "           layer_init(nn.Linear(256, 128)),\n",
        "           nn.Tanh(),\n",
        "           layer_init(nn.Linear(128,32)),\n",
        "           nn.Tanh(),\n",
        "           layer_init(nn.Linear(32,last_layer_dim_vf)),\n",
        "           nn.Tanh(),\n",
        "           )\n",
        "\n",
        "  def forward_actor(self, features: th.Tensor):\n",
        "    return self.policy_net(features)\n",
        "\n",
        "  def forward_critic(self, features: th.Tensor):\n",
        "    return self.value_net(features)\n",
        "  \n",
        "\n",
        "class StandkePolicy(ActorCriticPolicy):\n",
        "  def __init__(self, observation_space=gym.spaces.Box,action_space=gym.spaces.Box, lr_schedule=constant_fn(0.0003), activation_fn=nn.Tanh,*args,**kwargs):\n",
        "    \n",
        "        super(StandkePolicy, self).__init__(observation_space,action_space, lr_schedule, activation_fn,*args,**kwargs)\n",
        "        # non-shared features extractors \n",
        "        self.policy_features_extractor = StandkePolicyExtractor(observation_space)\n",
        "        self.value_features_extractor = StandkeValueExtractor(observation_space)\n",
        "        delattr(self, \"features_extractor\")  # remove the shared features extractor\n",
        "        # orthogonal initialization\n",
        "        self.ortho_init = False\n",
        "\n",
        "  def _build_mlp_extractor(self):\n",
        "    self.mlp_extractor = StandkeNetwork()\n",
        "\n",
        "  def extract_features(self, obs: th.Tensor):\n",
        "    policy_features = self.policy_features_extractor(obs)\n",
        "    value_features = self.value_features_extractor(obs)\n",
        "    return policy_features, value_features\n",
        "  \n",
        "  def forward(self, obs: th.Tensor, deterministic=False): \n",
        "    policy_features, value_features = self.extract_features(obs)\n",
        "    mu_pi = self.mlp_extractor.forward_actor(policy_features)\n",
        "    latent_vf = self.mlp_extractor.forward_critic(value_features)\n",
        "    # Evaluate the values for the given observations\n",
        "    distribution = self._get_action_dist_from_latent(mu_pi)\n",
        "    actions = distribution.get_actions(deterministic=deterministic)\n",
        "    log_prob = distribution.log_prob(actions)\n",
        "    vf = latent_vf\n",
        "    return actions, vf, log_prob\n",
        "\n",
        "  def evaluate_actions(self, obs: th.Tensor, actions: th.Tensor): \n",
        "    policy_features, value_features = self.extract_features(obs)\n",
        "    mu_pi = self.mlp_extractor.forward_actor(policy_features)\n",
        "    latent_vf = self.mlp_extractor.forward_critic(value_features)\n",
        "    distribution = self._get_action_dist_from_latent(mu_pi)\n",
        "    actions = distribution.get_actions(deterministic=False)\n",
        "    log_prob = distribution.log_prob(actions)\n",
        "    vf = latent_vf\n",
        "    return vf, log_prob, distribution.entropy()\n",
        "\n",
        "  def get_distribution(self, obs: th.Tensor):\n",
        "    policy_features, _ = self.extract_features(obs)\n",
        "    latent_pi = self.mlp_extractor.forward_actor(policy_features)\n",
        "    return self._get_action_dist_from_latent(latent_pi)\n",
        "\n",
        "  def predict_values(self, obs: th.Tensor):\n",
        "    _, value_features = self.extract_features(obs)\n",
        "    latent_vf = self.mlp_extractor.forward_critic(value_features)\n",
        "    return latent_vf"
      ],
      "metadata": {
        "id": "572gmNd6Q-kr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TensorboardCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(TensorboardCallback, self).__init__(verbose)\n",
        "        self.mean_reward = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "      self.mean_reward.append(self.locals['infos'][0]['reward'])\n",
        "      if (self.num_timesteps % 10000 == 0):\n",
        "        self.logger.record('Net Worth', self.locals['infos'][0]['Net Worth'])\n",
        "        self.logger.record('Mean Reward', np.mean(self.mean_reward))\n",
        "        self.mean_reward.clear()\n",
        "      return True"
      ],
      "metadata": {
        "id": "1hEB-iRM4GLz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HyperParmater Tuning \n",
        "\n",
        "Following the optimization sceme as outlined by Adam King in his article [Optimizing deep learning trading bots using state-of-the-art techniques](https://towardsdatascience.com/using-reinforcement-learning-to-trade-bitcoin-for-massive-profit-b69d0e8f583b) Bayesian Optimization was done using [Optuna](https://optuna.org/)\n",
        "After doing a categorical trial on the different rewards I have implemented, namely: \n",
        "* BalenceReward: simple reward that Adam King created that multiplies the balance by a delay which is just the offset/step in the environment\n",
        "* [sortinoRewardRatio](https://www.investopedia.com/terms/s/sortinoratio.asp) this ratio is multiplied by the balance  \n",
        "* [calmarRewardRatio](https://www.investopedia.com/terms/c/calmarratio.asp)this ratio is multiplied by the balance   \n",
        "* [omegaRewardRatio](https://www.wallstreetmojo.com/omega-ratio/) this ratio is multiplied by the balance  \n",
        "* StandkeCurrentValueReward: simple reward I created that is the difference of the previous trading day's networth and the current trading day's networth and is multiplied by the balance\n",
        "* StandkeSmallDrawDownReward: reward I created that takes the maximum and minimum networth of the past 10 trading days divided by the maximum value of the past 10 trading days and is multiplied by the balance \n",
        "* StandkeSumofDifferenceReward: simple reward I created that takes the difference of the past 10 trading days and sums the values before multiplying it by the balance\n",
        "\n",
        "BalenceReward was chosen as the defualt reward scheme for testing the hyperparmeters outlined by Adam King and the following hyperparmaters:\n",
        "\n",
        "!!!Still doing!!!!!!\n",
        "\n",
        "*  gamma: \n",
        "*  clip_range: \n",
        "*  clip_range_vf:\n",
        "*  ent_coef:\n",
        "*  vf_coef:\n",
        "*  target_kl: \n",
        "\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "LKGZpOQ39eNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_fn(trial):\n",
        "    # env_params = optimize_envs(trial) # just using default reward, test later\n",
        "    agent_params = optimize_ppo(trial)\n",
        "    \n",
        "    train_env = DummyVecEnv([lambda: StockTradingEnv(train, random=False)])\n",
        "    validation_env = DummyVecEnv([lambda: StockTradingEnv(test, random=False)])\n",
        "    model = PPO(StandkePolicy, train_env, **agent_params)\n",
        "    \n",
        "    model.learn(len(train_env.get_attr('data')[0].date)) # trains based on length of data \n",
        "                                                         # approx 3000\n",
        "    rewards, done = [], False\n",
        "    obs = validation_env.reset()\n",
        "    for i in range(len(validation_env.get_attr('data')[0].date)):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, reward, done, _ = validation_env.step(action)\n",
        "        if done:\n",
        "          break\n",
        "        if abs(reward) != np.inf and not np.isnan(reward):\n",
        "          rewards.append(reward)\n",
        "        else:\n",
        "          pass\n",
        "    return np.mean(rewards)\n",
        "\n",
        "def optimize_ppo(trial):\n",
        "    return {\n",
        "        'gamma': trial.suggest_float('gamma1', 0.1, 0.9, log=True),\n",
        "        'clip_range': trial.suggest_float('clip_range1', 0.1, 0.5, log=True),\n",
        "        'clip_range_vf': trial.suggest_float('clip_range_vf2', 0.1, 0.5, log=True),\n",
        "        'ent_coef':trial.suggest_float('ent_coef3', 0.1, 0.9, log=True), \n",
        "        'vf_coef':trial.suggest_float('vf_coef4', 0.1, 0.9, log=True),\n",
        "        'target_kl': trial.suggest_float('target_kl5', 0.01, 0.05, log=True),\n",
        "    }\n",
        "\n",
        "def optimize_envs(trial):\n",
        "    return {'reward_func': trial.suggest_categorical('reward_func', ['BalenceReward', 'sortinoRewardRatio', 'calmarRewardRatio', 'omegaRewardRatio', 'StandkeCurrentValueReward', 'StandkeSmallDrawDownReward', 'StandkeSumofDifferenceReward', 'Mean'])}\n",
        "\n",
        "\n",
        "study = optuna.create_study(study_name='StockEnvPPO_Parms', direction=\"maximize\", storage=\"sqlite:///PPOhyper.db\", load_if_exists=True)\n",
        "study.optimize(objective_fn, n_trials=5000, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "eskm_ga99dme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the hyperparmeters for training\n",
        "study = optuna.load_study(study_name='StockEnvPPO_Parms', storage='sqlite:///PPOhyper.db')\n",
        "params = study.best_trial.params"
      ],
      "metadata": {
        "id": "jMluKzP-E1u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg2HNywdyLaO"
      },
      "source": [
        "# Training and Validation Portion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of learning steps to train RL model is set to 100K\n",
        "MAX_STEPS = 1e5\n",
        "# the number of parallel environments for training  \n",
        "ENV = 1\n",
        "MODEL = \"StandkePV\"\n",
        "\n",
        "# hyperparameters to use for the env and agent\n",
        "env_params = {'reward_func': 'BalenceReward'}\n",
        "model_params = { \n",
        " 'gae_lambda': 0.9 \n",
        "}"
      ],
      "metadata": {
        "id": "wLYFzy2-HWYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAJYlkEqIwcK"
      },
      "outputs": [],
      "source": [
        "# create evaluation env that takes in test data that saves best model \n",
        "eval_env = DummyVecEnv([lambda: StockTradingEnv(test, **env_params, random=False)])\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=f'/content/drive/MyDrive/RLmodels/bestPPO/{MODEL}',\n",
        "                             log_path='/content/drive/MyDrive/RLmodels/logs/', eval_freq=MAX_STEPS/100,\n",
        "                             deterministic=False, render=False)\n",
        "\n",
        "# create training envs that takes in training data for training\n",
        "\n",
        "envs =  DummyVecEnv([lambda: StockTradingEnv(train, **env_params, random=False) for _ in range(0,ENV)])\n",
        "\n",
        "'''training model using the Seperate Standke Policy/Value network'''\n",
        "# optional additional keyword parameters to pass to model \n",
        "policy_kwargs = dict()\n",
        "model = PPO(StandkePolicy, envs, **model_params, verbose=1, tensorboard_log=f\"/content/PPO_SPY_tensorboard/{MODEL}\", policy_kwargs=policy_kwargs)\n",
        "\n",
        "# check to make sure no erros in the env, such as observation space errors or \n",
        "# nans\n",
        "check_env(StockTradingEnv(train))\n",
        "VecCheckNan(envs, raise_exception=True, check_inf=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General explanation of log output \n",
        "\n",
        "As detailed by araffin in his commit [Add explanation of logger output](https://github.com/DLR-RM/stable-baselines3/pull/803/files), for a given log block such as\n",
        "\n",
        "```\n",
        "-----------------------------------------\n",
        "  | eval/                   |             |\n",
        "  |    mean_ep_length       | 200         |\n",
        "  |    mean_reward          | -157        |\n",
        "  | rollout/                |             |\n",
        "  |    ep_len_mean          | 200         |\n",
        "  |    ep_rew_mean          | -227        |\n",
        "  | time/                   |             |\n",
        "  |    fps                  | 972         |\n",
        "  |    iterations           | 19          |\n",
        "  |    time_elapsed         | 80          |\n",
        "  |    total_timesteps      | 77824       |\n",
        "  | train/                  |             |\n",
        "  |    approx_kl            | 0.037781604 |\n",
        "  |    clip_fraction        | 0.243       |\n",
        "  |    clip_range           | 0.2         |\n",
        "  |    entropy_loss         | -1.06       |\n",
        "  |    explained_variance   | 0.999       |\n",
        "  |    learning_rate        | 0.001       |\n",
        "  |    loss                 | 0.245       |\n",
        "  |    n_updates            | 180         |\n",
        "  |    policy_gradient_loss | -0.00398    |\n",
        "  |    std                  | 0.205       |\n",
        "  |    value_loss           | 0.226       |\n",
        "  -----------------------------------------\n",
        "```\n",
        "``eval/`` \n",
        "- ``mean_ep_length``: Mean episode length\n",
        "- ``mean_reward``: Mean episodic reward (during evaluation)\n",
        "``rollout/``\n",
        "- ``ep_len_mean``: Mean episode length (averaged over 100 episodes)\n",
        "- ``ep_rew_mean``: Mean episodic training reward (averaged over 100 episodes)\n",
        "``time/``\n",
        "- ``episodes``: Total number of episodes\n",
        "- ``fps``: Number of frames per seconds (includes time taken by gradient update)\n",
        "- ``iterations``: Number of iterations (data collection + policy update for A2C/PPO)\n",
        "- ``time_elapsed``: Time in seconds since the beginning of training\n",
        "- ``total_timesteps``: Total number of timesteps (steps in the environments)\n",
        "``train/``\n",
        "- ``entropy_loss``: Mean value of the entropy loss (negative of the average policy entropy). \n",
        "  * ⚠**According to the formula as detailed [model](https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/ppo2/model.py#L91) on line 91, if ent_coef is 0 this term should not matter which is the default hyperparamter setting; difficult to interpret for this env due to it being negative**⚠\n",
        "  * **Furthermore according to [The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) which cites [Andrychowicz, et al. (2021)](https://openreview.net/forum?id=nIAxjsniDzg) overall find no evidence that the entropy term improves performance on continuous control environments (decision C13, figure 76 and 77)**\n",
        "- ``clip_fraction``: mean fraction of surrogate loss that was clipped (above clip_range threshold) for PPO.\n",
        "- ``clip_range``: Current value of the clipping factor for the surrogate loss of PPO\n",
        "- ``entropy_loss``: Mean value of the entropy loss (negative of the average policy entropy)\n",
        "    *  want the entropy to be decreasing slowly and smoothly over the course of training, as the agent trades exploration in favor of exploitation.\n",
        "- ``learning_rate``: Current learning rate value\n",
        "- ``n_updates``: Number of gradient updates applied so far\n",
        "- ``policy_gradient_loss``: Current value of the policy gradient loss (its value does not have much meaning)(lol I did not say this 😸)\n",
        "- ``std``: Current standard deviation of the noise when using generalized State-Dependent Exploration (gSDE) (which by default is not used)\n",
        "\n",
        "# Important Training Metrics to Focus On!!!! ✅✅✅✅✅✅✅✅✅\n",
        "- ``approx_kl``: approximate mean KL divergence between old and new policy (for PPO), it is an estimation of how much change happened in the update (i.e. information gain or loss)\n",
        "  * **Want this value to SMOOTHLY DECREASE during training and be as close as possible to 0**\n",
        "  * **Should be DECREASING**\n",
        "- ``explained_variance``: Fraction of the return variance explained by the value function. This metric calculates how good the value function is as a predicator of future rewards\n",
        "  * **Want this value to be as close as possible to 1 (i.e.perfect predictions) during training rather than less than or equal to 0 (i.e. no predictive power)**\n",
        "  * **Should be INCREASING**\n",
        "- ``loss``: called total loss is the the overall loss function\n",
        "  * **Want to MINIMIZE this during training** \n",
        "  * **Should be DECREASING**\n",
        "- ``value_loss``: error that value function is incurring \n",
        "  *   **Want to MINIMIZE this during training to 0 (though as discussed this isn't always possible due to randomness)**\n",
        "  * **Should be DECREASING**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9ATRRLz4RsLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6sMSFIuhLzx"
      },
      "outputs": [],
      "source": [
        "model.learn(total_timesteps=MAX_STEPS, callback=[eval_callback,TensorboardCallback()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction and Printout of Agent's Trading Strategy on Test Data"
      ],
      "metadata": {
        "id": "U4ClWZ6ZQqbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO.load(f\"/content/drive/MyDrive/RLmodels/bestPPO/{MODEL}/best_model.zip\")\n",
        "env = StockTradingEnv(test, **env_params, random=False)\n",
        "obs = env.reset()\n",
        "for i in range(len(test.date)):\n",
        "  action, _states = model.predict(obs, deterministic=False)\n",
        "  obs, rewards, done, info = env.step(action)\n",
        "  env.render()\n",
        "  if done:\n",
        "    break"
      ],
      "metadata": {
        "id": "BgbcsONJQqI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorBoard Analysis"
      ],
      "metadata": {
        "id": "hqTY07IM0Dyx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n02iHgNDTvYJ"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/PPO_SPY_tensorboard/ "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1BnRqmo4sw9dhUsiP_WO4HGPM9M7Cryi7",
      "authorship_tag": "ABX9TyO4MqTl11bz/BoRCUF4pbXW",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}