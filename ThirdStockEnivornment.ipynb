{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aCStandke/ReinforcementLearning/blob/main/ThirdStockEnivornment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TBwoqXizywK"
      },
      "source": [
        "# Third Stock Trading Environment\n",
        "\n",
        "\n",
        "  This third stock trading environment is based on Adam King's article as found here:[Creating Bitcoin trading bots don’t lose money](https://medium.com/towards-data-science/creating-bitcoin-trading-bots-that-dont-lose-money-2e7165fb0b29). Similar to the first stock trading environment based on Maxim Lapan's implementation as found in chapter eight of his book [Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition](https://www.amazon.com/Deep-Reinforcement-Learning-Hands-optimization/dp/1838826998), the agent is trading in the environment of the [SPY ETF](https://www.etf.com/SPY?L=1) except in this trading environment the agent is tasked with two discrete actions of not only buying, selling or holding shares but also tasked with determining the amount to buy/sell ranging from 1 to 100 (which will be converted into pecentage form i.e. 1/100=1%, 100/100=100%) based on its trading account/balance [trading account](https://www.investopedia.com/terms/t/tradingaccount.asp#:~:text=A%20trading%20account%20is%20an,margin%20requirements%20set%20by%20FINRA.).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OHO1Q4dS9tF0"
      },
      "outputs": [],
      "source": [
        "# ignore warning messages because they are annoying lol\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGbbfGz0KsDS"
      },
      "source": [
        "# Installing Necessary Package for Training the Trading Agent\n",
        "\n",
        "To train the Trading Agent the package [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/index.html) was used. As stated in the docs: \n",
        "> Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines. And steems from the paper [Stable-Baselines3: Reliable Reinforcement Learning Implementations](https://jmlr.org/papers/volume22/20-1364/20-1364.pdf)\n",
        "The algorithms in this package will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\n",
        "\n",
        "---\n",
        "## Proximal Policy Optimization(PPO):\n",
        "\n",
        "Because in this environment the Agent will be executing continous actions, the Proximal Policy Optimization(PPO) algorithm was chosen. As detailed by the authors [PPO](https://arxiv.org/pdf/1707.06347.pdf)\n",
        "\n",
        "\n",
        "> We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically).\n",
        "\n",
        "\n",
        "PPO uses the following novel objective function:\n",
        "\n",
        "$L^{CLIP}(θ)=\\hat{E}_t[min(r_{t}(θ)\\hat{A}_t,clip(r_{t}(θ), 1-ϵ, 1+ϵ)\\hat{A}_t]$\n",
        "\n",
        "*  $\\theta$ is the policy parameter\n",
        "*  $\\hat{E}_t$ denotes the empirical expectation over timesteps\n",
        "*  $r_{t}$ is the ratio of the probability under the new and old policies, respectively\n",
        "*  $\\hat{A}_t$ is the estimated advantage at time t\n",
        "*  $\\epsilon$ is the clipping hyperparameter, usually 0.1 or 0.2\n",
        "\n",
        "\n",
        "As detailed by the authors [openAI](https://openai.com/blog/openai-baselines-ppo/#ppo)\n",
        "\n",
        "\n",
        "> This objective implements a way to do a Trust Region update which is compatible with Stochastic Gradient Descent, and simplifies the algorithm by removing the KL penalty and need to make adaptive updates. In tests, this algorithm has displayed the best performance on continuous control tasks and almost matches ACER’s performance on Atari, despite being far simpler to implement\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqZJWIuzLFDb",
        "outputId": "2f08291a-f44f-4a0a-80da-bb3a9ba06b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.5.0)\n",
            "Requirement already satisfied: gym==0.21 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.21.0)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.4.2)\n",
            "Requirement already satisfied: ale-py==0.7.4 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.7.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: protobuf~=3.19.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.8.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (4.12.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4->stable-baselines3[extra]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4->stable-baselines3[extra]) (4.1.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.48.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2022.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HDwRKzPLJWG"
      },
      "source": [
        "# Installing the Necessary Packages for Visualizing the Trading Agent's Envirnoment on Google Colab Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT_p1bNSDnJ_",
        "outputId": "bd686b01-c837-4e26-96c1-1ffe457fdc86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mpl_finance in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mpl_finance) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mpl_finance) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mpl_finance) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (0.2.3.5)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy) (1.21.6)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (2.9.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.64.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageio_ffmpeg in /usr/local/lib/python3.7/dist-packages (0.4.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install mpl_finance #used for plotting the candelstick graph\n",
        "!pip install moviepy #\n",
        "!pip install imageio_ffmpeg #\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1 #used to create a display for vm\n",
        "!apt-get install x11-utils > /dev/null 2>&1 #\n",
        "!pip install pyglet==v1.3.2 > /dev/null 2>&1 #\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1 #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AoLhA3_b_XeK"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib import style\n",
        "import json\n",
        "import datetime as dt\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.env_util import DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_checker import VecCheckNan, check_env\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "import datetime\n",
        "from sklearn import preprocessing\n",
        "from mpl_finance import candlestick_ochl as candlestick\n",
        "import math\n",
        "import os\n",
        "import moviepy.video.io.ImageSequenceClip\n",
        "import glob\n",
        "import re\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB9FxIN_AQC4",
        "outputId": "85fc7f68-5271-47c3-9c46-43ce5e830a9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fea3ce027d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# stock environment parameters\n",
        "MAX_ACCOUNT_BALANCE = 2147483647\n",
        "MAX_NUM_SHARES = 2147483647\n",
        "MAX_SHARE_PRICE = 4294967295\n",
        "LOOKBACK_WINDOW_SIZE = 10\n",
        "MAX_STEPS = 20000\n",
        "# max trading of agent in real environment, 60 days\n",
        "MAX_TRADING_SESSION=60\n",
        "INITIAL_ACCOUNT_BALANCE = 10000\n",
        "# default percentage of stock price trading agent pays broker when \n",
        "# buying/selling, default is 0.1% (i.e. very reasonable)\n",
        "DA_COMMISION = 0.1\n",
        "\n",
        "\n",
        "# Visualization Parameters\n",
        "style.use('dark_background')\n",
        "VOLUME_CHART_HEIGHT = 0.33\n",
        "UP_COLOR = '#27A59A'\n",
        "DOWN_COLOR = '#EF534F'\n",
        "UP_TEXT_COLOR = '#73D3CC'\n",
        "DOWN_TEXT_COLOR = '#DC2C27'\n",
        "\n",
        "\n",
        "# Video Parameters\n",
        "fps=1\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y-VGjdGWxke"
      },
      "source": [
        " ## Creating visualization for Stock/ETF Environment\n",
        " As detalied by Adam King in his article titled\n",
        " [Rendering elegant stock trading agents using Matplotlib \n",
        " and Gym](https://towardsdatascience.com/visualizing-stock-trading-agents-using-matplotlib-and-gym-584c992bc6d4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R2xK5ujxWX7i"
      },
      "outputs": [],
      "source": [
        "def date2num(date):\n",
        "  converter = mdates.strpdate2num('%Y-%m-%d')\n",
        "  return converter(date)\n",
        "\n",
        "class StockTradingGraph:\n",
        "  \"\"\"A stock trading visualization using matplotlib made to render \n",
        "    OpenAI gym environments\"\"\"\n",
        "  def __init__(self, df, title=None):\n",
        "    self.df = df\n",
        "    self.net_worths = np.zeros(len(df.close))\n",
        "    self.count = 0\n",
        "    self._step = 0\n",
        "    \n",
        "\n",
        "    # Create a figure on screen and set the title\n",
        "    fig = plt.figure()\n",
        "    fig.suptitle(title)\n",
        "    # Create top subplot for net worth axis\n",
        "    self.net_worth_ax = plt.subplot2grid(shape=(6, 1), loc=(0, 0), rowspan=2,     \n",
        "      colspan=1)\n",
        "  \n",
        "    # Create bottom subplot for shared price/volume axis\n",
        "    self.price_ax = plt.subplot2grid(shape=(6, 1), loc=(2, 0), rowspan=8, \n",
        "      colspan=1, sharex=self.net_worth_ax)\n",
        "    # Create a new axis for volume which shares its x-axis with price\n",
        "    self.volume_ax = self.price_ax.twinx()\n",
        "    # Add padding to make graph easier to view\n",
        "    plt.subplots_adjust(left=0.11, bottom=0.24, right=0.90, \n",
        "      top=0.90, wspace=0.2, hspace=0)\n",
        "    \n",
        "  \n",
        "  def render(self, current_step, net_worth, trades, window_size=40):\n",
        "    self.net_worths[current_step] = net_worth\n",
        "    window_start = max(current_step - window_size, 0)\n",
        "    step_range = range(window_start, current_step + 1)\n",
        "    # Format dates as timestamps, necessary for candlestick graph\n",
        "    dates = np.array([date2num(x) for x in self.df.date[step_range]])\n",
        "\n",
        "    self._render_net_worth(current_step, net_worth, step_range, dates)\n",
        "    self._render_price(current_step, net_worth, dates, step_range)\n",
        "    self._render_volume(current_step, net_worth, dates, step_range)\n",
        "    self._render_trades(current_step, trades, step_range)\n",
        "        \n",
        "    # Format the date ticks to be more easily read\n",
        "    self.price_ax.set_xticklabels(self.df.date[step_range], rotation=45, ha='right')\n",
        "        \n",
        "    # Hide duplicate net worth date labels\n",
        "    plt.setp(self.net_worth_ax.get_xticklabels(), visible=False)\n",
        "    # Necessary to view frames before they are unrendered  \n",
        "    if self.count < 1:\n",
        "      plt.savefig(f'{current_step}.png')\n",
        "      self._step = current_step\n",
        "      self.count += 1\n",
        "    else: \n",
        "      if self._step + LOOKBACK_WINDOW_SIZE==current_step: \n",
        "        plt.savefig(f'{current_step}.png')\n",
        "        self._step = current_step\n",
        "\n",
        "  def _render_net_worth(self, current_step, net_worth, step_range, dates):\n",
        "    # Clear the frame rendered last step\n",
        "    self.net_worth_ax.clear()\n",
        "\n",
        "    # Plot net worths\n",
        "    self.net_worth_ax.plot_date(dates, self.net_worths[step_range],\n",
        "                                '-', label='Net Worth')\n",
        "\n",
        "    # Show legend, which uses the label we defined for the plot above\n",
        "    self.net_worth_ax.legend()\n",
        "    legend = self.net_worth_ax.legend(loc=2, ncol=2, prop={'size': 8})\n",
        "    legend.get_frame().set_alpha(0.4)\n",
        "\n",
        "    last_date = date2num(self.df.date[current_step])\n",
        "    last_net_worth = self.net_worths[current_step]\n",
        "\n",
        "    # Annotate the current net worth on the net worth graph\n",
        "    self.net_worth_ax.annotate('{0:.2f}'.format(net_worth), (last_date, last_net_worth),\n",
        "                                   xytext=(last_date, last_net_worth),\n",
        "                                   bbox=dict(boxstyle='round',\n",
        "                                             fc='w', ec='k', lw=1),\n",
        "                                   color=\"black\",\n",
        "                                   fontsize=\"small\")\n",
        "\n",
        "    # Add space above and below min/max net worth\n",
        "    self.net_worth_ax.set_ylim(\n",
        "            min(self.net_worths[np.nonzero(self.net_worths)]) / 1.25, max(self.net_worths) * 1.25)\n",
        "    \n",
        "\n",
        "  def _render_price(self, current_step, net_worth, dates, step_range):\n",
        "        self.price_ax.clear()\n",
        "\n",
        "        # Format data for OHCL candlestick graph\n",
        "        candlesticks = zip(dates,\n",
        "                           self.df.open[step_range], self._cur_close(step_range),\n",
        "                           self._cur_high(step_range), self._cur_low(step_range))\n",
        "\n",
        "        # Plot price using candlestick graph from mpl_finance\n",
        "        candlestick(self.price_ax, candlesticks, width=0.5,\n",
        "                    colorup=UP_COLOR, colordown=DOWN_COLOR)\n",
        "\n",
        "        last_date = date2num(self.df.date[current_step])\n",
        "        last_close = self._cur_close(current_step)\n",
        "        last_high = self._cur_high(current_step)\n",
        "\n",
        "        # Print the current price to the price axis\n",
        "        self.price_ax.annotate('{0:.2f}'.format(last_close), (last_date, last_close),\n",
        "                               xytext=(last_date, last_high),\n",
        "                               bbox=dict(boxstyle='round',\n",
        "                                         fc='w', ec='k', lw=1),\n",
        "                               color=\"black\",\n",
        "                               fontsize=\"small\")\n",
        "\n",
        "        # Shift price axis up to give volume chart space\n",
        "        ylim = self.price_ax.get_ylim()\n",
        "        self.price_ax.set_ylim(ylim[0] - (ylim[1] - ylim[0])\n",
        "                               * VOLUME_CHART_HEIGHT, ylim[1])\n",
        "        \n",
        "  def _render_volume(self, current_step, net_worth, dates, step_range):\n",
        "        self.volume_ax.clear()\n",
        "\n",
        "        volume = np.array(self.df.volume[step_range])\n",
        "\n",
        "        pos = self.df.open[step_range] - \\\n",
        "            self._cur_close([step_range]) < 0\n",
        "        neg = self.df.open[step_range] - \\\n",
        "            self._cur_close([step_range]) > 0\n",
        "\n",
        "\n",
        "        # Color volume bars based on price direction on that date\n",
        "        self.volume_ax.bar(dates[pos], volume[pos], color=UP_COLOR,\n",
        "                           alpha=0.4, width=0.5, align='center')\n",
        "        self.volume_ax.bar(dates[neg], volume[neg], color=DOWN_COLOR,\n",
        "                           alpha=0.4, width=0.5, align='center')\n",
        "\n",
        "        # Cap volume axis height below price chart and hide ticks\n",
        "        self.volume_ax.set_ylim(0, max(volume) / VOLUME_CHART_HEIGHT)\n",
        "        self.volume_ax.yaxis.set_ticks([])\n",
        "       \n",
        "\n",
        "  def _render_trades(self, current_step, trades, step_range):\n",
        "    for trade in trades:\n",
        "      if trade['step'] in step_range:\n",
        "        date = date2num(self.df.date[trade['step']])\n",
        "        high = self._cur_high(trade['step'])\n",
        "        low = self._cur_low(trade['step'])\n",
        "         \n",
        "        if trade['type'] == 'buy':\n",
        "          high_low = low\n",
        "          color = UP_TEXT_COLOR\n",
        "        else:\n",
        "          high_low = high\n",
        "          color = DOWN_TEXT_COLOR\n",
        "        \n",
        "        total = '{0:.2f}'.format(trade['total'])\n",
        "        \n",
        "        # Print the current price to the price axis\n",
        "        self.price_ax.annotate(f'${total}', (date, high_low),\n",
        "                             xytext=(date, high_low),\n",
        "                             color=color,\n",
        "                             fontsize=8,\n",
        "                             arrowprops=(dict(color=color)))\n",
        "                \n",
        "  def _cur_close(self, offset):\n",
        "      \"\"\"\n",
        "      Calculate real close price for the current bar\n",
        "      \"\"\"\n",
        "      return self.df.real_close[offset]\n",
        "\n",
        "  def _cur_high(self, offset):\n",
        "      \"\"\"\n",
        "      Calculate real close price for the current bar\n",
        "      \"\"\"\n",
        "      return self.df.real_high[offset]\n",
        "\n",
        "  def _cur_low(self, offset):\n",
        "      \"\"\"\n",
        "      Calculate real close price for the current bar\n",
        "      \"\"\"\n",
        "      return self.df.real_low[offset]\n",
        "\n",
        "  def close(self):\n",
        "    plt.close()\n",
        "              \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dJiMYPDIAj3y"
      },
      "outputs": [],
      "source": [
        "# Stock/ETF Trading Enviornment\n",
        "class StockTradingEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, data, random_ofs_on_reset=True):\n",
        "        super(StockTradingEnv, self).__init__()\n",
        "        self.data = data\n",
        "        self.random_ofs_on_reset = random_ofs_on_reset\n",
        "        self.visualization = None \n",
        "        self.track_reward = 0\n",
        "        self.bars_count = LOOKBACK_WINDOW_SIZE\n",
        "        self.commission = DA_COMMISION\n",
        "\n",
        "        # # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
        "        # self.action_space = spaces.MultiDiscrete(nvec=[3, 10])\n",
        "        self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([3, 1]), dtype=np.float32)\n",
        "\n",
        "        # Prices contains the OHCL values for the last five prices\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=1, shape=self.shape, dtype=np.float32)\n",
        "        \n",
        "        print(self.observation_space)\n",
        "        self.seed()\n",
        "\n",
        "    def reset(self):\n",
        "      # random offset portion \n",
        "      bars = self.bars_count\n",
        "      if self.random_ofs_on_reset:\n",
        "        offset = self.np_random.choice(self.data.high.shape[0]-bars*10)+bars\n",
        "      else:\n",
        "        offset = bars\n",
        "      self._reset(offset)\n",
        "      return self._next_observation()\n",
        "\n",
        "    def _reset(self, offset):\n",
        "      self.trades = []\n",
        "      self.balance = INITIAL_ACCOUNT_BALANCE\n",
        "      self.net_worth = INITIAL_ACCOUNT_BALANCE\n",
        "      self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
        "      self.standkeMaxBenchShares = 0\n",
        "      self.shares_held  = 0\n",
        "      self._offset = offset\n",
        "      # setting account history portion\n",
        "      self.account_history = np.repeat([\n",
        "            [self.net_worth/MAX_ACCOUNT_BALANCE],\n",
        "            [0],\n",
        "            [0],\n",
        "            [0],\n",
        "            [0]\n",
        "            ], LOOKBACK_WINDOW_SIZE, axis=1) \n",
        "\n",
        "\n",
        "    # shape of observation space is 2D\n",
        "    @property\n",
        "    def shape(self):\n",
        "      return (10, self.bars_count)\n",
        "\n",
        "    def _next_observation(self):\n",
        "      res = np.zeros(shape=(5, self.bars_count), dtype=np.float32)\n",
        "      ofs = self.bars_count-1\n",
        "      res[0] = self.data.open[self._offset-ofs:self._offset+1]\n",
        "      res[1] = self.data.high[self._offset-ofs:self._offset+1]\n",
        "      res[2] = self.data.low[self._offset-ofs:self._offset+1]\n",
        "      res[3] = self.data.close[self._offset-ofs:self._offset+1]\n",
        "      res[4] = self.data.volume[self._offset-ofs:self._offset+1]\n",
        "      res = np.append(res, [self.account_history[0][-self.bars_count:],\n",
        "                            self.account_history[1][-self.bars_count:],\n",
        "                            self.account_history[2][-self.bars_count:],\n",
        "                            self.account_history[3][-self.bars_count:],\n",
        "                            self.account_history[4][-self.bars_count:]],axis=0)\n",
        "      res = np.float32(res)\n",
        "      return res\n",
        "       \n",
        "    def _take_action(self, action):\n",
        "      current_price = self._cur_close()\n",
        "      action_type = action[0]\n",
        "      amount = action[1]\n",
        "\n",
        "      shares_bought = 0\n",
        "      shares_sold = 0\n",
        "      additional_cost = 0\n",
        "      sales = 0\n",
        "\n",
        "\n",
        "      if action_type < 1:\n",
        "        # Buy amount % of balance in shares\n",
        "        total_possible = self.balance / current_price\n",
        "        shares_bought = total_possible * amount\n",
        "        additional_cost = shares_bought * current_price * (1+self.commission)\n",
        "        self.balance -= additional_cost\n",
        "        self.standkeMaxBenchShares += shares_bought\n",
        "        self.shares_held += shares_bought\n",
        "        \n",
        "        \n",
        "        # visualization portion\n",
        "        if shares_bought > 0:\n",
        "          self.trades.append({'step': self._offset, 'shares': shares_bought, \n",
        "                              'total': additional_cost, 'type': \"buy\"})\n",
        "          \n",
        "      elif action_type < 2:\n",
        "        # Sell amount % of shares held\n",
        "        shares_sold = self.shares_held * amount  \n",
        "        sales = shares_sold * current_price * (1 - self.commission)\n",
        "        self.balance += sales\n",
        "        self.standkeMaxBenchShares -= shares_sold\n",
        "        \n",
        "\n",
        "        # visualization portion\n",
        "        if shares_sold > 0:\n",
        "          self.trades.append({'step': self._offset, 'shares': shares_sold, \n",
        "                                  'total': shares_sold * current_price, 'type': \"sell\"})  \n",
        "          \n",
        "      \n",
        "      self.netWorth = self.balance + self.shares_held * current_price\n",
        "      \n",
        "      if self.netWorth > self.max_net_worth:\n",
        "        self.max_net_worth = self.netWorth\n",
        "\n",
        "      # updating account history\n",
        "      self.account_history = np.append(self.account_history, [\n",
        "        [self.net_worth/MAX_ACCOUNT_BALANCE],\n",
        "        [shares_bought/MAX_NUM_SHARES],\n",
        "        [additional_cost/MAX_SHARE_PRICE],\n",
        "        [shares_sold/MAX_NUM_SHARES],\n",
        "        [sales/MAX_SHARE_PRICE]\n",
        "        ], axis=1)\n",
        "\n",
        "    def _cur_close(self):\n",
        "      \"\"\"\n",
        "      Calculate real close price for the current bar\n",
        "      \"\"\"\n",
        "      return self.data.real_close[self._offset]\n",
        "\n",
        "    def step(self, action):\n",
        "      reward = 0\n",
        "      current_price = self._cur_close()\n",
        "      # Execute one time step within the environment\n",
        "      self._take_action(action)\n",
        "    \n",
        "      self._offset += 1\n",
        "\n",
        "      if self._offset >= self.data.close.shape[0]-1 or self.netWorth <= 0:\n",
        "        done=True\n",
        "      else:\n",
        "        done=False\n",
        "  \n",
        "      \n",
        "      prev_netWorth = self.account_history[0][-2]\n",
        "      current_netWorth = self.account_history[0][-1]\n",
        "      reward = float((current_netWorth-prev_netWorth))\n",
        "\n",
        "      obs = self._next_observation()\n",
        "\n",
        "      return obs, reward, done, {}\n",
        "\n",
        "    def _render_to_file(self, filename='render.txt'):\n",
        "      f = open(filename, 'a+')\n",
        "      f.write(f\"Step: {self._offset}\\n\")\n",
        "      f.write(f\"Date: {self.data.date[self._offset]}\\n\")\n",
        "      f.write(f\"Balence: {self.balance}\\n\")\n",
        "      f.write(f\"Reward: {self.track_reward}\\n\")\n",
        "      f.write(f\"Amount Held: {self.shares_held}\\n\") \n",
        "      f.write(f\"Amount Sold: {self.total_shares_sold}\\n)\")\n",
        "      #add some more\n",
        "      f.close()\n",
        "\n",
        "    def render(self, mode='file', title=\"Agent's Trading Screen\", **kwargs):\n",
        "      # Render the environment to the screen\n",
        "      if mode == 'file':\n",
        "        self._render_to_file(kwargs.get('filename', 'render.txt'))\n",
        "      elif mode == 'live':\n",
        "        if self.visualization == None:\n",
        "          self.visualization = StockTradingGraph(self.data, title)\n",
        "        if self._offset > LOOKBACK_WINDOW_SIZE:\n",
        "          self.visualization.render(self._offset, self.netWorth,\n",
        "                                    self.trades, window_size=LOOKBACK_WINDOW_SIZE)\n",
        "\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "      self.np_random, seed1 = seeding.np_random(seed)\n",
        "      seed2 = seeding.hash_seed(seed1+1) % 2**33\n",
        "      return [seed1, seed2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "szUR1sYHHEVl"
      },
      "outputs": [],
      "source": [
        "# using sklearn's min-max scaler for the relative high and low\n",
        "x=preprocessing.MinMaxScaler()\n",
        "\n",
        "# taken from https://machinelearningmastery.com/remove-trends-seasonality-difference-transform-python/\n",
        "# create a differenced series\n",
        "def difference(dataset, interval=1):\n",
        "\tdiff = list()\n",
        "\tfor i in range(interval, len(dataset)):\n",
        "\t\tvalue = np.log(dataset[i]) - np.log(dataset[i - interval])\n",
        "\t\tdiff.append(value)\n",
        "\treturn diff\n",
        " \n",
        "\n",
        "# training data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/StockMarketData/archive/Data/ETFs/spy.us.txt')\n",
        "df = df.sort_values('Date')\n",
        "data=df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "\n",
        "# making OHLC data stationary before calculating relative and normalizing \n",
        "diff_o = np.array(difference(data['Open'], 1))\n",
        "# diff_o = np.append(diff_o, diff_o[-1])\n",
        "# diff_o = np.append(diff_o, diff_o[-1])\n",
        "\n",
        "diff_h = np.array(difference(data['High'], 1))\n",
        "# diff_h = np.append(diff_h, diff_h[-1])\n",
        "# diff_h = np.append(diff_h, diff_h[-1])\n",
        "\n",
        "diff_l = np.array(difference(data['Low'], 1))\n",
        "# diff_l = np.append(diff_l, diff_l[-1])\n",
        "# diff_l = np.append(diff_l, diff_l[-1])\n",
        "\n",
        "diff_c = np.array(difference(data['Close'], 1))\n",
        "# diff_c = np.append(diff_c, diff_c[-1])\n",
        "# diff_c = np.append(diff_c, diff_c[-1])\n",
        "\n",
        "# volumne data\n",
        "vol = data['Volume'].values/MAX_NUM_SHARES\n",
        "# vol = np.append(vol, vol[-1])\n",
        "# year data of year-month-day form\n",
        "dt = data['Date'].array\n",
        "# dt = np.append(dt, dt[-1])\n",
        "# calculating relative prices and normalizing data\n",
        "o =  (diff_o-diff_l)/(diff_h-diff_l)\n",
        "o =  x.fit_transform(o.reshape(-1,1)).reshape(-1)\n",
        "rc = (diff_c-diff_l)/(diff_h-diff_l)\n",
        "rc = x.fit_transform(rc.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "rh = x.fit_transform(diff_h.reshape(-1,1)).reshape(-1)\n",
        "rl = x.fit_transform(diff_l.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "Train_Data = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume', 'real_close', 'real_high', 'real_low'])\n",
        "train = Train_Data(date=dt,high=rh, low=rl, close=rc, open=o, volume=vol, real_close=data['Close'].values, real_high=data['High'].values, real_low=data['Low'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1TdubrWOfamD"
      },
      "outputs": [],
      "source": [
        "# Testing data\n",
        "test = pd.read_csv('/content/drive/MyDrive/Datasets/StockMarketData/test.csv')\n",
        "t_df = test.sort_values('Date')\n",
        "data_two=t_df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "# making OHLC data stationary before calculating relative and normalizing \n",
        "diff_o = np.array(difference(data_two['Open'], 1))\n",
        "# diff_o = np.append(diff_o, diff_o[-1])\n",
        "diff_h = np.array(difference(data_two['High'], 1))\n",
        "# diff_h = np.append(diff_h, diff_h[-1])\n",
        "diff_l = np.array(difference(data_two['Low'], 1))\n",
        "# diff_l = np.append(diff_l, diff_l[-1])\n",
        "diff_c = np.array(difference(data_two['Close'], 1))\n",
        "# diff_c = np.append(diff_c, diff_c[-1])\n",
        "\n",
        "# volumne data\n",
        "vol = data_two['Volume'].values/MAX_NUM_SHARES\n",
        "# year data of year-month-day form\n",
        "dt = data_two['Date'].array\n",
        "# calculating relative prices and normalizing data\n",
        "o =  (diff_o-diff_l)/(diff_h-diff_l)\n",
        "o =  x.fit_transform(o.reshape(-1,1)).reshape(-1)\n",
        "rc = (diff_c-diff_l)/(diff_h-diff_l)\n",
        "rc = x.fit_transform(rc.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "rh = x.fit_transform(diff_h.reshape(-1,1)).reshape(-1)\n",
        "rl = x.fit_transform(diff_l.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "Test_Data = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume', 'real_close', 'real_high', 'real_low'])\n",
        "test = Test_Data(date=dt,high=rh, low=rl, close=rc, open=o, volume=vol, real_close=data_two['Close'].values, real_high=data_two['High'].values, real_low=data_two['Low'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Custom CNN Feature Extractor"
      ],
      "metadata": {
        "id": "_-5jadJfRbDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "  def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
        "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
        "        # We assume HxW images\n",
        "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
        "        input = observation_space.shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input, 128, kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 64, kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(\n",
        "                th.as_tensor(observation_space.sample()[None]).float()\n",
        "            ).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.Sigmoid())\n",
        "        \n",
        "  def forward(self, observations):\n",
        "    return self.linear(self.cnn(observations))\n",
        "\n",
        "# additional keyword parameters to pass to model net_arch=[dict(pi=[256], vf=[256])]\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=CustomCNN,\n",
        "    features_extractor_kwargs=dict(features_dim=128)\n",
        ")"
      ],
      "metadata": {
        "id": "572gmNd6Q-kr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg2HNywdyLaO"
      },
      "source": [
        "# Training and Validation Portion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cAJYlkEqIwcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16dc6884-1005-4fbe-e3d9-b18112b6e875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], (10, 10), float32)\n",
            "Box([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], (10, 10), float32)\n",
            "Box([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], (10, 10), float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan at 0x7feacde59150>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# number of learning steps to train RL model is set to 200K\n",
        "MAX_STEPS = 1e6\n",
        "# number of epoch when optimizing the surrogate loss\n",
        "EPOCHS = 10\n",
        "# entropy coefficient for the loss calculation\n",
        "E_COEF = 0.0\n",
        "# limit the KL divergence between updates \n",
        "T_KL = None\n",
        "# minibatch size\n",
        "BATCH = 64\n",
        "# the number of steps to run for each environment per update \n",
        "# (i.e. rollout buffer size is n_steps * n_envs where n_envs is \n",
        "# number of environment copies running in parallel) NOTE: n_steps * n_envs \n",
        "# must be greater than 1 (because of the advantage normalization)\n",
        "N_STEPS=2033\n",
        "# value function coefficient for the loss calculation\n",
        "VF_COEF= 0.5\n",
        "# the number of parallel environments \n",
        "ENV = 1\n",
        "\n",
        "# create evaluation env that takes in test data for validation\n",
        "eval_env = DummyVecEnv([lambda: StockTradingEnv(test, random_ofs_on_reset=False)])\n",
        "# use deterministic actions for evaluation callback\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path='/content/drive/MyDrive/RLmodels/bestPPO/',\n",
        "                             log_path='/content/drive/MyDrive/RLmodels/logs/', eval_freq=MAX_STEPS/100,\n",
        "                             deterministic=False, render=False)\n",
        "\n",
        "# create training envs that takes in training data for training\n",
        "envs =  DummyVecEnv([lambda: StockTradingEnv(train, random_ofs_on_reset=False) for _ in range(0,ENV)])\n",
        "\n",
        "# policy_kwargs=policy_kwargs\n",
        "# training model using CNNPolicy\n",
        "model = PPO(\"MlpPolicy\", envs, tensorboard_log=\"/content/PPO_SPY_tensorboard/\")\n",
        "check_env(StockTradingEnv(train, random_ofs_on_reset=False))\n",
        "VecCheckNan(envs, raise_exception=True, check_inf=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "O6sMSFIuhLzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eedd02b-2934-4036-c156-f4151389ad9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=60000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=70000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=110000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=120000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=130000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=150000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=160000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=170000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=180000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=190000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=200000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=210000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=220000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=230000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=240000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=250000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=260000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=270000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=280000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=290000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=300000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=310000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=320000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=330000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=340000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=350000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=360000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=370000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=380000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=390000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=400000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=410000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=420000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=430000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=440000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=450000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=460000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=470000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=480000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=490000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=500000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=510000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=520000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=530000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=540000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=550000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=560000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=570000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=580000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=590000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=600000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=610000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=620000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=630000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=640000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=650000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=660000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=670000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=680000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=690000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=700000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=710000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=720000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=730000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=740000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=750000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=760000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=770000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=780000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=790000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=800000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=810000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=820000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=830000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=840000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=850000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=860000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=870000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=880000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=890000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=900000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=910000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=920000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=930000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=940000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=950000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=960000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=970000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=980000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=990000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "Eval num_timesteps=1000000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 1198.00 +/- 0.00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7fea3cd580d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model.learn(total_timesteps=MAX_STEPS, callback=eval_callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS1VdLn8yHB_"
      },
      "source": [
        "# Prediction and Rendering Environment Portion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dpOBmCDTKyEB"
      },
      "outputs": [],
      "source": [
        "# model = PPO.load(\"/content/drive/MyDrive/RLmodels/bestPPO/best_model.zip\")\n",
        "# env = StockTradingEnv(test_data, random_ofs_on_reset=False)\n",
        "# obs = env.reset()\n",
        "# for i in range(len(test_data.date)):\n",
        "#   action, _states = model.predict(obs, deterministic=False)\n",
        "#   obs, rewards, done, info = env.step(action)\n",
        "#   env.render()\n",
        "#   if done:\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "U6s5dFWF1ffH"
      },
      "outputs": [],
      "source": [
        "# taken from https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside\n",
        "\n",
        "# def atoi(text):\n",
        "#     return int(text) if text.isdigit() else text\n",
        "\n",
        "# def natural_keys(text):\n",
        "#     '''\n",
        "#     alist.sort(key=natural_keys) sorts in human order\n",
        "#     http://nedbatchelder.com/blog/200712/human_sorting.html\n",
        "#     (See Toothy's implementation in the comments)\n",
        "#     '''\n",
        "#     return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
        "\n",
        "# list_of_files = [img for img in os.listdir('/content') if img.endswith(\".png\")]\n",
        "# list_of_files.sort(key=natural_keys)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Hp59hzzGC-vd"
      },
      "outputs": [],
      "source": [
        "# taken from https://stackoverflow.com/questions/44947505/how-to-make-a-movie-out-of-images-in-python\n",
        "# clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(list_of_files, fps=fps)\n",
        "# clip.write_videofile('agent_trading.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1lHH0sY5lhLQ"
      },
      "outputs": [],
      "source": [
        "# taken from https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=8nj5sjsk15IT\n",
        "\n",
        "# def show_video():\n",
        "#   mp4list = glob.glob('agent_trading.mp4')\n",
        "#   if len(mp4list) > 0:\n",
        "#     mp4 = mp4list[0]\n",
        "#     video = io.open(mp4, 'r+b').read()\n",
        "#     encoded = base64.b64encode(video)\n",
        "#     ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "#                 loop controls style=\"height: 400px;\">\n",
        "#                 <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "#              </video>'''.format(encoded.decode('ascii'))))\n",
        "#   else: \n",
        "#     print(\"Could not find video\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-Vr_aCGPlvbP"
      },
      "outputs": [],
      "source": [
        "# show_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wpIyXz52nYs4"
      },
      "outputs": [],
      "source": [
        "# !rm -r *.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "n02iHgNDTvYJ"
      },
      "outputs": [],
      "source": [
        "# %tensorflow_version 2\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir /content/PPO_SPY_tensorboard/ "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1BnRqmo4sw9dhUsiP_WO4HGPM9M7Cryi7",
      "authorship_tag": "ABX9TyNbB/NmEpSE43ui6jXQVUCU",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}