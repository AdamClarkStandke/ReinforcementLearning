{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aCStandke/ReinforcementLearning/blob/main/ThirdStockEnivornment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGbbfGz0KsDS"
      },
      "source": [
        "# Installing Necessary Packages for Training the Trading Agent\n",
        "\n",
        "To train the Trading Agent the package [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/index.html) was used. As stated in the docs: \n",
        "> Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines. And steems from the paper [Stable-Baselines3: Reliable Reinforcement Learning Implementations](https://jmlr.org/papers/volume22/20-1364/20-1364.pdf)\n",
        "The algorithms in this package will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\n",
        "\n",
        "---\n",
        "## Proximal Policy Optimization(PPO):\n",
        "\n",
        "Because in this environment the Agent will be executing continous actions, the Proximal Policy Optimization(PPO) algorithm was chosen. As detailed by the authors [PPO](https://arxiv.org/pdf/1707.06347.pdf)\n",
        "\n",
        "\n",
        "> We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically).\n",
        "\n",
        "\n",
        "PPO uses the following novel objective function:\n",
        "\n",
        "$L^{CLIP}(θ)=\\hat{E}_t[min(r_{t}(θ)\\hat{A}_t,clip(r_{t}(θ), 1-ϵ, 1+ϵ)\\hat{A}_t]$\n",
        "\n",
        "*  $\\theta$ is the policy parameter\n",
        "*  $\\hat{E}_t$ denotes the empirical expectation over timesteps\n",
        "*  $r_{t}$ is the ratio of the probability under the new and old policies, respectively\n",
        "*  $\\hat{A}_t$ is the estimated advantage at time t\n",
        "*  $\\epsilon$ is the clipping hyperparameter, usually 0.1 or 0.2\n",
        "\n",
        "\n",
        "As detailed by the authors [openAI](https://openai.com/blog/openai-baselines-ppo/#ppo)\n",
        "\n",
        "\n",
        "> This objective implements a way to do a Trust Region update which is compatible with Stochastic Gradient Descent, and simplifies the algorithm by removing the KL penalty and need to make adaptive updates. In tests, this algorithm has displayed the best performance on continuous control tasks and almost matches ACER’s performance on Atari, despite being far simpler to implement\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OHO1Q4dS9tF0"
      },
      "outputs": [],
      "source": [
        "# ignore warning messages because they are annoying lol\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZqZJWIuzLFDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c517b3af-8c16-47b6-a2ae-7024a4c55f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-1.6.2-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 29.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Collecting importlib-metadata~=4.13\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 87.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.12.1+cu113)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.64.1)\n",
            "Collecting rich\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 82.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Collecting ale-py==0.7.4\n",
            "  Downloading ale_py-0.7.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 65.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.9.1)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata~=4.13->stable-baselines3[extra]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata~=4.13->stable-baselines3[extra]) (3.9.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.37.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.49.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.17.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2022.4)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich->stable-baselines3[extra]) (2.6.1)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616824 sha256=105880c544b269d3ab8cc7ca35293d0c8782d17059659141eab8dbc78bdc2f2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=e8f45f02b4fed56cd9c3e253b4ad67a7e4b5c19609339a3ded821ec110ef1839\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: importlib-metadata, gym, commonmark, AutoROM.accept-rom-license, autorom, stable-baselines3, rich, ale-py\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 5.0.0\n",
            "    Uninstalling importlib-metadata-5.0.0:\n",
            "      Successfully uninstalled importlib-metadata-5.0.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.4 autorom-0.4.2 commonmark-0.9.1 gym-0.21.0 importlib-metadata-4.13.0 rich-12.6.0 stable-baselines3-1.6.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting empyrical\n",
            "  Downloading empyrical-0.5.5.tar.gz (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from empyrical) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from empyrical) (1.3.5)\n",
            "Requirement already satisfied: scipy>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from empyrical) (1.7.3)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.7/dist-packages (from empyrical) (0.9.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.16.1->empyrical) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.16.1->empyrical) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader>=0.2->empyrical) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pandas-datareader>=0.2->empyrical) (4.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.16.1->empyrical) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical) (2022.9.24)\n",
            "Building wheels for collected packages: empyrical\n",
            "  Building wheel for empyrical (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for empyrical: filename=empyrical-0.5.5-py3-none-any.whl size=39780 sha256=03c5ea1f8ec8e2b9625dc07a1c2aa5c1794fe870547b2c4b8586cb44d04c0206\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/91/4b/654fcff57477efcf149eaca236da2fce991526cbab431bf312\n",
            "Successfully built empyrical\n",
            "Installing collected packages: empyrical\n",
            "Successfully installed empyrical-0.5.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.3-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 37.2 MB/s \n",
            "\u001b[?25hCollecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 10.7 MB/s \n",
            "\u001b[?25hCollecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 93.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (4.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.41)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (1.1.3.post0)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.4.1)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 96.2 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 65.3 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.1-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=bb93043199acb9b0feb45247350d88c4b84c291ce8bb1d50b0fd891a840dff1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.3 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: importlib-metadata==4.13.0 in /usr/local/lib/python3.7/dist-packages (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata==4.13.0) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata==4.13.0) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3[extra] # reinforcement learning library\n",
        "!pip install empyrical # financial calculation library\n",
        "!pip install optuna # hyperparameter library\n",
        "!pip install --upgrade importlib-metadata==4.13.0 # for backwards compatibility issue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AoLhA3_b_XeK"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import gym \n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import datetime as dt\n",
        "import optuna\n",
        "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.utils import constant_fn\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.env_util import DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_checker import VecCheckNan, check_env\n",
        "from stable_baselines3.common.callbacks import BaseCallback, CallbackList, StopTrainingOnRewardThreshold\n",
        "from empyrical import sortino_ratio, calmar_ratio, omega_ratio\n",
        "import sqlite3\n",
        "from sqlite3 import Error\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "import datetime\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import os\n",
        "import csv\n",
        "from csv import DictWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TBwoqXizywK"
      },
      "source": [
        "# Third Stock Trading Environment\n",
        "\n",
        "\n",
        "  This third stock trading environment is based on Adam King's articles as found here:[Creating Bitcoin trading bots don’t lose money](https://medium.com/towards-data-science/creating-bitcoin-trading-bots-that-dont-lose-money-2e7165fb0b29) and here:[Optimizing deep learning trading bots using state-of-the-art techniques](https://towardsdatascience.com/using-reinforcement-learning-to-trade-bitcoin-for-massive-profit-b69d0e8f583b)\n",
        "Furthermore, the random offset in the reset method is based on Maxim Lapan's implementation as found in chapter eight of his book [Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition](https://www.amazon.com/Deep-Reinforcement-Learning-Hands-optimization/dp/1838826998).\n",
        "\n",
        "Similar to the second stock trading environment, the agent is trading in the [SPY ETF](https://www.etf.com/SPY?L=1) environment and is trading in a continous action space(i.e.[0-3] for buying, selling, or holding and [0-1] for % sold/bought where 1 is equivalent to 100%)  and  observation space(i.e. [0-1]);however, in the second stock trading environment this space ranges from [-1,1].Also unlike the second stock trading environment, an additional observation was added to the agent's observations space; namely, an account history/ledger of the agent's past networths from trading in the SPY ETF environment with the given trading window (*note: this window is set by the variable LOOKBACK_WINDOW_SIZE and its default is 10 days*) and a commision parameter used in the cost and sales calculation(default is 0.1%). \n",
        "\n",
        "Additionally, different ways of calculating the agent's reward were added, namely: \n",
        "* BalenceReward: simple reward scheme that Adam King created that multiplies the agent's balance by a delay modifier that is based on the current offset(i.e.step) of the agent in the environment[Creating Bitcoin trading bots don’t lose money](https://medium.com/towards-data-science/creating-bitcoin-trading-bots-that-dont-lose-money-2e7165fb0b29)\n",
        "* [sortinoRewardRatio](https://www.investopedia.com/terms/s/sortinoratio.asp)\n",
        "* [calmarRewardRatio](https://www.investopedia.com/terms/c/calmarratio.asp)\n",
        "* [omegaRewardRatio](https://www.wallstreetmojo.com/omega-ratio/) \n",
        "* StandkeCurrentValueReward: simple reward scheme that I created that is the difference of the previous trading day's networth and the current trading day's networth (and can be multiplied by the agent's balance)\n",
        "* StandkeSmallDrawDownReward: simple reward scheme that I created that takes the maximum and minimum networth of the past 10 trading days divided by the maximum value of the past 10 trading days (and can be multiplied by the agent's balance) \n",
        "* StandkeSumofDifferenceReward: simple reward scheme that I created that takes the difference of the past 10 trading days and sums the values before multiplying it by the agent's balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "OB9FxIN_AQC4"
      },
      "outputs": [],
      "source": [
        "# stock environment parameters\n",
        "MAX_ACCOUNT_BALANCE = 2147483647 \n",
        "MAX_NUM_SHARES = 2147483647\n",
        "MAX_SHARE_PRICE = 4294967295\n",
        "LOOKBACK_WINDOW_SIZE = 10 # trading window, default is 10 frames \n",
        "MAX_STEPS = 1e6 # max steps agent can take in environment \n",
        "INITIAL_ACCOUNT_BALANCE = 10000 # starting balance and networth\n",
        "\n",
        "# parameters for ratio calculations\n",
        "RISK_FREE = 0.0 # risk free rate, default is 0.0\n",
        "REQ_RETURN = 0.0 # required return from agent over trading window, default 0.0 \n",
        "PERIOD = 'daily'\n",
        "# default percentage of stock price trading agent pays broker when \n",
        "# buying/selling, default is 0.1% (i.e. very reasonable)\n",
        "DA_COMMISION = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "dJiMYPDIAj3y"
      },
      "outputs": [],
      "source": [
        "# Stock/ETF Trading Enviornment\n",
        "class StockTradingEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, data, reward_func='sortinoRewardRatio', random=True):\n",
        "        super(StockTradingEnv, self).__init__()\n",
        "        self.data = data\n",
        "        self.scale = preprocessing.MinMaxScaler()\n",
        "        self.random_ofs_on_reset = random\n",
        "        self.reward_func = reward_func\n",
        "        self.bars_count = LOOKBACK_WINDOW_SIZE\n",
        "        self.commission = DA_COMMISION\n",
        "        self.hold= False\n",
        "\n",
        "        # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
        "        self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([3, 1]), dtype=np.float32)\n",
        "\n",
        "        # Prices contains the OHCL values for the last five prices the state \n",
        "        # space is 12 dim i.e. 6 x 2 = 12\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=1, shape=self.shape, dtype=np.float32)\n",
        "        \n",
        "        self.seed()\n",
        "\n",
        "    def reset(self):\n",
        "      # random offset portion \n",
        "      bars = self.bars_count\n",
        "      if self.random_ofs_on_reset:\n",
        "        offset = self.np_random.choice(self.data.high.shape[0]-bars*10)+bars\n",
        "      else:\n",
        "        offset = bars\n",
        "      self._reset(offset)\n",
        "      return self._next_observation()\n",
        "\n",
        "    def _reset(self, offset):\n",
        "      self.trades = []\n",
        "      self.balance = INITIAL_ACCOUNT_BALANCE\n",
        "      self.netWorth = INITIAL_ACCOUNT_BALANCE\n",
        "      self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
        "      self.standkeMaxBenchShares = 0\n",
        "      self.shares_held  = 0\n",
        "      self._offset = offset\n",
        "      # setting account history portion\n",
        "      self.account_history = np.repeat([[self.netWorth/MAX_ACCOUNT_BALANCE]], LOOKBACK_WINDOW_SIZE, axis=1)\n",
        "\n",
        "    # shape of observation space is 2D\n",
        "    @property\n",
        "    def shape(self):\n",
        "      return (6, self.bars_count)\n",
        "\n",
        "    def _next_observation(self):\n",
        "      res = np.zeros(shape=(6, self.bars_count), dtype=np.float32)\n",
        "      ofs = self.bars_count-1\n",
        "      res[0] = self.data.volume[self._offset-ofs:self._offset+1]\n",
        "      res[1] = self.data.high[self._offset-ofs:self._offset+1]\n",
        "      res[2] = self.data.low[self._offset-ofs:self._offset+1]\n",
        "      res[3] = self.data.open[self._offset-ofs:self._offset+1]\n",
        "      res[4] = self.account_history[0][-self.bars_count:]\n",
        "      res[5] = self.data.close[self._offset-ofs:self._offset+1]\n",
        "      res = np.float32(res)\n",
        "      return res\n",
        "       \n",
        "    def _take_action(self, action):\n",
        "      reward = 0\n",
        "      current_price = self._cur_close()\n",
        "      action_type = action[0]\n",
        "      amount = action[1]\n",
        "      \n",
        "      shares_bought = 0\n",
        "      shares_sold = 0\n",
        "      additional_cost = 0\n",
        "      sales = 0\n",
        "\n",
        "\n",
        "      if action_type < 1 :\n",
        "        # Buy amount % of balance in shares\n",
        "        total_possible = self.balance / (current_price * (1+self.commission))\n",
        "        shares_bought = total_possible * amount\n",
        "        additional_cost = shares_bought * current_price * (1+self.commission)\n",
        "        self.balance -= additional_cost\n",
        "        self.standkeMaxBenchShares += shares_bought\n",
        "        self.shares_held += shares_bought\n",
        "        \n",
        "        \n",
        "        #STILL TO DO\n",
        "        if shares_bought > 0:\n",
        "          self.trades.append({'step': self._offset, 'shares': shares_bought, \n",
        "                              'total': additional_cost, 'type': \"buy\"})\n",
        "          \n",
        "          \n",
        "      elif action_type < 2:\n",
        "        # Sell amount % of shares held\n",
        "        shares_sold = self.shares_held * amount  \n",
        "        sales = shares_sold * current_price * (1 - self.commission)\n",
        "        self.balance += sales\n",
        "        self.standkeMaxBenchShares -= shares_sold\n",
        "        self.shares_held -= shares_sold\n",
        "        \n",
        "\n",
        "        # STILL TO DO\n",
        "        if shares_sold > 0:\n",
        "          self.trades.append({'step': self._offset, 'shares': -shares_sold, \n",
        "                                  'total': shares_sold * current_price, 'type': \"sell\"})  \n",
        "          \n",
        "      \n",
        "      self.netWorth = self.balance + self.shares_held * current_price\n",
        "      \n",
        "      if self.netWorth > self.max_net_worth:\n",
        "        self.max_net_worth = self.netWorth\n",
        "\n",
        "      # updating account history\n",
        "      self.account_history = np.append(self.account_history, [[self.netWorth/MAX_ACCOUNT_BALANCE]], axis=1)\n",
        "      \n",
        "      # reward Calculations\n",
        "      returns = self.account_history[0][-self.bars_count:]\n",
        "      # calcualtion for ratio based rewards\n",
        "      r = np.diff(returns*MAX_ACCOUNT_BALANCE)\n",
        "      # BalenceReward \n",
        "      if self.reward_func == 'BalenceReward':\n",
        "        delay_modifier = (self._offset / MAX_STEPS)\n",
        "        reward = self.balance * delay_modifier\n",
        "      # Sortino Ratio\n",
        "      elif self.reward_func == 'sortinoRewardRatio':\n",
        "        ratio = sortino_ratio(r, required_return=REQ_RETURN,  period=PERIOD) # default period daily\n",
        "        reward= ratio \n",
        "      # Calmar Ratio\n",
        "      elif self.reward_func == 'calmarRewardRatio':\n",
        "        ratio = calmar_ratio(r, period=PERIOD) # default period daily\n",
        "        reward= ratio \n",
        "      # Omega Ratio\n",
        "      elif self.reward_func == 'omegaRewardRatio':\n",
        "        ratio = omega_ratio(r, risk_free=RISK_FREE, required_return=REQ_RETURN) # default annualiazation trading window\n",
        "        reward= ratio\n",
        "      # StandkeCurrentValueReward\n",
        "      elif self.reward_func == 'StandkeCurrentValueReward':\n",
        "        prev_net = returns[-2]\n",
        "        current_net = returns[-1]\n",
        "        ratio = current_net-prev_net\n",
        "        reward = ratio #* self.balance\n",
        "      # StandkeSmallDrawDownReward\n",
        "      elif self.reward_func == 'StandkeSmallDrawDownReward':\n",
        "        mx = np.max(returns)\n",
        "        mi = np.min(returns)\n",
        "        ratio = round(abs(mx-mi/mx), 1) \n",
        "        reward = ratio #* self.balance\n",
        "      # StandkeSumofDifferenceReward\n",
        "      elif self.reward_func == 'StandkeSumofDifferenceReward':\n",
        "        ratio = np.sum(np.diff(returns))\n",
        "        reward = ratio * self.balance\n",
        "      # None\n",
        "      else:\n",
        "        ratio = np.mean(returns)\n",
        "        reward = ratio #* self.balance\n",
        "      return reward if abs(reward) != np.inf and not np.isnan(reward) else 0\n",
        "\n",
        "      \n",
        "    def _cur_close(self):\n",
        "      \"\"\"\n",
        "      Calculate real close price for the current bar\n",
        "      \"\"\"\n",
        "      return self.data.real_close[self._offset]\n",
        "\n",
        "    def step(self, action):\n",
        "      # Execute one time step within the environment\n",
        "      reward = self._take_action(action)\n",
        "    \n",
        "      self._offset += 1\n",
        "\n",
        "      if self._offset >= self.data.close.shape[0]-1 or int(self.netWorth)<1 or int(self.netWorth)>=MAX_ACCOUNT_BALANCE:\n",
        "        done=True\n",
        "      else:\n",
        "        done=False\n",
        "  \n",
        "      obs = self._next_observation()\n",
        "\n",
        "      info = {\"Net Worth\":self.netWorth, \"reward\": reward}\n",
        "      \n",
        "      return obs, reward, done, info\n",
        "\n",
        "    def _render_to_file(self, filename='results.csv'):\n",
        "      csv_columns = ['Date','Net_Worth','Balence', 'StandkeMaxBenchShares']\n",
        "      dict_data = {'Date':self.data.date[self._offset], 'Net_Worth':self.netWorth, 'Balence':self.balance, 'StandkeMaxBenchShares':self.standkeMaxBenchShares}\n",
        "      with open(filename, 'a+', newline='') as f:\n",
        "        writer = DictWriter(f, fieldnames=csv_columns)\n",
        "        writer.writerow(dict_data)\n",
        "        f.close()\n",
        " \n",
        "    def render(self, mode='file', title=\"Agent's Trading Screen\", **kwargs):\n",
        "      # Render the environment to the screen\n",
        "      if mode == 'file':\n",
        "        self._render_to_file()\n",
        "\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "      self.np_random, seed1 = seeding.np_random(seed)\n",
        "      seed2 = seeding.hash_seed(seed1+1) % 2**33\n",
        "      return [seed1, seed2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing Steps\n",
        "\n",
        "1.   First the data is made [stationary](https://machinelearningmastery.com/remove-trends-seasonality-difference-transform-python/) to remove any trends or seasonality associated with the time series data\n",
        "2.   Then the price data is converted into releative prices to model the relative change rather than absolute change in prices (i.e. generally used in [technical analysis](https://www.investopedia.com/terms/t/technicalanalysis.asp))\n",
        "3. Lastly the data is normalized using sklearn's [min-max scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) as to fit within the environment's observation space of [0-1]\n"
      ],
      "metadata": {
        "id": "-wu_j-g5YIA4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "szUR1sYHHEVl"
      },
      "outputs": [],
      "source": [
        "# using sklearn's min-max scaler for the relative high and low\n",
        "x=preprocessing.MinMaxScaler()\n",
        "\n",
        "# create a differenced series as done in step 1 (see link for more info)\n",
        "def difference(dataset, interval=1):\n",
        "\tdiff = list()\n",
        "\tfor i in range(interval, len(dataset)):\n",
        "\t\tvalue = np.log(dataset[i]) - np.log(dataset[i - interval])\n",
        "\t\tdiff.append(value)\n",
        "\treturn diff\n",
        " \n",
        "# training data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/StockMarketData/archive/Data/ETFs/spy.us.txt') # load csv data from directory\n",
        "df = df.sort_values('Date')\n",
        "data=df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "# making OHLC data stationary before calculating relative and normalizing \n",
        "diff_o = np.array(difference(data['Open'], 1))\n",
        "diff_h = np.array(difference(data['High'], 1))\n",
        "diff_l = np.array(difference(data['Low'], 1))\n",
        "diff_c = np.array(difference(data['Close'], 1))\n",
        "# volumne data\n",
        "vol = data['Volume'].values/MAX_NUM_SHARES\n",
        "# year data of year-month-day form\n",
        "dt = data['Date'].array\n",
        "# calculating relative prices and normalizing data\n",
        "o =  (diff_o-diff_l)/(diff_h-diff_l)\n",
        "o =  x.fit_transform(o.reshape(-1,1)).reshape(-1)\n",
        "rc = (diff_c-diff_l)/(diff_h-diff_l)\n",
        "rc = x.fit_transform(rc.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "rh = x.fit_transform(diff_h.reshape(-1,1)).reshape(-1)\n",
        "rl = x.fit_transform(diff_l.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "Train_Data = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume', 'real_open',  'real_close', 'real_high', 'real_low', 'real_vol'])\n",
        "train = Train_Data(date=dt,high=rh, low=rl, close=rc, open=o, volume=vol, real_open=data['Open'].values, real_close=data['Close'].values, real_high=data['High'].values, real_low=data['Low'].values, real_vol=data['Volume'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1TdubrWOfamD"
      },
      "outputs": [],
      "source": [
        "# Testing data\n",
        "test = pd.read_csv('/content/drive/MyDrive/Datasets/StockMarketData/test.csv') # load csv data from directory\n",
        "t_df = test.sort_values('Date')\n",
        "data_two=t_df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "# making OHLC data stationary before calculating relative and normalizing \n",
        "diff_o = np.array(difference(data_two['Open'], 1))\n",
        "diff_h = np.array(difference(data_two['High'], 1))\n",
        "diff_l = np.array(difference(data_two['Low'], 1))\n",
        "diff_c = np.array(difference(data_two['Close'], 1))\n",
        "# volumne data\n",
        "vol = data_two['Volume'].values/MAX_NUM_SHARES\n",
        "# year data of year-month-day form\n",
        "dt = data_two['Date'].array\n",
        "# calculating relative prices and normalizing data\n",
        "o =  (diff_o-diff_l)/(diff_h-diff_l)\n",
        "o =  x.fit_transform(o.reshape(-1,1)).reshape(-1)\n",
        "rc = (diff_c-diff_l)/(diff_h-diff_l)\n",
        "rc = x.fit_transform(rc.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "rh = x.fit_transform(diff_h.reshape(-1,1)).reshape(-1)\n",
        "rl = x.fit_transform(diff_l.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "Test_Data = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume', 'real_open', 'real_close', 'real_high', 'real_low', 'real_vol'])\n",
        "test = Test_Data(date=dt,high=rh, low=rl, close=rc, open=o, volume=vol, real_open=data['Open'].values, real_close=data_two['Close'].values, real_high=data_two['High'].values, real_low=data_two['Low'].values, real_vol=data['Volume'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yandex Data\n",
        "\n",
        "* 10 frames make up a minute\n",
        "* spans from beginning of 2015 to end of 2016\n",
        "* training data consists of 196,581 samples\n",
        "* testing data consists of 65,527 samples"
      ],
      "metadata": {
        "id": "WG5ZWUQVPFxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvf /content/drive/MyDrive/Datasets/StockMarketData/ch08-small-quotes/ch08-small-quotes.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZvV5ESnOsn4",
        "outputId": "92009305-12af-4fea-8e84-df0c6ca628d2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YNDX_150101_151231.csv\n",
            "YNDX_160101_161231.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading data\n",
        "df_train = pd.read_csv('/content/YNDX_150101_151231.csv') \n",
        "df_test = pd.read_csv('/content/YNDX_160101_161231.csv')\n",
        "combine = pd.concat([df_train, df_test], axis=0)\n",
        "X_train, X_test = train_test_split(combine, test_size=0.25, shuffle=False)"
      ],
      "metadata": {
        "id": "WCjujGGrUvrx"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=X_train[['<TIME>', '<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<VOL>']]\n",
        "X_test=X_test[['<TIME>', '<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<VOL>']]"
      ],
      "metadata": {
        "id": "1z_EWuvRmHJj"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ydk=preprocessing.MinMaxScaler()\n",
        "\n",
        "# making OHLC data stationary before normalizing \n",
        "diff_o = np.array(X_train['<OPEN>'].diff().dropna())\n",
        "diff_h = np.array(X_train['<HIGH>'].diff().dropna())\n",
        "diff_l = np.array(X_train['<LOW>'].diff().dropna())\n",
        "diff_c = np.array(X_train['<CLOSE>'].diff().dropna())\n",
        "# volumne data\n",
        "vol = X_train['<VOL>'].values/70000\n",
        "# Time in seconds\n",
        "dt = X_train['<TIME>'].array\n",
        "\n",
        "# normalizing data\n",
        "o =  ydk.fit_transform(diff_o.reshape(-1,1)).reshape(-1)\n",
        "c =  ydk.fit_transform(diff_c.reshape(-1,1)).reshape(-1)\n",
        "h =  ydk.fit_transform(diff_h.reshape(-1,1)).reshape(-1)\n",
        "l =  ydk.fit_transform(diff_l.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "# Train_Data = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume', 'real_open',  'real_close', 'real_high', 'real_low', 'real_vol'])\n",
        "# train = Train_Data(date=dt,high=rh, low=rl, close=rc, open=o, volume=vol, real_open=X_train['<OPEN>'].values, real_close=X_train['<CLOSE>'].values, real_high=X_train['<HIGH>'].values, real_low=X_train['<LOW>'].values, real_vol=X_train['<VOL>'].values)"
      ],
      "metadata": {
        "id": "44cg5Iq9QN2V"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=diff_c > 1\n",
        "p=diff_c < 0"
      ],
      "metadata": {
        "id": "uEdhYgs1e5vJ"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxkFyvqbl94F",
        "outputId": "fcc116b3-70fc-4230-ef2e-f8e36d4453a9"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14927"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmqsG6UHmBca",
        "outputId": "875bef84-4458-42fb-cfa4-b0bb187e8cae"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30324"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Seperate Policy/Value Network Class using a 1D CNN Feature extractor\n",
        "\n",
        "Stable-baselines3's lists the following blog on PPO [37 implementation details of PPO](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) which breaks down the different implementations of PPO. Furthermore, as the authors of [WHAT MATTERS FOR ON-POLICY DEEP ACTOR CRITIC METHODS? A LARGE-SCALE STUDY](https://openreview.net/pdf?id=nIAxjsniDzg) detail: \n",
        "\n",
        "\n",
        "> Separate value and policy networks (C47) appear to lead to better performance on our out of five environments (Fig. 15). To avoid analyzing the other choices based on bad models, we thus focus for the rest of this experiment only on agents with separate value and policy networks. Regarding network sizes, the optimal width of the policy MLP depends on the complexity of the environment (Fig. 18) and too low or too high values can cause significant drop in performance while for the value function there seems to be no downside in using wider networks (Fig. 21). Moreover, on some environments it is beneficial to make the value network wider than the policy one, e.g. on HalfCheetah the best results are achieved with 16 − 32 units per layer in the policy network and 256\n",
        "in the value network. Two hidden layers appear to work well for policy (Fig. 22) and value networks (Fig. 20) in all tested environments. As for activation functions, we observe that tanh activations perform best and relu worst\n",
        "\n",
        "> Interestingly, the initial policy appears to have a surprisingly high impact on the training performance.The key recipe is to initialize the policy at the beginning of training so that the action distribution is centered around 0<sup>10</sup> regardless of the observation and has a rather small standard deviation. This can be achieved by initializing the policy MLP with smaller weights in the last layer (C57, Fig. 24, this alone boosts the performance on Humanoid by 66%) so that the initial action distribution is almost\n",
        "independent of the observation...Other choices appear to be less important: The scale of the last layer initialization matters much less for the value MLP (C58) than for the policy MLP (Fig. 19). Apart from the last layer scaling, the\n",
        "network initialization scheme (C56) does not matter too much (Fig. 27)\n",
        "\n",
        "> Recommendation. Initialize the last policy layer with 100× smaller weights. Use softplus to transform network output into action standard deviation and add a (negative) offset to its input to decrease the initial standard deviation of actions. Tune this offset if possible. Use tanh both as the activation function (if the networks are not too deep) and to transform the samples from the normal\n",
        "distribution to the bounded action space. Use a wide value MLP (no layers shared with the policy) but tune the policy width (it might need to be narrower than the value MLP)\n",
        "\n",
        "With these statements in mind, I decided to implement a seperate-network architecture for PPO for both the policy and value network as detailed below. \n",
        "\n",
        "## 1D CNN Feature Extractor\n",
        "\n",
        "This is the architecture used for the Policy's feature extractor: \n",
        "```\n",
        "self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input, 32, kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(32, 64, kernel_size=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.Flatten(),\n",
        "```\n",
        "This is the architecture used for the Value's feature extractor: \n",
        "```\n",
        "self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input,128,kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 256, kernel_size=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "```\n",
        "## Policy Network\n",
        "This is the architecture used for the Policy Network: \n",
        "```\n",
        "  self.policy_net = nn.Sequential(\n",
        "            layer_init(nn.Linear(feature_dim, 32)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(32, last_layer_dim_pi), std=0.01),\n",
        "            nn.Tanh(),  \n",
        "        )\n",
        "```\n",
        "## Value Network \n",
        "This is the architecture used for the Value Network:\n",
        "```\n",
        "self.value_net = nn.Sequential(\n",
        "           layer_init(nn.Linear(256, 256)),\n",
        "           nn.Tanh(),\n",
        "           layer_init(nn.Linear(256, 128)),\n",
        "           nn.Tanh(),\n",
        "           layer_init(nn.Linear(128,32)),\n",
        "           nn.Tanh(),\n",
        "           layer_init(nn.Linear(32,last_layer_dim_vf)),\n",
        "           nn.Tanh(),\n",
        "           )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_-5jadJfRbDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    th.nn.init.orthogonal_(layer.weight, std)\n",
        "    th.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class StandkePolicyExtractor(BaseFeaturesExtractor):\n",
        "  def __init__(self, observation_space=gym.spaces.Box, features_dim=128):\n",
        "        super(StandkePolicyExtractor, self).__init__(observation_space, features_dim)\n",
        "        input = observation_space.shape[0]\n",
        "        # Feature Extractor\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input,16,kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(16, 32, kernel_size=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(\n",
        "                th.as_tensor(observation_space.sample()[None]).float()\n",
        "            ).shape[1]\n",
        "  \n",
        "  def forward(self, observations):\n",
        "    return self.cnn(observations)\n",
        "\n",
        "\n",
        "class StandkeValueExtractor(BaseFeaturesExtractor):\n",
        "  def __init__(self, observation_space=gym.spaces.Box, features_dim=128):\n",
        "        super(StandkeValueExtractor, self).__init__(observation_space, features_dim)\n",
        "        input = observation_space.shape[0]\n",
        "        # Feature Extractor\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input,128,kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 256, kernel_size=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(\n",
        "                th.as_tensor(observation_space.sample()[None]).float()\n",
        "            ).shape[1]\n",
        "\n",
        "        \n",
        "  def forward(self, observations):\n",
        "    return self.cnn(observations)\n",
        "\n",
        "\n",
        "\n",
        "class StandkeNetwork(nn.Module):\n",
        "  def __init__(self,feature_dim=32, last_layer_dim_pi=2, last_layer_dim_vf=1):\n",
        "        super(StandkeNetwork, self).__init__()\n",
        "\n",
        "        # IMPORTANT:\n",
        "        # Save output dimensions, used to create the distributions\n",
        "        self.latent_dim_pi = last_layer_dim_pi\n",
        "        self.latent_dim_vf = last_layer_dim_vf\n",
        "\n",
        "         # Policy Network\n",
        "        self.policy_net = nn.Sequential(\n",
        "            layer_init(nn.Linear(feature_dim, 32)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(32, last_layer_dim_pi), std=0.01),\n",
        "            nn.Tanh(),  \n",
        "        )\n",
        "\n",
        "\n",
        "         # Value Network\n",
        "        self.value_net = nn.Sequential(\n",
        "           layer_init(nn.Linear(256, 256)),\n",
        "           nn.Tanh(),\n",
        "           layer_init(nn.Linear(256, 128)),\n",
        "           nn.Tanh(),\n",
        "           layer_init(nn.Linear(128,32)),\n",
        "           nn.Tanh(),\n",
        "           layer_init(nn.Linear(32,last_layer_dim_vf)),\n",
        "           nn.Tanh(),\n",
        "           )\n",
        "\n",
        "  def forward_actor(self, features: th.Tensor):\n",
        "    return self.policy_net(features)\n",
        "\n",
        "  def forward_critic(self, features: th.Tensor):\n",
        "    return self.value_net(features)\n",
        "  \n",
        "\n",
        "class StandkePolicy(ActorCriticPolicy):\n",
        "  def __init__(self, observation_space=gym.spaces.Box,action_space=gym.spaces.Box, lr_schedule=constant_fn(0.0003), activation_fn=nn.Tanh,*args,**kwargs):\n",
        "    \n",
        "        super(StandkePolicy, self).__init__(observation_space,action_space, lr_schedule, activation_fn,*args,**kwargs)\n",
        "        # non-shared features extractors \n",
        "        self.policy_features_extractor = StandkePolicyExtractor(observation_space)\n",
        "        self.value_features_extractor = StandkeValueExtractor(observation_space)\n",
        "        delattr(self, \"features_extractor\")  # remove the shared features extractor\n",
        "        # orthogonal initialization\n",
        "        self.ortho_init = False\n",
        "\n",
        "  def _build_mlp_extractor(self):\n",
        "    self.mlp_extractor = StandkeNetwork()\n",
        "\n",
        "  def extract_features(self, obs: th.Tensor):\n",
        "    policy_features = self.policy_features_extractor(obs)\n",
        "    value_features = self.value_features_extractor(obs)\n",
        "    return policy_features, value_features\n",
        "  \n",
        "  def forward(self, obs: th.Tensor, deterministic=False): \n",
        "    policy_features, value_features = self.extract_features(obs)\n",
        "    mu_pi = self.mlp_extractor.forward_actor(policy_features)\n",
        "    latent_vf = self.mlp_extractor.forward_critic(value_features)\n",
        "    # Evaluate the values for the given observations\n",
        "    distribution = self._get_action_dist_from_latent(mu_pi)\n",
        "    actions = distribution.get_actions(deterministic=deterministic)\n",
        "    log_prob = distribution.log_prob(actions)\n",
        "    vf = latent_vf\n",
        "    return actions, vf, log_prob\n",
        "\n",
        "  def evaluate_actions(self, obs: th.Tensor, actions: th.Tensor): \n",
        "    policy_features, value_features = self.extract_features(obs)\n",
        "    mu_pi = self.mlp_extractor.forward_actor(policy_features)\n",
        "    latent_vf = self.mlp_extractor.forward_critic(value_features)\n",
        "    distribution = self._get_action_dist_from_latent(mu_pi)\n",
        "    actions = distribution.get_actions(deterministic=False)\n",
        "    log_prob = distribution.log_prob(actions)\n",
        "    vf = latent_vf\n",
        "    return vf, log_prob, distribution.entropy()\n",
        "\n",
        "  def get_distribution(self, obs: th.Tensor):\n",
        "    policy_features, _ = self.extract_features(obs)\n",
        "    latent_pi = self.mlp_extractor.forward_actor(policy_features)\n",
        "    return self._get_action_dist_from_latent(latent_pi)\n",
        "\n",
        "  def predict_values(self, obs: th.Tensor):\n",
        "    _, value_features = self.extract_features(obs)\n",
        "    latent_vf = self.mlp_extractor.forward_critic(value_features)\n",
        "    return latent_vf"
      ],
      "metadata": {
        "id": "572gmNd6Q-kr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorboard Callback to graph Networth on Test Data"
      ],
      "metadata": {
        "id": "PhSzVq3lcvN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(model,env,n_eval_episodes=5,deterministic=False):\n",
        "    episode_rewards, networths = [], []\n",
        "    for i in range(n_eval_episodes):\n",
        "        done, state = False, None\n",
        "        episode_reward = 0.0\n",
        "        episode_length = 0\n",
        "        networth = 0\n",
        "        obs = env.reset()\n",
        "        while not done:\n",
        "            action, state = model.predict(obs, state=state, deterministic=deterministic)\n",
        "            new_obs, reward, done, _info = env.step(action)\n",
        "            obs = new_obs\n",
        "            episode_reward += reward\n",
        "            # networth += _info[0]['Net Worth']\n",
        "        episode_rewards.append(episode_reward)\n",
        "        networths.append(_info[0]['Net Worth'])\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    mean_networth = np.mean(networths)\n",
        "    return mean_reward, mean_networth\n",
        "\n",
        "\n",
        "class TensorboardCallback(BaseCallback):\n",
        "    def __init__(self, env, eval,  verbose=1):\n",
        "        super(TensorboardCallback, self).__init__(verbose)\n",
        "        self.eval= eval\n",
        "        self.env = env\n",
        "\n",
        "    def _on_step(self):\n",
        "      if (self.num_timesteps % self.eval == 0):\n",
        "        _, net = evaluate_policy(self.model, self.env)\n",
        "        self.logger.record('Validation Mean Net Worth', net)\n",
        "      return True"
      ],
      "metadata": {
        "id": "1hEB-iRM4GLz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HyperParmater Tuning to Maxamize Networth\n",
        "\n",
        "Following the optimization sceme as outlined by Adam King in his article [Optimizing deep learning trading bots using state-of-the-art techniques](https://towardsdatascience.com/using-reinforcement-learning-to-trade-bitcoin-for-massive-profit-b69d0e8f583b) Bayesian Optimization was done using [Optuna](https://optuna.org/)\n",
        "After doing a categorical trial on the different rewards with different gamma values (i.e. the discount factor), sortinoRewardRatio was chosen as the defualt reward scheme, since it finished with the highest networth after 2800 trials. With the reward scheme in place, the following hyperparmater values were generated after 3000 trials:\n",
        "\n",
        "*  clip_range:  0.2618070956510424\n",
        "*  clip_range_vf: 0.38928293555959836\n",
        "*  ent_coef: 0.12121492884945033\n",
        "*  vf_coef: 0.3343061244258561\n",
        "\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "LKGZpOQ39eNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# objective is maxamize networth rather than reward\n",
        "\n",
        "def objective_fn(trial):\n",
        "    # env_params = optimize_envs(trial)\n",
        "    agent_params = optimize_ppo(trial)\n",
        "    \n",
        "    train_env = DummyVecEnv([lambda: StockTradingEnv(train, random=False)])\n",
        "    validation_env = DummyVecEnv([lambda: StockTradingEnv(test, random=False)])\n",
        "    model = PPO(StandkePolicy, train_env, **agent_params)\n",
        "    \n",
        "    model.learn(len(train_env.get_attr('data')[0].date)) # trains based on length of data \n",
        "                                                         # approx 3000\n",
        "    done, networth  = False, []\n",
        "    obs = validation_env.reset()\n",
        "    for i in range(len(validation_env.get_attr('data')[0].date)):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, reward, done, _info = validation_env.step(action)\n",
        "        networth.append(_info[0]['Net Worth'])\n",
        "        if done:\n",
        "          break\n",
        "    return np.mean(networth)\n",
        "\n",
        "def optimize_ppo(trial):\n",
        "    return {\n",
        "        # 'gamma': trial.suggest_float('gamma1', 0.1, 0.9, log=True),\n",
        "        'gamma': 0.20423582857078215,\n",
        "        'clip_range': trial.suggest_float('clip_range1', 0.1, 0.5, log=True),\n",
        "        'clip_range_vf': trial.suggest_float('clip_range_vf2', 0.1, 0.5, log=True),\n",
        "        'ent_coef':trial.suggest_float('ent_coef3', 0.1, 0.9, log=True), \n",
        "        'vf_coef':trial.suggest_float('vf_coef4', 0.1, 0.9, log=True),\n",
        "        # 'target_kl': trial.suggest_float('target_kl5', 0.01, 0.05, log=True),\n",
        "    }\n",
        "\n",
        "# def optimize_envs(trial):\n",
        "#     return {'reward_func': trial.suggest_categorical('reward_func1', ['BalenceReward', 'sortinoRewardRatio', 'calmarRewardRatio', 'omegaRewardRatio', 'StandkeCurrentValueReward', 'StandkeSmallDrawDownReward','StandkeSumofDifferenceReward'])}\n",
        "\n",
        "\n",
        "study = optuna.create_study(study_name='StockEnvPPO_Parms', direction=\"maximize\", storage=\"sqlite:///PPOhyper.db\", load_if_exists=True)\n",
        "study.optimize(objective_fn, n_trials=3000, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "eskm_ga99dme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # loading the hyperparmeters for training\n",
        "# study = optuna.load_study(study_name='StockEnvPPO_Parms', storage='sqlite:///PPOhyper.db')\n",
        "# params = study.best_trial.params\n",
        "\n",
        "# # other hyperparameters after 3000 trials given gamma and sortinoRewardRatio \n",
        "# model_params = {\n",
        "#     'clip_range': params['clip_range1'],\n",
        "#     'ent_coef': params['ent_coef3'],\n",
        "#     'clip_range_vf': params['clip_range_vf2'],\n",
        "#     'vf_coef': params['vf_coef4']\n",
        "# }\n",
        "\n",
        "# # gamma parameter after 2853 trials under the sortinoRewardRatio \n",
        "# model_params['gamma'] = 0.20423582857078215"
      ],
      "metadata": {
        "id": "jMluKzP-E1u5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = {}"
      ],
      "metadata": {
        "id": "lvyb0Q83yoim"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg2HNywdyLaO"
      },
      "source": [
        "# Training and Validation Portion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FEQ = MAX_STEPS/100\n",
        "\n",
        "# the number of concurrent environments for training  \n",
        "ENV = 1\n",
        "\n",
        "# different model reward schemes\n",
        "# MODEL = \"BalenceReward\"\n",
        "MODEL = \"sortinoRewardRatio\"\n",
        "# MODEL = \"calmarRewardRatio\"\n",
        "# MODEL = \"omegaRewardRatio\"\n",
        "# MODEL = \"StandkeCurrentValueReward\"\n",
        "# MODEL = \"StandkeSmallDrawDownReward\"\n",
        "# MODEL = \"StandkeSumofDifferenceReward\"\n",
        "env_params = {'reward_func': MODEL}"
      ],
      "metadata": {
        "id": "wLYFzy2-HWYI"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "cAJYlkEqIwcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa7cda0-e9ff-46cf-8fb2-bf2e7161a127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan at 0x7fd720b85190>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# create evaluation env that takes in test data that saves best model \n",
        "eval_env = DummyVecEnv([lambda: StockTradingEnv(test, **env_params, random=False)])\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=f'/content/drive/MyDrive/RLmodels/bestPPO/{MODEL}',\n",
        "                             log_path='/content/drive/MyDrive/RLmodels/logs/', eval_freq=FEQ,\n",
        "                             deterministic=False, render=False, n_eval_episodes=5)\n",
        "\n",
        "# create training envs that takes in training data for training\n",
        "envs =  DummyVecEnv([lambda: StockTradingEnv(train, **env_params, random=True) for _ in range(0,ENV)])\n",
        "\n",
        "'''training model using the Seperate Standke Policy/Value network'''\n",
        "# optional additional keyword parameters to pass to model \n",
        "policy_kwargs = dict()\n",
        "\n",
        "# creating callback list for tracking purposes \n",
        "callback = CallbackList([TensorboardCallback(eval_env, FEQ), eval_callback])\n",
        "\n",
        "model = PPO(StandkePolicy, envs, **model_params, verbose=1, tensorboard_log=f\"/content/PPO_SPY_tensorboard/{MODEL}\", policy_kwargs=policy_kwargs)\n",
        "\n",
        "# check to make sure no erros in the env, such as observation space errors or nans\n",
        "check_env(StockTradingEnv(train))\n",
        "VecCheckNan(envs, raise_exception=True, check_inf=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General explanation of log output \n",
        "\n",
        "As detailed by araffin in his commit [Add explanation of logger output](https://github.com/DLR-RM/stable-baselines3/pull/803/files), for a given log block such as\n",
        "\n",
        "```\n",
        "-----------------------------------------\n",
        "  | eval/                   |             |\n",
        "  |    mean_ep_length       | 200         |\n",
        "  |    mean_reward          | -157        |\n",
        "  | rollout/                |             |\n",
        "  |    ep_len_mean          | 200         |\n",
        "  |    ep_rew_mean          | -227        |\n",
        "  | time/                   |             |\n",
        "  |    fps                  | 972         |\n",
        "  |    iterations           | 19          |\n",
        "  |    time_elapsed         | 80          |\n",
        "  |    total_timesteps      | 77824       |\n",
        "  | train/                  |             |\n",
        "  |    approx_kl            | 0.037781604 |\n",
        "  |    clip_fraction        | 0.243       |\n",
        "  |    clip_range           | 0.2         |\n",
        "  |    entropy_loss         | -1.06       |\n",
        "  |    explained_variance   | 0.999       |\n",
        "  |    learning_rate        | 0.001       |\n",
        "  |    loss                 | 0.245       |\n",
        "  |    n_updates            | 180         |\n",
        "  |    policy_gradient_loss | -0.00398    |\n",
        "  |    std                  | 0.205       |\n",
        "  |    value_loss           | 0.226       |\n",
        "  -----------------------------------------\n",
        "```\n",
        "``eval/`` \n",
        "- ``mean_ep_length``: Mean episode length\n",
        "- ``mean_reward``: Mean episodic reward (during evaluation)\n",
        "``rollout/``\n",
        "- ``ep_len_mean``: Mean episode length (averaged over 100 episodes)\n",
        "- ``ep_rew_mean``: Mean episodic training reward (averaged over 100 episodes)\n",
        "``time/``\n",
        "- ``episodes``: Total number of episodes\n",
        "- ``fps``: Number of frames per seconds (includes time taken by gradient update)\n",
        "- ``iterations``: Number of iterations (data collection + policy update for A2C/PPO)\n",
        "- ``time_elapsed``: Time in seconds since the beginning of training\n",
        "- ``total_timesteps``: Total number of timesteps (steps in the environments)\n",
        "``train/``\n",
        "- ``clip_fraction``: mean fraction of surrogate loss that was clipped (above clip_range threshold) for PPO.\n",
        "- ``clip_range``: Current value of the clipping factor for the surrogate loss of PPO\n",
        "- ``learning_rate``: Current learning rate value\n",
        "- ``n_updates``: Number of gradient updates applied so far\n",
        "- ``policy_gradient_loss``: Current value of the policy gradient loss (its value does not have much meaning)\n",
        "- ``std``: Current standard deviation of the noise when using generalized State-Dependent Exploration (gSDE) (which by default is not used)\n",
        "\n",
        "# Important Training Metrics\n",
        "- ``approx_kl``: approximate mean KL divergence between old and new policy (for PPO), it is an estimation of how much change happened in the update (i.e. information gain or loss)\n",
        "  * **Want this value to SMOOTHLY decrease during training and be as close as possible to 0**\n",
        "- ``explained_variance``: Fraction of the return variance explained by the value function.\n",
        "  * **Should be SMOOTHLY increasing, and technically want values closer to 1 (but statistic interpertation is debatable because even with 0 the model does have predictive power, so the correct interpertaion of this statistic should be: *explained variation measures the proportion to which a mathematical model ACCOUNTS FOR THE VARIATION,not predictive power* [Explained Variation](https://en.wikipedia.org/wiki/Explained_variation)\"**\n",
        "- ``entropy_loss``: Mean value of the entropy loss (negative of the average policy entropy). \n",
        "  * **Should be MOVING**\n",
        "- ``loss``: called total loss is the the overall loss function\n",
        "  * **Should be SMOOTHLY decreasing, and technically, want to minimize this during training(though as discussed this isn't always possible due to randomness)**\n",
        "- ``value_loss``: error that value function is incurring \n",
        "  * **Should be SMOOTHLY decreasing, and technically,want to minimize this during training(though as discussed this isn't always possible due to randomness) **\n",
        "\n",
        "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
        "# Standke's Order of Imporance Regarding PPO Metrics To Focus On When Trading in Continous Environments!!!!!!!\n",
        "\n",
        "1.  value_loss & loss\n",
        "2.  entropy_loss\n",
        "3.  approx_kl\n",
        "4.  explained_variance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9ATRRLz4RsLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "O6sMSFIuhLzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "550f833e-fea2-4b6f-8a0a-f1c9472c769e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /content/PPO_SPY_tensorboard/sortinoRewardRatio/PPO_12\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 463  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 4    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 363       |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 11        |\n",
            "|    total_timesteps      | 4096      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.0927541 |\n",
            "|    clip_fraction        | 0.815     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.94     |\n",
            "|    explained_variance   | -4.77e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 5.71e+06  |\n",
            "|    n_updates            | 10        |\n",
            "|    policy_gradient_loss | 0.474     |\n",
            "|    std                  | 1.11      |\n",
            "|    value_loss           | 1.54e+08  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 338       |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 18        |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.1897771 |\n",
            "|    clip_fraction        | 0.819     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -3.14     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.57e+05  |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | 0.782     |\n",
            "|    std                  | 1.22      |\n",
            "|    value_loss           | 2.29e+05  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 326       |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 25        |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 1.9561832 |\n",
            "|    clip_fraction        | 0.817     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -3.34     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 3.54e+04  |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | 0.675     |\n",
            "|    std                  | 1.35      |\n",
            "|    value_loss           | 1.14e+05  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=64620.79 +/- 45640.33\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| Validation Mean Net Worth | 1.31e+04  |\n",
            "| eval/                     |           |\n",
            "|    mean_ep_length         | 1.2e+03   |\n",
            "|    mean_reward            | 6.46e+04  |\n",
            "| time/                     |           |\n",
            "|    total_timesteps        | 10000     |\n",
            "| train/                    |           |\n",
            "|    approx_kl              | 16.880959 |\n",
            "|    clip_fraction          | 0.823     |\n",
            "|    clip_range             | 0.2       |\n",
            "|    entropy_loss           | -3.53     |\n",
            "|    explained_variance     | 0         |\n",
            "|    learning_rate          | 0.0003    |\n",
            "|    loss                   | 2.92e+04  |\n",
            "|    n_updates              | 40        |\n",
            "|    policy_gradient_loss   | 1.59      |\n",
            "|    std                    | 1.49      |\n",
            "|    value_loss             | 9.99e+04  |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 190   |\n",
            "|    iterations      | 5     |\n",
            "|    time_elapsed    | 53    |\n",
            "|    total_timesteps | 10240 |\n",
            "------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 202       |\n",
            "|    iterations           | 6         |\n",
            "|    time_elapsed         | 60        |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.0100067 |\n",
            "|    clip_fraction        | 0.818     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -3.72     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 9.11e+07  |\n",
            "|    n_updates            | 50        |\n",
            "|    policy_gradient_loss | 0.442     |\n",
            "|    std                  | 1.62      |\n",
            "|    value_loss           | 8.39e+07  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 211       |\n",
            "|    iterations           | 7         |\n",
            "|    time_elapsed         | 67        |\n",
            "|    total_timesteps      | 14336     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 3.4186783 |\n",
            "|    clip_fraction        | 0.825     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -3.9      |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 9.16e+06  |\n",
            "|    n_updates            | 60        |\n",
            "|    policy_gradient_loss | 0.903     |\n",
            "|    std                  | 1.79      |\n",
            "|    value_loss           | 1.63e+08  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 219       |\n",
            "|    iterations           | 8         |\n",
            "|    time_elapsed         | 74        |\n",
            "|    total_timesteps      | 16384     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.0214396 |\n",
            "|    clip_fraction        | 0.819     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -4.09     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 3.6e+07   |\n",
            "|    n_updates            | 70        |\n",
            "|    policy_gradient_loss | 0.546     |\n",
            "|    std                  | 1.95      |\n",
            "|    value_loss           | 1.52e+08  |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 225      |\n",
            "|    iterations           | 9        |\n",
            "|    time_elapsed         | 81       |\n",
            "|    total_timesteps      | 18432    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 7.431927 |\n",
            "|    clip_fraction        | 0.823    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -4.28    |\n",
            "|    explained_variance   | 0        |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 1.67e+05 |\n",
            "|    n_updates            | 80       |\n",
            "|    policy_gradient_loss | 1.31     |\n",
            "|    std                  | 2.15     |\n",
            "|    value_loss           | 2.42e+05 |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=150270.50 +/- 121996.90\n",
            "Episode length: 1198.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| Validation Mean Net Worth | 1.33e+04  |\n",
            "| eval/                     |           |\n",
            "|    mean_ep_length         | 1.2e+03   |\n",
            "|    mean_reward            | 1.5e+05   |\n",
            "| time/                     |           |\n",
            "|    total_timesteps        | 20000     |\n",
            "| train/                    |           |\n",
            "|    approx_kl              | 2.4878683 |\n",
            "|    clip_fraction          | 0.823     |\n",
            "|    clip_range             | 0.2       |\n",
            "|    entropy_loss           | -4.47     |\n",
            "|    explained_variance     | 0         |\n",
            "|    learning_rate          | 0.0003    |\n",
            "|    loss                   | 6.27e+06  |\n",
            "|    n_updates              | 90        |\n",
            "|    policy_gradient_loss   | 0.684     |\n",
            "|    std                    | 2.36      |\n",
            "|    value_loss             | 6.34e+07  |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 184   |\n",
            "|    iterations      | 10    |\n",
            "|    time_elapsed    | 111   |\n",
            "|    total_timesteps | 20480 |\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7fd720259dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "model.learn(total_timesteps=MAX_STEPS, callback=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction and CSV Printout of Agent's Trading Behaviour on Test Data\n",
        "\n",
        "\n",
        "## Model Statistics-Random Offset\n",
        "```\n",
        "Eval num_timesteps=270000, episode_reward=12437.74 +/- 5.91\n",
        "Episode length: 1198.00 +/- 0.00\n",
        "-----------------------------------------\n",
        "| Validation Mean Net Worth | 1.32e+04  |\n",
        "| eval/                     |           |\n",
        "|    mean_ep_length         | 1.2e+03   |\n",
        "|    mean_reward            | 1.24e+04  |\n",
        "| time/                     |           |\n",
        "|    total_timesteps        | 270000    |\n",
        "| train/                    |           |\n",
        "|    approx_kl              | 46.243534 |\n",
        "|    clip_fraction          | 0.761     |\n",
        "|    clip_range             | 0.262     |\n",
        "|    clip_range_vf          | 0.389     |\n",
        "|    entropy_loss           | -17.4     |\n",
        "|    explained_variance     | 0.0251    |\n",
        "|    learning_rate          | 0.0003    |\n",
        "|    loss                   | 263       |\n",
        "|    n_updates              | 1310      |\n",
        "|    policy_gradient_loss   | 3.56      |\n",
        "|    std                    | 1.53e+03  |\n",
        "|    value_loss             | 1.44e+03  |\n",
        "-----------------------------------------\n",
        "New best mean reward!\n",
        "```\n",
        "## Model Statistics-No Random Offset\n",
        "```\n",
        "Eval num_timesteps=220000, episode_reward=12336.98 +/- 77.83\n",
        "Episode length: 1198.00 +/- 0.00\n",
        "-----------------------------------------\n",
        "| Validation Mean Net Worth | 1.22e+04  |\n",
        "| eval/                     |           |\n",
        "|    mean_ep_length         | 1.2e+03   |\n",
        "|    mean_reward            | 1.23e+04  |\n",
        "| time/                     |           |\n",
        "|    total_timesteps        | 220000    |\n",
        "| train/                    |           |\n",
        "|    approx_kl              | 2.9549363 |\n",
        "|    clip_fraction          | 0.768     |\n",
        "|    clip_range             | 0.262     |\n",
        "|    clip_range_vf          | 0.389     |\n",
        "|    entropy_loss           | -14.5     |\n",
        "|    explained_variance     | 0.00241   |\n",
        "|    learning_rate          | 0.0003    |\n",
        "|    loss                   | 95.8      |\n",
        "|    n_updates              | 1070      |\n",
        "|    policy_gradient_loss   | 1.77      |\n",
        "|    std                    | 349       |\n",
        "|    value_loss             | 9.13e+03  |\n",
        "-----------------------------------------\n",
        "New best mean reward!\n",
        "```\n",
        "## Model Statistics-Random Offset-5 environments\n",
        "\n",
        "```\n",
        "Eval num_timesteps=200000, episode_reward=12421.62 +/- 18.28\n",
        "Episode length: 1198.00 +/- 0.00\n",
        "-----------------------------------------\n",
        "| Validation Mean Net Worth | 1.36e+04  |\n",
        "| eval/                     |           |\n",
        "|    mean_ep_length         | 1.2e+03   |\n",
        "|    mean_reward            | 1.24e+04  |\n",
        "| time/                     |           |\n",
        "|    total_timesteps        | 200000    |\n",
        "| train/                    |           |\n",
        "|    approx_kl              | 2.1056263 |\n",
        "|    clip_fraction          | 0.826     |\n",
        "|    clip_range             | 0.262     |\n",
        "|    clip_range_vf          | 0.389     |\n",
        "|    entropy_loss           | -20.1     |\n",
        "|    explained_variance     | 0         |\n",
        "|    learning_rate          | 0.0003    |\n",
        "|    loss                   | 176       |\n",
        "|    n_updates              | 190       |\n",
        "|    policy_gradient_loss   | 0.666     |\n",
        "|    std                    | 6.95e+03  |\n",
        "|    value_loss             | 3.02e+03  |\n",
        "-----------------------------------------\n",
        "New best mean reward!\n",
        "```\n",
        "\n",
        "## Model Statistics-No Random Offset-5 environments\n",
        "```\n",
        "Eval num_timesteps=200000, episode_reward=12399.05 +/- 24.77\n",
        "Episode length: 1198.00 +/- 0.00\n",
        "-----------------------------------------\n",
        "| Validation Mean Net Worth | 1.31e+04  |\n",
        "| eval/                     |           |\n",
        "|    mean_ep_length         | 1.2e+03   |\n",
        "|    mean_reward            | 1.24e+04  |\n",
        "| time/                     |           |\n",
        "|    total_timesteps        | 200000    |\n",
        "| train/                    |           |\n",
        "|    approx_kl              | 1.7766869 |\n",
        "|    clip_fraction          | 0.824     |\n",
        "|    clip_range             | 0.262     |\n",
        "|    clip_range_vf          | 0.389     |\n",
        "|    entropy_loss           | -20.1     |\n",
        "|    explained_variance     | 0         |\n",
        "|    learning_rate          | 0.0003    |\n",
        "|    loss                   | 68.3      |\n",
        "|    n_updates              | 190       |\n",
        "|    policy_gradient_loss   | 0.814     |\n",
        "|    std                    | 6.86e+03  |\n",
        "|    value_loss             | 9.26e+03  |\n",
        "-----------------------------------------\n",
        "New best mean reward!\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p7JobsGk8f25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model-Random Offset\n",
        "# model = PPO.load(f\"/content/drive/MyDrive/RLmodels/bestPPO/{MODEL}/best_model_random_offset.zip\")\n",
        "# # Model-No Random Offset\n",
        "# model = PPO.load(f\"/content/drive/MyDrive/RLmodels/bestPPO/{MODEL}/best_model.zip\")\n",
        "env = StockTradingEnv(test, **env_params, random=False)\n",
        "obs = env.reset()\n",
        "for i in range(len(test.date)):\n",
        "  action, _states = model.predict(obs, deterministic=False)\n",
        "  obs, rewards, done, info = env.step(action)\n",
        "  env.render()\n",
        "  if done:\n",
        "    break"
      ],
      "metadata": {
        "id": "BgbcsONJQqI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorBoard Analysis"
      ],
      "metadata": {
        "id": "hqTY07IM0Dyx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n02iHgNDTvYJ"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/PPO_SPY_tensorboard/ "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvf /content/drive/MyDrive/Datasets/StockMarketData/ch08-small-quotes/ch08-small-quotes.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXZ92Qp5uWE7",
        "outputId": "dfc7946b-c931-4c6e-ac19-7455d01f873c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YNDX_150101_151231.csv\n",
            "YNDX_160101_161231.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1BnRqmo4sw9dhUsiP_WO4HGPM9M7Cryi7",
      "authorship_tag": "ABX9TyPjhx5TwoMK0Z7QnMq0ZDww",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}