{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aCStandke/ReinforcementLearning/blob/main/ThirdStockEnivornment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TBwoqXizywK"
      },
      "source": [
        "# Third Stock Trading Environment\n",
        "\n",
        "\n",
        "  This third stock trading environment is based on Adam King's article as found here:[Creating Bitcoin trading bots don’t lose money](https://medium.com/towards-data-science/creating-bitcoin-trading-bots-that-dont-lose-money-2e7165fb0b29). Similar to the first stock trading environment based on Maxim Lapan's implementation as found in chapter eight of his book [Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition](https://www.amazon.com/Deep-Reinforcement-Learning-Hands-optimization/dp/1838826998), the agent is trading in the environment of the [SPY ETF](https://www.etf.com/SPY?L=1) except in this trading environment the agent is tasked with two discrete actions of not only buying, selling or holding shares but also tasked with determining the amount to buy/sell ranging from 1 to 100 (which will be converted into pecentage form i.e. 1/100=1%, 100/100=100%) based on its trading account/balance [trading account](https://www.investopedia.com/terms/t/tradingaccount.asp#:~:text=A%20trading%20account%20is%20an,margin%20requirements%20set%20by%20FINRA.).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OHO1Q4dS9tF0"
      },
      "outputs": [],
      "source": [
        "# ignore warning messages because they are annoying lol\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGbbfGz0KsDS"
      },
      "source": [
        "# Installing Necessary Package for Training the Trading Agent\n",
        "\n",
        "To train the Trading Agent the package [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/index.html) was used. As stated in the docs: \n",
        "> Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines. And steems from the paper [Stable-Baselines3: Reliable Reinforcement Learning Implementations](https://jmlr.org/papers/volume22/20-1364/20-1364.pdf)\n",
        "The algorithms in this package will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\n",
        "\n",
        "---\n",
        "## Proximal Policy Optimization(PPO):\n",
        "\n",
        "Because in this environment the Agent will be executing continous actions, the Proximal Policy Optimization(PPO) algorithm was chosen. As detailed by the authors [PPO](https://arxiv.org/pdf/1707.06347.pdf)\n",
        "\n",
        "\n",
        "> We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically).\n",
        "\n",
        "\n",
        "PPO uses the following novel objective function:\n",
        "\n",
        "$L^{CLIP}(θ)=\\hat{E}_t[min(r_{t}(θ)\\hat{A}_t,clip(r_{t}(θ), 1-ϵ, 1+ϵ)\\hat{A}_t]$\n",
        "\n",
        "*  $\\theta$ is the policy parameter\n",
        "*  $\\hat{E}_t$ denotes the empirical expectation over timesteps\n",
        "*  $r_{t}$ is the ratio of the probability under the new and old policies, respectively\n",
        "*  $\\hat{A}_t$ is the estimated advantage at time t\n",
        "*  $\\epsilon$ is the clipping hyperparameter, usually 0.1 or 0.2\n",
        "\n",
        "\n",
        "As detailed by the authors [openAI](https://openai.com/blog/openai-baselines-ppo/#ppo)\n",
        "\n",
        "\n",
        "> This objective implements a way to do a Trust Region update which is compatible with Stochastic Gradient Descent, and simplifies the algorithm by removing the KL penalty and need to make adaptive updates. In tests, this algorithm has displayed the best performance on continuous control tasks and almost matches ACER’s performance on Atari, despite being far simpler to implement\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZqZJWIuzLFDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79b16d9e-acec-4cd3-fd4e-e0b6db497ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.12.1+cu113)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Requirement already satisfied: gym==0.21 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.21.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
            "Requirement already satisfied: ale-py==0.7.4 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.7.4)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.10.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (4.12.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4->stable-baselines3[extra]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4->stable-baselines3[extra]) (3.8.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.48.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.37.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2022.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: empyrical in /usr/local/lib/python3.7/dist-packages (0.5.5)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from empyrical) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from empyrical) (1.3.5)\n",
            "Requirement already satisfied: scipy>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from empyrical) (1.7.3)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.7/dist-packages (from empyrical) (0.9.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.16.1->empyrical) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.16.1->empyrical) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader>=0.2->empyrical) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pandas-datareader>=0.2->empyrical) (4.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.16.1->empyrical) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.8.1)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.41)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (4.12.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.4.1)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.4.2)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (0.5.1)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.10.0)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3[extra]\n",
        "!pip install empyrical\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HDwRKzPLJWG"
      },
      "source": [
        "# Installing the Necessary Packages for Visualizing the Trading Agent's Envirnoment on Google Colab Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xT_p1bNSDnJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a761d9-58d4-4be0-d234-917557a89dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mpl_finance in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mpl_finance) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mpl_finance) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mpl_finance) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mpl_finance) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (0.2.3.5)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.64.1)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (2.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy) (1.21.6)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageio_ffmpeg in /usr/local/lib/python3.7/dist-packages (0.4.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install mpl_finance #used for plotting the candelstick graph\n",
        "!pip install moviepy #\n",
        "!pip install imageio_ffmpeg #\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1 #used to create a display for vm\n",
        "!apt-get install x11-utils > /dev/null 2>&1 #\n",
        "!pip install pyglet==v1.3.2 > /dev/null 2>&1 #\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1 #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AoLhA3_b_XeK"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import datetime as dt\n",
        "import optuna\n",
        "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.utils import constant_fn\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.env_util import DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_checker import VecCheckNan, check_env\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from empyrical import sortino_ratio, calmar_ratio, omega_ratio\n",
        "import sqlite3\n",
        "from sqlite3 import Error\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "import datetime\n",
        "from sklearn import preprocessing\n",
        "import math\n",
        "import os\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OB9FxIN_AQC4"
      },
      "outputs": [],
      "source": [
        "# stock environment parameters\n",
        "MAX_ACCOUNT_BALANCE = 2147483647\n",
        "MAX_NUM_SHARES = 2147483647\n",
        "MAX_SHARE_PRICE = 4294967295\n",
        "LOOKBACK_WINDOW_SIZE = 10\n",
        "MAX_STEPS = 20000\n",
        "INITIAL_ACCOUNT_BALANCE = 10000\n",
        "# default percentage of stock price trading agent pays broker when \n",
        "# buying/selling, default is 0.1% (i.e. very reasonable)\n",
        "DA_COMMISION = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dJiMYPDIAj3y"
      },
      "outputs": [],
      "source": [
        "# Stock/ETF Trading Enviornment\n",
        "class StockTradingEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, data):\n",
        "        super(StockTradingEnv, self).__init__()\n",
        "        self.data = data\n",
        "        self.scale = preprocessing.MinMaxScaler()\n",
        "        self.random_ofs_on_reset = True\n",
        "        self.reward_func = 'calmar'\n",
        "        self.bars_count = LOOKBACK_WINDOW_SIZE\n",
        "        self.commission = DA_COMMISION\n",
        "        self.hold= False\n",
        "\n",
        "        # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
        "        self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([3, 1]), dtype=np.float32)\n",
        "\n",
        "        # Prices contains the OHCL values for the last five prices the state \n",
        "        # space is 12 dim i.e. 6 x 2 = 12\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=1, shape=self.shape, dtype=np.float32)\n",
        "        \n",
        "        self.seed()\n",
        "\n",
        "    def reset(self):\n",
        "      # random offset portion \n",
        "      bars = self.bars_count\n",
        "      if self.random_ofs_on_reset:\n",
        "        offset = self.np_random.choice(self.data.high.shape[0]-bars*10)+bars\n",
        "      else:\n",
        "        offset = bars\n",
        "      self._reset(offset)\n",
        "      return self._next_observation()\n",
        "\n",
        "    def _reset(self, offset):\n",
        "      self.trades = []\n",
        "      self.balance = INITIAL_ACCOUNT_BALANCE\n",
        "      self.netWorth = INITIAL_ACCOUNT_BALANCE\n",
        "      self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
        "      self.standkeMaxBenchShares = 0\n",
        "      self.shares_held  = 0\n",
        "      self._offset = offset\n",
        "      # setting account history portion\n",
        "      self.account_history = np.repeat([[self.netWorth/MAX_ACCOUNT_BALANCE]], LOOKBACK_WINDOW_SIZE, axis=1)\n",
        "\n",
        "    # shape of observation space is 2D\n",
        "    @property\n",
        "    def shape(self):\n",
        "      return (6, self.bars_count)\n",
        "\n",
        "    def _next_observation(self):\n",
        "      res = np.zeros(shape=(6, self.bars_count), dtype=np.float32)\n",
        "      ofs = self.bars_count-1\n",
        "      res[0] = self.data.volume[self._offset-ofs:self._offset+1]\n",
        "      res[1] = self.data.high[self._offset-ofs:self._offset+1]\n",
        "      res[2] = self.data.low[self._offset-ofs:self._offset+1]\n",
        "      res[3] = self.data.open[self._offset-ofs:self._offset+1]\n",
        "      res[4] = self.account_history[0][-self.bars_count:]\n",
        "      res[5] = self.data.close[self._offset-ofs:self._offset+1]\n",
        "      res = np.float32(res)\n",
        "      return res\n",
        "       \n",
        "    def _take_action(self, action):\n",
        "      reward = 0\n",
        "      current_price = self._cur_close()\n",
        "      action_type = action[0]\n",
        "      amount = action[1]\n",
        "      \n",
        "      shares_bought = 0\n",
        "      shares_sold = 0\n",
        "      additional_cost = 0\n",
        "      sales = 0\n",
        "\n",
        "\n",
        "      if action_type < 1 :\n",
        "        # Buy amount % of balance in shares\n",
        "        total_possible = self.balance / (current_price * (1+self.commission))\n",
        "        shares_bought = total_possible * amount\n",
        "        additional_cost = shares_bought * current_price * (1+self.commission)\n",
        "        self.balance -= additional_cost\n",
        "        self.standkeMaxBenchShares += shares_bought\n",
        "        self.shares_held += shares_bought\n",
        "        \n",
        "        \n",
        "        # visualization portion\n",
        "        if shares_bought > 0:\n",
        "          self.trades.append({'step': self._offset, 'shares': shares_bought, \n",
        "                              'total': additional_cost, 'type': \"buy\"})\n",
        "          \n",
        "          \n",
        "      elif action_type < 2:\n",
        "        # Sell amount % of shares held\n",
        "        shares_sold = self.shares_held * amount  \n",
        "        sales = shares_sold * current_price * (1 - self.commission)\n",
        "        self.balance += sales\n",
        "        self.standkeMaxBenchShares -= shares_sold\n",
        "        self.shares_held -= shares_sold\n",
        "        \n",
        "\n",
        "        # visualization portion\n",
        "        if shares_sold > 0:\n",
        "          self.trades.append({'step': self._offset, 'shares': -shares_sold, \n",
        "                                  'total': shares_sold * current_price, 'type': \"sell\"})  \n",
        "          \n",
        "      \n",
        "      self.netWorth = self.balance + self.shares_held * current_price\n",
        "      \n",
        "      if self.netWorth > self.max_net_worth:\n",
        "        self.max_net_worth = self.netWorth\n",
        "\n",
        "      # updating account history\n",
        "      self.account_history = np.append(self.account_history, [[self.netWorth/MAX_ACCOUNT_BALANCE]], axis=1)\n",
        "      # Reward Calculations\n",
        "      returns = self.account_history[0][-self.bars_count:]\n",
        "      if self.reward_func == 'BalenceReward':\n",
        "        delay_modifier = (self._offset / MAX_STEPS)\n",
        "        reward = self.balance * delay_modifier\n",
        "      elif self.reward_func == 'sortinoRewardRatio':\n",
        "        ratio = sortino_ratio(returns, period=\"daily\")\n",
        "        if ratio < 0: \n",
        "          reward = 0\n",
        "        elif ratio > 0:\n",
        "          reward = 1\n",
        "        else:\n",
        "          reward= ratio\n",
        "      elif self.reward_func == 'calmarRewardRatio':\n",
        "        ratio = calmar_ratio(returns, period=\"daily\")\n",
        "        if ratio < 0: \n",
        "          reward = 0\n",
        "        elif ratio > 0:\n",
        "          reward = 1\n",
        "        else:\n",
        "          reward= ratio\n",
        "      elif self.reward_func == 'omegaRewardRatio':\n",
        "        ratio = omega_ratio(returns,  annualization=self.bars_count)\n",
        "        if ratio < 0: \n",
        "          reward = 0\n",
        "        elif ratio > 0:\n",
        "          reward = 1\n",
        "        else:\n",
        "          reward= ratio\n",
        "      elif self.reward_func == 'StandkeCurrentValueReward':\n",
        "        prev_net = returns[-2]\n",
        "        current_net = returns[-1]\n",
        "        if (current_net-prev_net)<0:\n",
        "          reward += 0\n",
        "        else:\n",
        "          reward += 1\n",
        "          if current_net > self.max_net_worth:\n",
        "            reward *= 10\n",
        "      elif self.reward_func == 'StandkeSmallDrawDownReward':\n",
        "        mx = np.max(returns)\n",
        "        mi = np.min(returns)\n",
        "        ratio = round(abs(mx-mi/mx), 1) \n",
        "        if math.isclose(ratio, 0.0):\n",
        "          reward = 11  \n",
        "        elif math.isclose(ratio, 0.1):\n",
        "          reward = 1/0.1 \n",
        "        elif math.isclose(ratio, 0.2):\n",
        "          reward = 1/0.2\n",
        "        elif math.isclose(ratio, 0.3):\n",
        "          reward = 1/0.3\n",
        "        elif math.isclose(ratio, 0.4):\n",
        "          reward = 1/0.4\n",
        "        elif math.isclose(ratio, 0.5):\n",
        "          reward = 1/0.5\n",
        "        elif math.isclose(ratio, 0.6):\n",
        "          reward = 1/0.6\n",
        "        elif math.isclose(ratio, 0.7):\n",
        "          reward = 1/0.7\n",
        "        elif math.isclose(ratio, 0.8):\n",
        "          reward = 1/0.8\n",
        "        elif math.isclose(ratio, 0.9):\n",
        "          reward = 1/0.9\n",
        "        else:\n",
        "          reward = 0\n",
        "      elif self.reward_func == 'StandkeLargeDrawDownReward':\n",
        "        mx = np.max(returns)\n",
        "        mi = np.min(returns)\n",
        "        ratio = round(abs(mx-mi/mx), 1) \n",
        "        if math.isclose(ratio, 0.0):\n",
        "          reward = 0\n",
        "        elif math.isclose(ratio, 0.1):\n",
        "          reward = 0.1 \n",
        "        elif math.isclose(ratio, 0.2):\n",
        "          reward = 0.2\n",
        "        elif math.isclose(ratio, 0.3):\n",
        "          reward = 0.3\n",
        "        elif math.isclose(ratio, 0.4):\n",
        "          reward = 0.4\n",
        "        elif math.isclose(ratio, 0.5):\n",
        "          reward = 0.5\n",
        "        elif math.isclose(ratio, 0.6):\n",
        "          reward = 0.6\n",
        "        elif math.isclose(ratio, 0.7):\n",
        "          reward = 0.7\n",
        "        elif math.isclose(ratio, 0.8):\n",
        "          reward = 0.8\n",
        "        elif math.isclose(ratio, 0.9):\n",
        "          reward = 0.9\n",
        "        else:\n",
        "          reward = 1\n",
        "      elif self.reward_func == 'StandkeSumofDifferenceReward':\n",
        "        reward = np.sum(np.diff(returns))\n",
        "      else:\n",
        "        reward = np.mean(returns)\n",
        "\n",
        "      return reward if abs(reward) != np.inf and not np.isnan(reward) else 0\n",
        "\n",
        "      \n",
        "    def _cur_close(self):\n",
        "      \"\"\"\n",
        "      Calculate real close price for the current bar\n",
        "      \"\"\"\n",
        "      return self.data.real_close[self._offset]\n",
        "\n",
        "    def step(self, action):\n",
        "      # Execute one time step within the environment\n",
        "      reward = self._take_action(action)\n",
        "    \n",
        "      self._offset += 1\n",
        "\n",
        "      if self._offset >= self.data.close.shape[0]-1 or self.netWorth <= 0 or self.netWorth>=MAX_ACCOUNT_BALANCE:\n",
        "        done=True\n",
        "      else:\n",
        "        done=False\n",
        "  \n",
        "      obs = self._next_observation()\n",
        "\n",
        "      info = {\"Net Worth\":self.netWorth, \"reward\": reward}\n",
        "      \n",
        "      return obs, reward, done, info\n",
        "\n",
        "    def _render_to_file(self, filename='render.txt'):\n",
        "      f = open(filename, 'a+')\n",
        "      f.write(f\"Step: {self._offset}\\n\")\n",
        "      f.write(f\"Date: {self.data.date[self._offset]}\\n\")\n",
        "      f.write(f\"Net Worth: {self.netWorth}\\n\")\n",
        "      f.write(f\"Balence: {self.balance}\\n\")\n",
        "      f.write(f\"StandkeMaxBenchShares: {self.standkeMaxBenchShares}\\n\")\n",
        "      f.close()\n",
        "    \n",
        "\n",
        "    def render(self, mode='file', title=\"Agent's Trading Screen\", **kwargs):\n",
        "      # Render the environment to the screen\n",
        "      if mode == 'file':\n",
        "        self._render_to_file()\n",
        "\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "      self.np_random, seed1 = seeding.np_random(seed)\n",
        "      seed2 = seeding.hash_seed(seed1+1) % 2**33\n",
        "      return [seed1, seed2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "szUR1sYHHEVl"
      },
      "outputs": [],
      "source": [
        "# using sklearn's min-max scaler for the relative high and low\n",
        "x=preprocessing.MinMaxScaler()\n",
        "\n",
        "# taken from https://machinelearningmastery.com/remove-trends-seasonality-difference-transform-python/\n",
        "# create a differenced series\n",
        "def difference(dataset, interval=1):\n",
        "\tdiff = list()\n",
        "\tfor i in range(interval, len(dataset)):\n",
        "\t\tvalue = np.log(dataset[i]) - np.log(dataset[i - interval])\n",
        "\t\tdiff.append(value)\n",
        "\treturn diff\n",
        " \n",
        "# training data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/StockMarketData/archive/Data/ETFs/spy.us.txt')\n",
        "df = df.sort_values('Date')\n",
        "data=df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "# making OHLC data stationary before calculating relative and normalizing \n",
        "diff_o = np.array(difference(data['Open'], 1))\n",
        "diff_h = np.array(difference(data['High'], 1))\n",
        "diff_l = np.array(difference(data['Low'], 1))\n",
        "diff_c = np.array(difference(data['Close'], 1))\n",
        "# volumne data\n",
        "vol = data['Volume'].values/MAX_NUM_SHARES\n",
        "# year data of year-month-day form\n",
        "dt = data['Date'].array\n",
        "# calculating relative prices and normalizing data\n",
        "o =  (diff_o-diff_l)/(diff_h-diff_l)\n",
        "o =  x.fit_transform(o.reshape(-1,1)).reshape(-1)\n",
        "rc = (diff_c-diff_l)/(diff_h-diff_l)\n",
        "rc = x.fit_transform(rc.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "rh = x.fit_transform(diff_h.reshape(-1,1)).reshape(-1)\n",
        "rl = x.fit_transform(diff_l.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "Train_Data = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume', 'real_open',  'real_close', 'real_high', 'real_low', 'real_vol'])\n",
        "train = Train_Data(date=dt,high=rh, low=rl, close=rc, open=o, volume=vol, real_open=data['Open'].values, real_close=data['Close'].values, real_high=data['High'].values, real_low=data['Low'].values, real_vol=data['Volume'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1TdubrWOfamD"
      },
      "outputs": [],
      "source": [
        "# Testing data\n",
        "test = pd.read_csv('/content/drive/MyDrive/Datasets/StockMarketData/test.csv')\n",
        "t_df = test.sort_values('Date')\n",
        "data_two=t_df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "# making OHLC data stationary before calculating relative and normalizing \n",
        "diff_o = np.array(difference(data_two['Open'], 1))\n",
        "diff_h = np.array(difference(data_two['High'], 1))\n",
        "diff_l = np.array(difference(data_two['Low'], 1))\n",
        "diff_c = np.array(difference(data_two['Close'], 1))\n",
        "# volumne data\n",
        "vol = data_two['Volume'].values/MAX_NUM_SHARES\n",
        "# year data of year-month-day form\n",
        "dt = data_two['Date'].array\n",
        "# calculating relative prices and normalizing data\n",
        "o =  (diff_o-diff_l)/(diff_h-diff_l)\n",
        "o =  x.fit_transform(o.reshape(-1,1)).reshape(-1)\n",
        "rc = (diff_c-diff_l)/(diff_h-diff_l)\n",
        "rc = x.fit_transform(rc.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "rh = x.fit_transform(diff_h.reshape(-1,1)).reshape(-1)\n",
        "rl = x.fit_transform(diff_l.reshape(-1,1)).reshape(-1)\n",
        "\n",
        "Test_Data = collections.namedtuple('Data', field_names=['date','high', 'low', 'close', 'open', 'volume', 'real_open', 'real_close', 'real_high', 'real_low', 'real_vol'])\n",
        "test = Test_Data(date=dt,high=rh, low=rl, close=rc, open=o, volume=vol, real_open=data['Open'].values, real_close=data_two['Close'].values, real_high=data_two['High'].values, real_low=data_two['Low'].values, real_vol=data['Volume'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Shared Standke Policy/Value Network Class"
      ],
      "metadata": {
        "id": "y-xccbrPy34D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class StandkeExtractor(BaseFeaturesExtractor):\n",
        "#   def __init__(self, observation_space=gym.spaces.Box, features_dim=128):\n",
        "#         super(StandkeExtractor, self).__init__(observation_space, features_dim)\n",
        "        \n",
        "#         input = observation_space.shape[0]\n",
        "\n",
        "#         # Feature Extractor\n",
        "#         self.cnn = nn.Sequential(\n",
        "#             nn.Conv1d(input, 128, kernel_size=2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv1d(128, 512, kernel_size=4),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool1d(kernel_size=4)\n",
        "#             nn.Flatten(),\n",
        "#         )\n",
        "\n",
        "#         # Compute shape by doing one forward pass\n",
        "#         with th.no_grad():\n",
        "#             n_flatten = self.cnn(\n",
        "#                 th.as_tensor(observation_space.sample()[None]).float()\n",
        "#             ).shape[1]\n",
        "\n",
        "        \n",
        "#   def forward(self, observations):\n",
        "#     return self.cnn(observations)\n",
        "\n",
        "\n",
        "\n",
        "# class StandkeNetwork(nn.Module):\n",
        "#   def __init__(self,feature_dim=3072, last_layer_dim_pi=2, last_layer_dim_vf=1):\n",
        "#         super(StandkeNetwork, self).__init__()\n",
        "\n",
        "#         # IMPORTANT:\n",
        "#         # Save output dimensions, used to create the distributions\n",
        "#         self.latent_dim_pi = last_layer_dim_pi\n",
        "#         self.latent_dim_vf = last_layer_dim_vf\n",
        "\n",
        "#          # Policy Network\n",
        "#         self.policy_net = nn.Sequential(\n",
        "#             nn.Linear(feature_dim, 256),\n",
        "#             nn.BatchNorm1d(256),\n",
        "#             nn.Tanh(),\n",
        "#             nn.Linear(256,last_layer_dim_pi),\n",
        "#             nn.BatchNorm1d(last_layer_dim_pi), \n",
        "#             nn.Softplus(),  \n",
        "#         )\n",
        "\n",
        "#          # Value Network\n",
        "#         self.value_net = nn.Sequential(\n",
        "#            nn.Linear(feature_dim, 512),\n",
        "#            nn.BatchNorm1d(512),\n",
        "#            nn.Dropout(),\n",
        "#            nn.Tanh(),\n",
        "#            nn.Linear(512,256),\n",
        "#            nn.BatchNorm1d(256),\n",
        "#            nn.Dropout(),\n",
        "#            nn.Tanh(),\n",
        "#            nn.Linear(256,last_layer_dim_vf),\n",
        "#            nn.BatchNorm1d(last_layer_dim_vf),\n",
        "#            nn.Tanh(),\n",
        "#            )\n",
        "\n",
        "#   def forward_actor(self, features):\n",
        "#         return self.policy_net(features)\n",
        "\n",
        "#   def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
        "#         return self.value_net(features)\n",
        "  \n",
        "\n",
        "# class SharedStandkePolicy(ActorCriticPolicy):\n",
        "#   def __init__(self, std, observation_space=gym.spaces.Box,action_space=gym.spaces.Box, lr_schedule=constant_fn(0.0003), activation_fn=nn.ReLU,*args,**kwargs):\n",
        "    \n",
        "#         super(SharedStandkePolicy, self).__init__(observation_space,action_space, lr_schedule, activation_fn,*args,**kwargs)\n",
        "#         self.std = std\n",
        "#         # shared features extractor for the actor and the critic\n",
        "#         self.policy_features_extractor = StandkeExtractor(observation_space)\n",
        "#         delattr(self, \"features_extractor\")  # remove the shared features extractor\n",
        "#         # orthogonal initialization\n",
        "#         self.ortho_init = True\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RdQupd-gy0fY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Seperate Standke Policy/ValueNetwork Class"
      ],
      "metadata": {
        "id": "_-5jadJfRbDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StandkeExtractor(BaseFeaturesExtractor):\n",
        "  def __init__(self, observation_space=gym.spaces.Box, features_dim=128):\n",
        "        super(StandkeExtractor, self).__init__(observation_space, features_dim)\n",
        "        input = observation_space.shape[0]\n",
        "        # Feature Extractor\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input, 64, kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, 128, kernel_size=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(\n",
        "                th.as_tensor(observation_space.sample()[None]).float()\n",
        "            ).shape[1]\n",
        "\n",
        "        \n",
        "  def forward(self, observations):\n",
        "    return self.cnn(observations)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class StandkeNetwork(nn.Module):\n",
        "  def __init__(self,feature_dim=128, last_layer_dim_pi=2, last_layer_dim_vf=1):\n",
        "        super(StandkeNetwork, self).__init__()\n",
        "\n",
        "        # IMPORTANT:\n",
        "        # Save output dimensions, used to create the distributions\n",
        "        self.latent_dim_pi = last_layer_dim_pi\n",
        "        self.latent_dim_vf = last_layer_dim_vf\n",
        "\n",
        "         # Policy Network\n",
        "        self.policy_net = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, last_layer_dim_pi),\n",
        "            nn.Tanh(),  \n",
        "        )\n",
        "\n",
        "\n",
        "         # Value Network\n",
        "        self.value_net = nn.Sequential(\n",
        "           nn.Linear(feature_dim, 256),\n",
        "           nn.BatchNorm1d(256),\n",
        "           nn.Dropout(),\n",
        "           nn.Tanh(),\n",
        "           nn.Linear(256, 128),\n",
        "           nn.BatchNorm1d(128),\n",
        "           nn.Dropout(0.2),\n",
        "           nn.Tanh(),\n",
        "           nn.Linear(128, 32),\n",
        "           nn.BatchNorm1d(32),\n",
        "           nn.Tanh(),\n",
        "           nn.Linear(32,last_layer_dim_vf),\n",
        "           nn.Tanh(),\n",
        "           )\n",
        "\n",
        "  def forward_actor(self, features: th.Tensor):\n",
        "    return self.policy_net(features)\n",
        "\n",
        "  def forward_critic(self, features: th.Tensor):\n",
        "    return self.value_net(features)\n",
        "  \n",
        "\n",
        "class StandkePolicy(ActorCriticPolicy):\n",
        "  def __init__(self, observation_space=gym.spaces.Box,action_space=gym.spaces.Box, lr_schedule=constant_fn(0.0003), activation_fn=nn.Tanh,*args,**kwargs):\n",
        "    \n",
        "        super(StandkePolicy, self).__init__(observation_space,action_space, lr_schedule, activation_fn,*args,**kwargs)\n",
        "        # non-shared features extractors for the actor and the critic\n",
        "        self.policy_features_extractor = StandkeExtractor(observation_space)\n",
        "        self.value_features_extractor = StandkeExtractor(observation_space)\n",
        "        delattr(self, \"features_extractor\")  # remove the shared features extractor\n",
        "        # Do not Disable orthogonal initialization\n",
        "        self.ortho_init = True\n",
        "\n",
        "  def _build_mlp_extractor(self):\n",
        "    self.mlp_extractor = StandkeNetwork()\n",
        "\n",
        "  def extract_features(self, obs: th.Tensor):\n",
        "    policy_features = self.policy_features_extractor(obs)\n",
        "    value_features = self.value_features_extractor(obs)\n",
        "    return policy_features, value_features\n",
        "  \n",
        "  def forward(self, obs: th.Tensor, deterministic=False): \n",
        "    policy_features, value_features = self.extract_features(obs)\n",
        "    mu_pi = self.mlp_extractor.forward_actor(policy_features)\n",
        "    latent_vf = self.mlp_extractor.forward_critic(value_features)\n",
        "    # Evaluate the values for the given observations\n",
        "    distribution = self._get_action_dist_from_latent(mu_pi)\n",
        "    actions = distribution.get_actions(deterministic=deterministic)\n",
        "    log_prob = distribution.log_prob(actions)\n",
        "    vf = latent_vf\n",
        "    return actions, vf, log_prob\n",
        "\n",
        "  def evaluate_actions(self, obs: th.Tensor, actions: th.Tensor): \n",
        "    policy_features, value_features = self.extract_features(obs)\n",
        "    mu_pi = self.mlp_extractor.forward_actor(policy_features)\n",
        "    latent_vf = self.mlp_extractor.forward_critic(value_features)\n",
        "    distribution = self._get_action_dist_from_latent(mu_pi)\n",
        "    actions = distribution.get_actions(deterministic=False)\n",
        "    log_prob = distribution.log_prob(actions)\n",
        "    vf = latent_vf\n",
        "    return vf, log_prob, distribution.entropy()\n",
        "\n",
        "  def get_distribution(self, obs: th.Tensor):\n",
        "    policy_features, _ = self.extract_features(obs)\n",
        "    latent_pi = self.mlp_extractor.forward_actor(policy_features)\n",
        "    return self._get_action_dist_from_latent(latent_pi)\n",
        "\n",
        "  def predict_values(self, obs: th.Tensor):\n",
        "    _, value_features = self.extract_features(obs)\n",
        "    latent_vf = self.mlp_extractor.forward_critic(value_features)\n",
        "    return latent_vf"
      ],
      "metadata": {
        "id": "572gmNd6Q-kr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TensorboardCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(TensorboardCallback, self).__init__(verbose)\n",
        "        self.mean_reward = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "      self.mean_reward.append(self.locals['infos'][0]['reward'])\n",
        "      if (self.num_timesteps % 10000 == 0):\n",
        "        self.logger.record('Net Worth', self.locals['infos'][0]['Net Worth'])\n",
        "        self.logger.record('Mean Reward', np.mean(self.mean_reward))\n",
        "        self.mean_reward.clear()\n",
        "      return True"
      ],
      "metadata": {
        "id": "1hEB-iRM4GLz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating SQLite Database to Store Hyperparmaters "
      ],
      "metadata": {
        "id": "Tw69Zw1sBTop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_connection(db_file):\n",
        "#     \"\"\" create a database connection to a SQLite database \"\"\"\n",
        "#     conn = None\n",
        "#     try:\n",
        "#         conn = sqlite3.connect(db_file)\n",
        "#         print(sqlite3.version)\n",
        "#     except Error as e:\n",
        "#         print(e)\n",
        "#     finally:\n",
        "#         if conn:\n",
        "#             conn.close()\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     create_connection(r\"/content/drive/MyDrive/RLmodels/sqlite:params.db\")"
      ],
      "metadata": {
        "id": "h5R7GTYHBTSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HyperParmater Tuning "
      ],
      "metadata": {
        "id": "LKGZpOQ39eNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# envs =  DummyVecEnv([lambda: StockTradingEnv(train)])\n",
        "\n",
        "# def optimize(n_trials = 5000, n_jobs = 4):\n",
        "#     study = optuna.create_study(study_name='optimize_hyperparmaters', storage=\"/content/drive/MyDrive/RLmodels/sqlite:params.db\", load_if_exists=True)\n",
        "#     study.optimize(objective_fn, n_trials=n_trials, n_jobs=n_jobs)\n",
        "\n",
        "# def objective_fn(trial):\n",
        "#     env_params = optimize_envs(trial)\n",
        "#     agent_params = optimize_ppo(trial)\n",
        "    \n",
        "#     train_env, validation_env = initialize_envs(**env_params)\n",
        "#     model = PPO(StandkePolicy, envs, **agent_params)\n",
        "    \n",
        "#     model.learn(len(train_env.df))\n",
        "    \n",
        "#     rewards, done = [], False\n",
        "\n",
        "#     obs = validation_env.reset()\n",
        "#     for i in range(len(validation_env.df)):\n",
        "#         action, _ = model.predict(obs)\n",
        "#         obs, reward, done, _ = validation_env.step(action)\n",
        "#         rewards += reward\n",
        "    \n",
        "#     return -np.mean(rewards)\n",
        "\n",
        "# def optimize_ppo(trial):\n",
        "#     return {\n",
        "#         'n_steps': int(trial.suggest_loguniform('n_steps', 16, 2048)),\n",
        "#         'batch_size': int(trial.suggest_loguniform('batch_size', 10, 100)),\n",
        "#         'n_epochs': int(trial.suggest_loguniform('n_epochs', 10, 30)),\n",
        "#         'gamma': trial.suggest_loguniform('gamma', 0.11, 0.99),\n",
        "#         'normalize_advantage': trial.suggest_categorical('normalize_advantage', [True, False]), \n",
        "#         'cliprange': trial.suggest_uniform('cliprange', 0.11, 0.99),\n",
        "#         'clip_range_vf': trial.suggest_uniform('clip_range_vf', 0.11, 0.99),\n",
        "#         'vf_coef': trial.suggest_uniform('vf_coef', 0.1, 0.9),\n",
        "#         'target_kl': trial.suggest_uniform('target_kl', 0.001, 0.09),\n",
        "#         'gae_lambda': trial.suggest_uniform('gae_lambda', 0.11, 0.99),\n",
        "#     }\n",
        "\n",
        "# def optimize_envs(trial):\n",
        "#     return {\n",
        "#         'reward_func': trial.suggest_categorical(\"reward_func\", ['BalenceReward', 'sortinoRewardRatio',\n",
        "#          'calmarRewardRatio', 'omegaRewardRatio', \n",
        "#          'StandkeCurrentValueReward','StandkeSmallDrawDownReward', \n",
        "#          'StandkeLargeDrawDownReward','StandkeSumofDifferenceReward', \n",
        "#          'Mean'])\n",
        "#     }\n",
        "\n",
        "# # run optimization to find good hyperparmaters for agent and enviornmnet\n",
        "# optimize()"
      ],
      "metadata": {
        "id": "eskm_ga99dme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the hyperparmeters for training\n",
        "# study = optuna.load_study(study_name='optimize_hyperparmaters', storage='/content/drive/MyDrive/RLmodels/sqlite:params.db')\n",
        "# params = study.best_trial.params\n",
        "# env_params = {\n",
        "#     'reward_func': params['reward_func'])\n",
        "# }\n",
        "# model_params = {\n",
        "#     'n_steps': int(params['n_steps']),\n",
        "#     'gamma': params['gamma'],\n",
        "#     'learning_rate': params['learning_rate'],\n",
        "#     'ent_coef': params['ent_coef'],\n",
        "#     'cliprange': params['cliprange'],\n",
        "#     'noptepochs': int(params['noptepochs']),\n",
        "#     'lam': params['lam']\n",
        "# }"
      ],
      "metadata": {
        "id": "jMluKzP-E1u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg2HNywdyLaO"
      },
      "source": [
        "# Training and Validation Portion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of learning steps to train RL model is set to 200K\n",
        "MAX_STEPS = 2e5\n",
        "# the number of parallel environments for training  \n",
        "ENV = 1\n",
        "MODEL = \"StandkeSharedPV_neg0.2offset\""
      ],
      "metadata": {
        "id": "wLYFzy2-HWYI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "cAJYlkEqIwcK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "a9a1ab60-ecd7-4718-b02e-121ead0a82cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'training model using the Shared Standke Policy/Value network'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# create evaluation env that takes in test data to save best model \n",
        "eval_env = DummyVecEnv([lambda: StockTradingEnv(test)])\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=f'/content/drive/MyDrive/RLmodels/bestPPO/{MODEL}',\n",
        "                             log_path='/content/drive/MyDrive/RLmodels/logs/', eval_freq=10000,\n",
        "                             deterministic=False, render=False)\n",
        "\n",
        "# create training envs that takes in training data for training\n",
        "envs =  DummyVecEnv([lambda: StockTradingEnv(train) for _ in range(0,ENV)])\n",
        "\n",
        "\n",
        "# optional additional keyword parameters to pass to model \n",
        "policy_kwargs = dict()\n",
        "\n",
        "'''training model using the Seperate Standke Policy/Value network'''\n",
        "model = PPO(StandkePolicy, envs, verbose=1, tensorboard_log=f\"/content/PPO_SPY_tensorboard/{MODEL}\", policy_kwargs=policy_kwargs)\n",
        "check_env(StockTradingEnv(train))\n",
        "VecCheckNan(envs, raise_exception=True, check_inf=True)\n",
        "\n",
        "'''training model using the Shared Standke Policy/Value network''' \n",
        "# model = PPO(SharedStandkePolicy, envs, verbose=1, tensorboard_log=f\"/content/PPO_SPY_tensorboard/{}\", policy_kwargs=policy_kwargs)\n",
        "# check_env(StockTradingEnv(train, random_ofs_on_reset=True))\n",
        "# VecCheckNan(envs, raise_exception=True, check_inf=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General explanation of log output \n",
        "\n",
        "As detailed by araffin in his commit [Add explanation of logger output](https://github.com/DLR-RM/stable-baselines3/pull/803/files), for a given log block such as\n",
        "\n",
        "```\n",
        "-----------------------------------------\n",
        "  | eval/                   |             |\n",
        "  |    mean_ep_length       | 200         |\n",
        "  |    mean_reward          | -157        |\n",
        "  | rollout/                |             |\n",
        "  |    ep_len_mean          | 200         |\n",
        "  |    ep_rew_mean          | -227        |\n",
        "  | time/                   |             |\n",
        "  |    fps                  | 972         |\n",
        "  |    iterations           | 19          |\n",
        "  |    time_elapsed         | 80          |\n",
        "  |    total_timesteps      | 77824       |\n",
        "  | train/                  |             |\n",
        "  |    approx_kl            | 0.037781604 |\n",
        "  |    clip_fraction        | 0.243       |\n",
        "  |    clip_range           | 0.2         |\n",
        "  |    entropy_loss         | -1.06       |\n",
        "  |    explained_variance   | 0.999       |\n",
        "  |    learning_rate        | 0.001       |\n",
        "  |    loss                 | 0.245       |\n",
        "  |    n_updates            | 180         |\n",
        "  |    policy_gradient_loss | -0.00398    |\n",
        "  |    std                  | 0.205       |\n",
        "  |    value_loss           | 0.226       |\n",
        "  -----------------------------------------\n",
        "```\n",
        "``eval/`` \n",
        "- ``mean_ep_length``: Mean episode length\n",
        "- ``mean_reward``: Mean episodic reward (during evaluation)\n",
        "``rollout/``\n",
        "- ``ep_len_mean``: Mean episode length (averaged over 100 episodes)\n",
        "- ``ep_rew_mean``: Mean episodic training reward (averaged over 100 episodes)\n",
        "``time/``\n",
        "- ``episodes``: Total number of episodes\n",
        "- ``fps``: Number of frames per seconds (includes time taken by gradient update)\n",
        "- ``iterations``: Number of iterations (data collection + policy update for A2C/PPO)\n",
        "- ``time_elapsed``: Time in seconds since the beginning of training\n",
        "- ``total_timesteps``: Total number of timesteps (steps in the environments)\n",
        "``train/``\n",
        "- ``entropy_loss``: Mean value of the entropy loss (negative of the average policy entropy). \n",
        "  * ⚠**According to the formula as detailed [model](https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/ppo2/model.py#L91) on line 91, if ent_coef is 0 this term should not matter which is the default hyperparamter setting; difficult to interpret for this env due to it being negative**⚠\n",
        "  * **Furthermore according to [The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) which cites [Andrychowicz, et al. (2021)](https://openreview.net/forum?id=nIAxjsniDzg) overall find no evidence that the entropy term improves performance on continuous control environments (decision C13, figure 76 and 77)**\n",
        "- ``clip_fraction``: mean fraction of surrogate loss that was clipped (above clip_range threshold) for PPO.\n",
        "- ``clip_range``: Current value of the clipping factor for the surrogate loss of PPO\n",
        "- ``entropy_loss``: Mean value of the entropy loss (negative of the average policy entropy)\n",
        "    *  want the entropy to be decreasing slowly and smoothly over the course of training, as the agent trades exploration in favor of exploitation.\n",
        "- ``learning_rate``: Current learning rate value\n",
        "- ``n_updates``: Number of gradient updates applied so far\n",
        "- ``policy_gradient_loss``: Current value of the policy gradient loss (its value does not have much meaning)(lol I did not say this 😸)\n",
        "- ``std``: Current standard deviation of the noise when using generalized State-Dependent Exploration (gSDE) (which by default is not used)\n",
        "\n",
        "# Important Training Metrics to Focus On!!!! ✅✅✅✅✅✅✅✅✅\n",
        "- ``approx_kl``: approximate mean KL divergence between old and new policy (for PPO), it is an estimation of how much change happened in the update (i.e. information gain or loss)\n",
        "  * **Want this value to SMOOTHLY DECREASE during training and be as close as possible to 0**\n",
        "  * **Should be DECREASING**\n",
        "- ``explained_variance``: Fraction of the return variance explained by the value function. This metric calculates how good the value function is as a predicator of future rewards\n",
        "  * **Want this value to be as close as possible to 1 (i.e.perfect predictions) during training rather than less than or equal to 0 (i.e. no predictive power)**\n",
        "  * **Should be INCREASING**\n",
        "- ``loss``: called total loss is the the overall loss function\n",
        "  * **Want to MINIMIZE this during training** \n",
        "  * **Should be DECREASING**\n",
        "- ``value_loss``: error that value function is incurring \n",
        "  *   **Want to MINIMIZE this during training to 0 (though as discussed this isn't always possible due to randomness)**\n",
        "  * **Should be DECREASING**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9ATRRLz4RsLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "O6sMSFIuhLzx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de6b92ae-a045-41f5-c9ee-e4665fa16c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /content/PPO_SPY_tensorboard/StandkeSharedPV_neg0.2offset/PPO_12\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 451  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 4    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 327       |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 12        |\n",
            "|    total_timesteps      | 4096      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.4544518 |\n",
            "|    clip_fraction        | 0.823     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.94     |\n",
            "|    explained_variance   | -6.2      |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.605     |\n",
            "|    n_updates            | 10        |\n",
            "|    policy_gradient_loss | 0.828     |\n",
            "|    std                  | 1.11      |\n",
            "|    value_loss           | 0.0717    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 310       |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 19        |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.1741157 |\n",
            "|    clip_fraction        | 0.823     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -3.16     |\n",
            "|    explained_variance   | -3.51     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.335     |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | 1.07      |\n",
            "|    std                  | 1.24      |\n",
            "|    value_loss           | 0.00509   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 302       |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 27        |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 1.9482815 |\n",
            "|    clip_fraction        | 0.816     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -3.38     |\n",
            "|    explained_variance   | -3.56     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.124     |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | 0.804     |\n",
            "|    std                  | 1.38      |\n",
            "|    value_loss           | 0.00136   |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 845.20 +/- 236.27\n",
            "--------------------------------------\n",
            "| eval/                   |          |\n",
            "|    mean_ep_length       | 845      |\n",
            "|    mean_reward          | 0.000516 |\n",
            "| time/                   |          |\n",
            "|    total_timesteps      | 10000    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 2.847426 |\n",
            "|    clip_fraction        | 0.821    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -3.58    |\n",
            "|    explained_variance   | -1.54    |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 0.967    |\n",
            "|    n_updates            | 40       |\n",
            "|    policy_gradient_loss | 1.46     |\n",
            "|    std                  | 1.52     |\n",
            "|    value_loss           | 0.000937 |\n",
            "--------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| Mean Reward        | 4.61e-07 |\n",
            "| Net Worth          | 7.43e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 238      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 42       |\n",
            "|    total_timesteps | 10240    |\n",
            "---------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 244       |\n",
            "|    iterations           | 6         |\n",
            "|    time_elapsed         | 50        |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.5687485 |\n",
            "|    clip_fraction        | 0.815     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -3.77     |\n",
            "|    explained_variance   | -1.96     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 11.5      |\n",
            "|    n_updates            | 50        |\n",
            "|    policy_gradient_loss | 2.53      |\n",
            "|    std                  | 1.68      |\n",
            "|    value_loss           | 0.000697  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 248       |\n",
            "|    iterations           | 7         |\n",
            "|    time_elapsed         | 57        |\n",
            "|    total_timesteps      | 14336     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 3.7381396 |\n",
            "|    clip_fraction        | 0.817     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -3.97     |\n",
            "|    explained_variance   | -2.49     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 3.23      |\n",
            "|    n_updates            | 60        |\n",
            "|    policy_gradient_loss | 2.3       |\n",
            "|    std                  | 1.85      |\n",
            "|    value_loss           | 0.000394  |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 252      |\n",
            "|    iterations           | 8        |\n",
            "|    time_elapsed         | 65       |\n",
            "|    total_timesteps      | 16384    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 2.042972 |\n",
            "|    clip_fraction        | 0.817    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -4.16    |\n",
            "|    explained_variance   | -1.75    |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 0.288    |\n",
            "|    n_updates            | 70       |\n",
            "|    policy_gradient_loss | 0.883    |\n",
            "|    std                  | 2.03     |\n",
            "|    value_loss           | 0.000297 |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 253       |\n",
            "|    iterations           | 9         |\n",
            "|    time_elapsed         | 72        |\n",
            "|    total_timesteps      | 18432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.2329402 |\n",
            "|    clip_fraction        | 0.816     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -4.34     |\n",
            "|    explained_variance   | -1.28     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.567     |\n",
            "|    n_updates            | 80        |\n",
            "|    policy_gradient_loss | 0.824     |\n",
            "|    std                  | 2.21      |\n",
            "|    value_loss           | 0.000158  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 702.00 +/- 191.03\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 702       |\n",
            "|    mean_reward          | 0.000499  |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 20000     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.7597551 |\n",
            "|    clip_fraction        | 0.815     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -4.52     |\n",
            "|    explained_variance   | -3.25     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.67      |\n",
            "|    n_updates            | 90        |\n",
            "|    policy_gradient_loss | 1.38      |\n",
            "|    std                  | 2.42      |\n",
            "|    value_loss           | 0.000139  |\n",
            "---------------------------------------\n",
            "---------------------------------\n",
            "| Mean Reward        | 3.17e-07 |\n",
            "| Net Worth          | 1.18e-07 |\n",
            "| time/              |          |\n",
            "|    fps             | 239      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 85       |\n",
            "|    total_timesteps | 20480    |\n",
            "---------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 242       |\n",
            "|    iterations           | 11        |\n",
            "|    time_elapsed         | 92        |\n",
            "|    total_timesteps      | 22528     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.7853842 |\n",
            "|    clip_fraction        | 0.817     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -4.7      |\n",
            "|    explained_variance   | -5.39     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.38      |\n",
            "|    n_updates            | 100       |\n",
            "|    policy_gradient_loss | 1.26      |\n",
            "|    std                  | 2.67      |\n",
            "|    value_loss           | 0.000125  |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 245      |\n",
            "|    iterations           | 12       |\n",
            "|    time_elapsed         | 100      |\n",
            "|    total_timesteps      | 24576    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 3.586854 |\n",
            "|    clip_fraction        | 0.826    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -4.9     |\n",
            "|    explained_variance   | -2.68    |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 0.324    |\n",
            "|    n_updates            | 110      |\n",
            "|    policy_gradient_loss | 0.984    |\n",
            "|    std                  | 2.94     |\n",
            "|    value_loss           | 9.81e-05 |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 248       |\n",
            "|    iterations           | 13        |\n",
            "|    time_elapsed         | 107       |\n",
            "|    total_timesteps      | 26624     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 3.5517335 |\n",
            "|    clip_fraction        | 0.822     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -5.09     |\n",
            "|    explained_variance   | -4.3      |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.222     |\n",
            "|    n_updates            | 120       |\n",
            "|    policy_gradient_loss | 0.755     |\n",
            "|    std                  | 3.22      |\n",
            "|    value_loss           | 9.95e-05  |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 250      |\n",
            "|    iterations           | 14       |\n",
            "|    time_elapsed         | 114      |\n",
            "|    total_timesteps      | 28672    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 6.326577 |\n",
            "|    clip_fraction        | 0.814    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -5.28    |\n",
            "|    explained_variance   | -2.17    |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 3.06     |\n",
            "|    n_updates            | 130      |\n",
            "|    policy_gradient_loss | 1.99     |\n",
            "|    std                  | 3.55     |\n",
            "|    value_loss           | 7.91e-05 |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 593.80 +/- 363.88\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 594       |\n",
            "|    mean_reward          | 0.000641  |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 30000     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.2804213 |\n",
            "|    clip_fraction        | 0.828     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -5.47     |\n",
            "|    explained_variance   | -2.24     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.125     |\n",
            "|    n_updates            | 140       |\n",
            "|    policy_gradient_loss | 1.01      |\n",
            "|    std                  | 3.92      |\n",
            "|    value_loss           | 7.53e-05  |\n",
            "---------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| Mean Reward        | 4.55e-07 |\n",
            "| Net Worth          | 299      |\n",
            "| time/              |          |\n",
            "|    fps             | 241      |\n",
            "|    iterations      | 15       |\n",
            "|    time_elapsed    | 126      |\n",
            "|    total_timesteps | 30720    |\n",
            "---------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 243       |\n",
            "|    iterations           | 16        |\n",
            "|    time_elapsed         | 134       |\n",
            "|    total_timesteps      | 32768     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 3.2080886 |\n",
            "|    clip_fraction        | 0.822     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -5.67     |\n",
            "|    explained_variance   | 0.267     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.34      |\n",
            "|    n_updates            | 150       |\n",
            "|    policy_gradient_loss | 1.81      |\n",
            "|    std                  | 4.31      |\n",
            "|    value_loss           | 5.89e-05  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 245       |\n",
            "|    iterations           | 17        |\n",
            "|    time_elapsed         | 141       |\n",
            "|    total_timesteps      | 34816     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 2.2378094 |\n",
            "|    clip_fraction        | 0.815     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -5.85     |\n",
            "|    explained_variance   | -2.08     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.26      |\n",
            "|    n_updates            | 160       |\n",
            "|    policy_gradient_loss | 1.07      |\n",
            "|    std                  | 4.72      |\n",
            "|    value_loss           | 7.4e-05   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 247      |\n",
            "|    iterations           | 18       |\n",
            "|    time_elapsed         | 148      |\n",
            "|    total_timesteps      | 36864    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 3.866612 |\n",
            "|    clip_fraction        | 0.817    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -6.04    |\n",
            "|    explained_variance   | -0.711   |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 2.03     |\n",
            "|    n_updates            | 170      |\n",
            "|    policy_gradient_loss | 1.11     |\n",
            "|    std                  | 5.18     |\n",
            "|    value_loss           | 4.95e-05 |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 248       |\n",
            "|    iterations           | 19        |\n",
            "|    time_elapsed         | 156       |\n",
            "|    total_timesteps      | 38912     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 3.0769532 |\n",
            "|    clip_fraction        | 0.818     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -6.22     |\n",
            "|    explained_variance   | -0.743    |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.251     |\n",
            "|    n_updates            | 180       |\n",
            "|    policy_gradient_loss | 0.977     |\n",
            "|    std                  | 5.69      |\n",
            "|    value_loss           | 4.78e-05  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 895.60 +/- 202.82\n",
            "--------------------------------------\n",
            "| eval/                   |          |\n",
            "|    mean_ep_length       | 896      |\n",
            "|    mean_reward          | 0.00103  |\n",
            "| time/                   |          |\n",
            "|    total_timesteps      | 40000    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 4.261757 |\n",
            "|    clip_fraction        | 0.821    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -6.41    |\n",
            "|    explained_variance   | -3.71    |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 0.316    |\n",
            "|    n_updates            | 190      |\n",
            "|    policy_gradient_loss | 1.88     |\n",
            "|    std                  | 6.28     |\n",
            "|    value_loss           | 4.08e-05 |\n",
            "--------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| Mean Reward        | 4.35e-07 |\n",
            "| Net Worth          | 60.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 238      |\n",
            "|    iterations      | 20       |\n",
            "|    time_elapsed    | 171      |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 240      |\n",
            "|    iterations           | 21       |\n",
            "|    time_elapsed         | 178      |\n",
            "|    total_timesteps      | 43008    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 4.272872 |\n",
            "|    clip_fraction        | 0.818    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -6.61    |\n",
            "|    explained_variance   | -2.45    |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 1.33     |\n",
            "|    n_updates            | 200      |\n",
            "|    policy_gradient_loss | 1.71     |\n",
            "|    std                  | 6.93     |\n",
            "|    value_loss           | 3.99e-05 |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 242       |\n",
            "|    iterations           | 22        |\n",
            "|    time_elapsed         | 186       |\n",
            "|    total_timesteps      | 45056     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 1.9162999 |\n",
            "|    clip_fraction        | 0.82      |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -6.79     |\n",
            "|    explained_variance   | -11.1     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.459     |\n",
            "|    n_updates            | 210       |\n",
            "|    policy_gradient_loss | 0.667     |\n",
            "|    std                  | 7.53      |\n",
            "|    value_loss           | 3.34e-05  |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 243      |\n",
            "|    iterations           | 23       |\n",
            "|    time_elapsed         | 193      |\n",
            "|    total_timesteps      | 47104    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 6.361765 |\n",
            "|    clip_fraction        | 0.817    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -6.97    |\n",
            "|    explained_variance   | -2.13    |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 0.659    |\n",
            "|    n_updates            | 220      |\n",
            "|    policy_gradient_loss | 0.812    |\n",
            "|    std                  | 8.23     |\n",
            "|    value_loss           | 2.89e-05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 244      |\n",
            "|    iterations           | 24       |\n",
            "|    time_elapsed         | 200      |\n",
            "|    total_timesteps      | 49152    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 4.080618 |\n",
            "|    clip_fraction        | 0.821    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -7.15    |\n",
            "|    explained_variance   | -4.95    |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 0.587    |\n",
            "|    n_updates            | 230      |\n",
            "|    policy_gradient_loss | 0.947    |\n",
            "|    std                  | 9.07     |\n",
            "|    value_loss           | 2.71e-05 |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=0.00 +/- 0.00\n",
            "Episode length: 611.60 +/- 241.40\n",
            "--------------------------------------\n",
            "| eval/                   |          |\n",
            "|    mean_ep_length       | 612      |\n",
            "|    mean_reward          | 0.00133  |\n",
            "| time/                   |          |\n",
            "|    total_timesteps      | 50000    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 3.62052  |\n",
            "|    clip_fraction        | 0.813    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -7.34    |\n",
            "|    explained_variance   | -6.14    |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 0.57     |\n",
            "|    n_updates            | 240      |\n",
            "|    policy_gradient_loss | 1.33     |\n",
            "|    std                  | 9.99     |\n",
            "|    value_loss           | 3.1e-05  |\n",
            "--------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| Mean Reward        | 6.82e-07 |\n",
            "| Net Worth          | 1.04e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 240      |\n",
            "|    iterations      | 25       |\n",
            "|    time_elapsed    | 213      |\n",
            "|    total_timesteps | 51200    |\n",
            "---------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 241      |\n",
            "|    iterations           | 26       |\n",
            "|    time_elapsed         | 220      |\n",
            "|    total_timesteps      | 53248    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 2.75199  |\n",
            "|    clip_fraction        | 0.817    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -7.53    |\n",
            "|    explained_variance   | -1.92    |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 0.221    |\n",
            "|    n_updates            | 250      |\n",
            "|    policy_gradient_loss | 0.819    |\n",
            "|    std                  | 10.9     |\n",
            "|    value_loss           | 2.37e-05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 242      |\n",
            "|    iterations           | 27       |\n",
            "|    time_elapsed         | 227      |\n",
            "|    total_timesteps      | 55296    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 2.970882 |\n",
            "|    clip_fraction        | 0.817    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -7.72    |\n",
            "|    explained_variance   | -0.777   |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 0.466    |\n",
            "|    n_updates            | 260      |\n",
            "|    policy_gradient_loss | 1.42     |\n",
            "|    std                  | 12.1     |\n",
            "|    value_loss           | 2.31e-05 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 244      |\n",
            "|    iterations           | 28       |\n",
            "|    time_elapsed         | 234      |\n",
            "|    total_timesteps      | 57344    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 4.198842 |\n",
            "|    clip_fraction        | 0.816    |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -7.91    |\n",
            "|    explained_variance   | 0.34     |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 1.6      |\n",
            "|    n_updates            | 270      |\n",
            "|    policy_gradient_loss | 0.768    |\n",
            "|    std                  | 13.2     |\n",
            "|    value_loss           | 2e-05    |\n",
            "--------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-dbbb257e51a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTensorboardCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-eec4a7aceb3a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mpolicy_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mmu_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mlatent_vf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Evaluate the values for the given observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-eec4a7aceb3a>\u001b[0m in \u001b[0;36mforward_actor\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.learn(total_timesteps=MAX_STEPS, callback=[eval_callback,TensorboardCallback()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction and Printout of Agent's Trading Strategy on Test Data"
      ],
      "metadata": {
        "id": "U4ClWZ6ZQqbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO.load(f\"/content/drive/MyDrive/RLmodels/bestPPO/{REWARD}.zip\")\n",
        "env = StockTradingEnv(test, random_ofs_on_reset=False, reward_cal=ratio[0])\n",
        "obs = env.reset()\n",
        "for i in range(len(test.date)):\n",
        "  action, _states = model.predict(obs, deterministic=False)\n",
        "  obs, rewards, done, info = env.step(action)\n",
        "  env.render()\n",
        "  if done:\n",
        "    break"
      ],
      "metadata": {
        "id": "BgbcsONJQqI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorBoard Analysis"
      ],
      "metadata": {
        "id": "hqTY07IM0Dyx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n02iHgNDTvYJ"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/PPO_SPY_tensorboard/ "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "y-xccbrPy34D",
        "U4ClWZ6ZQqbZ",
        "hqTY07IM0Dyx"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1BnRqmo4sw9dhUsiP_WO4HGPM9M7Cryi7",
      "authorship_tag": "ABX9TyMSEU6cP3CEMwPIGiqU45Qg",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}