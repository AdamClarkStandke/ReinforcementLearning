{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpyTradingAgent",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1EXh-2mp-AwE3OsMXYMgFGNCtenUG_T67",
      "authorship_tag": "ABX9TyNyOsFq3Tk+irKoKhPfDhen",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aCStandke/ReinforcementLearning/blob/main/SpyTradingAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stocks Trading Using Reinforcment Learning\n",
        "\n",
        "\n",
        "This implementation comes straight from chapter 10 from the  book \n",
        "[Deep Reinforcement Learning Hands-On - Second Edition by Maxim Lapan](https://www.amazon.com/Deep-Reinforcement-Learning-Hands-optimization/dp/1838826998/ref=asc_df_1838826998/?tag=hyprod-20&linkCode=df0&hvadid=416741343328&hvpos=&hvnetw=g&hvrand=7234438034400691228&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9008183&hvtargid=pla-871456510229&psc=1&tag=&ref=&adgrpid=93867144477&hvpone=&hvptwo=&hvadid=416741343328&hvpos=&hvnetw=g&hvrand=7234438034400691228&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9008183&hvtargid=pla-871456510229)\n",
        "\n",
        "As stated in chapter 10: \n",
        "\n",
        "> Rather than learning new methods to solve toy reinforcement learning (RL) problems in this chapter, we will try to utilize our deep Q-network (DQN) knowledge to deal with the much more practical problem of financial trading. \n",
        "\n",
        "Namely, a RL agent has some observation of the market, and has to take an action to either buy, sell, or hold. If the agent buys before the price goes up, profit will be positive; otherwise, the agent will get a negative reward. The agent is tyring to obtain as much profit as possible in the trading environment. \n",
        "\n"
      ],
      "metadata": {
        "id": "iTCX_DxmyQLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ptan"
      ],
      "metadata": {
        "id": "i42g9KIlrVJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "ntC5suZYsSvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "import gym.spaces\n",
        "from gym.utils import seeding\n",
        "from gym.envs.registration import EnvSpec\n",
        "import enum\n",
        "import glob\n",
        "import os\n",
        "import collections\n",
        "import csv\n",
        "import sys\n",
        "import time"
      ],
      "metadata": {
        "id": "I7jj6N5k264p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_run(env, net, episodes=100, device=\"cpu\", epsilon=0.02, comission=0.1):\n",
        "    stats = {\n",
        "        'episode_reward': [],\n",
        "        'episode_steps': [],\n",
        "        'order_profits': [],\n",
        "        'order_steps': [],\n",
        "    }\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs = env.reset()\n",
        "\n",
        "        total_reward = 0.0\n",
        "        position = None\n",
        "        position_steps = None\n",
        "        episode_steps = 0\n",
        "\n",
        "        while True:\n",
        "            obs_v = torch.tensor([obs]).to(device)\n",
        "            out_v = net(obs_v)\n",
        "\n",
        "            action_idx = out_v.max(dim=1)[1].item()\n",
        "            if np.random.random() < epsilon:\n",
        "                action_idx = env.action_space.sample()\n",
        "            action = Actions(action_idx)\n",
        "\n",
        "            close_price = env._state._cur_close()\n",
        "\n",
        "            if action == Actions.Buy and position is None:\n",
        "                position = close_price\n",
        "                position_steps = 0\n",
        "            elif action == Actions.Close and position is not None:\n",
        "                profit = close_price - position - (close_price + position) * comission / 100\n",
        "                profit = 100.0 * profit / position\n",
        "                stats['order_profits'].append(profit)\n",
        "                stats['order_steps'].append(position_steps)\n",
        "                position = None\n",
        "                position_steps = None\n",
        "\n",
        "            obs, reward, done, _ = env.step(action_idx)\n",
        "            total_reward += reward\n",
        "            episode_steps += 1\n",
        "            if position_steps is not None:\n",
        "                position_steps += 1\n",
        "            if done:\n",
        "                if position is not None:\n",
        "                    profit = close_price - position - (close_price + position) * comission / 100\n",
        "                    profit = 100.0 * profit / position\n",
        "                    stats['order_profits'].append(profit)\n",
        "                    stats['order_steps'].append(position_steps)\n",
        "                break\n",
        "\n",
        "        stats['episode_reward'].append(total_reward)\n",
        "        stats['episode_steps'].append(episode_steps)\n",
        "\n",
        "    return { key: np.mean(vals) for key, vals in stats.items() }"
      ],
      "metadata": {
        "id": "Y0K7eALDp3RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardTracker:\n",
        "    def __init__(self, writer, stop_reward, group_rewards=1):\n",
        "        self.writer = writer\n",
        "        self.stop_reward = stop_reward\n",
        "        self.reward_buf = []\n",
        "        self.steps_buf = []\n",
        "        self.group_rewards = group_rewards\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.ts = time.time()\n",
        "        self.ts_frame = 0\n",
        "        self.total_rewards = []\n",
        "        self.total_steps = []\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.writer.close()\n",
        "\n",
        "    def reward(self, reward_steps, frame, epsilon=None):\n",
        "        reward, steps = reward_steps\n",
        "        self.reward_buf.append(reward)\n",
        "        self.steps_buf.append(steps)\n",
        "        if len(self.reward_buf) < self.group_rewards:\n",
        "            return False\n",
        "        reward = np.mean(self.reward_buf)\n",
        "        steps = np.mean(self.steps_buf)\n",
        "        self.reward_buf.clear()\n",
        "        self.steps_buf.clear()\n",
        "        self.total_rewards.append(reward)\n",
        "        self.total_steps.append(steps)\n",
        "        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n",
        "        self.ts_frame = frame\n",
        "        self.ts = time.time()\n",
        "        mean_reward = np.mean(self.total_rewards[-100:])\n",
        "        mean_steps = np.mean(self.total_steps[-100:])\n",
        "        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n",
        "        print(\"%d: done %d games, mean reward %.3f, mean steps %.2f, speed %.2f f/s%s\" % (\n",
        "            frame, len(self.total_rewards)*self.group_rewards, mean_reward, mean_steps, speed, epsilon_str\n",
        "        ))\n",
        "        sys.stdout.flush()\n",
        "        if epsilon is not None:\n",
        "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
        "        self.writer.add_scalar(\"speed\", speed, frame)\n",
        "        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n",
        "        self.writer.add_scalar(\"reward\", reward, frame)\n",
        "        self.writer.add_scalar(\"steps_100\", mean_steps, frame)\n",
        "        self.writer.add_scalar(\"steps\", steps, frame)\n",
        "        if mean_reward > self.stop_reward:\n",
        "            print(\"Solved in %d frames!\" % frame)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "def calc_values_of_states(states, net, device=\"cpu\"):\n",
        "    mean_vals = []\n",
        "    for batch in np.array_split(states, 64):\n",
        "        states_v = torch.tensor(batch).to(device)\n",
        "        action_values_v = net(states_v)\n",
        "        best_action_values_v = action_values_v.max(1)[0]\n",
        "        mean_vals.append(best_action_values_v.mean().item())\n",
        "    return np.mean(mean_vals)\n",
        "\n",
        "\n",
        "def unpack_batch(batch):\n",
        "    states, actions, rewards, dones, last_states = [], [], [], [], []\n",
        "    for exp in batch:\n",
        "        state = np.array(exp.state, copy=False)\n",
        "        states.append(state)\n",
        "        actions.append(exp.action)\n",
        "        rewards.append(exp.reward)\n",
        "        dones.append(exp.last_state is None)\n",
        "        if exp.last_state is None:\n",
        "            last_states.append(state)       # the result will be masked anyway\n",
        "        else:\n",
        "            last_states.append(np.array(exp.last_state, copy=False))\n",
        "    return np.array(states, copy=False), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "           np.array(dones, dtype=np.uint8), np.array(last_states, copy=False)\n",
        "\n",
        "\n",
        "def calc_loss(batch, net, tgt_net, gamma, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, next_states = unpack_batch(batch)\n",
        "\n",
        "    states_v = torch.tensor(states).to(device)\n",
        "    next_states_v = torch.tensor(next_states).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.ByteTensor(dones).to(device)\n",
        "\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_actions = net(next_states_v).max(1)[1]\n",
        "    next_state_values = tgt_net(next_states_v).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_values[done_mask] = 0.0\n",
        "\n",
        "    expected_state_action_values = next_state_values.detach() * gamma + rewards_v\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n"
      ],
      "metadata": {
        "id": "4AkcT1EdL2sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Price Data for Trading Environment\n",
        "\n",
        "The chapter uses Russian stock market prices from the period ranging from 2015-2016 for the technology company [Yandex](https://en.wikipedia.org/wiki/Yandex) for its reinforcment trading agent. It contained over 130,000 rows, where every row represented a single minute in time,and price movement during that minute was captured by five variables: open, high, low,close, and volume. \n",
        "\n",
        "Rather than use one stock, I decided to use a basket of stocks found in the [SPY ETF](https://www.etf.com/SPY#:~:text=SPY%20is%20the%20best%2Drecognized,US%20index%2C%20the%20S%26P%20500.). This would give a longer term trading horizon, rather than the trading horizon  provided by the Yandex data. The period ranged from 2005 to 2022. Each row represented a single trading day and price movement during the trading day was captured by five variables: open, high, low, close, and volume. "
      ],
      "metadata": {
        "id": "X5_4V4z3oX9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Prices = collections.namedtuple('Prices', field_names=['open', 'high', 'low', 'close', 'volume'])\n",
        "\n",
        "def read_csv(file_name, sep=',', filter_data=True, fix_open_prices=False):\n",
        "  print(\"Reading\", file_name)\n",
        "  with open(file_name, 'r') as fd:\n",
        "    reader = csv.reader(fd)\n",
        "    h = next(reader)\n",
        "    indices = [h.index(s) for s in ('Open', 'High', 'Low', 'Close', 'Volume')]\n",
        "    o, h, l, c, v = [], [], [], [], []\n",
        "    count_out = 0\n",
        "    count_filter = 0 \n",
        "    count_fixed = 0\n",
        "    prev_vals = None\n",
        "    for row in reader:\n",
        "      vals = list(map(float, [row[idx] for idx in indices])) \n",
        "      if filter_data and all(map(lambda v: abs(v-vals[0]) < 1e-8, vals[:-1])):\n",
        "        count_filter += 1\n",
        "        continue\n",
        "      \n",
        "      po, ph, pl, pc, pv = vals\n",
        "\n",
        "      count_out +=1\n",
        "      o.append(po)\n",
        "      c.append(pc)\n",
        "      h.append(ph)\n",
        "      l.append(pl)\n",
        "      v.append(pv)\n",
        "      prev_vals = vals\n",
        "  #print(\"Read done, got %d rows, %d filtered, %d open prices adjusted\" % (count_filter+count_out, count_filter, count_fixed))\n",
        "  return Prices(open=np.array(o, dtype=np.float32),high=np.array(h, dtype=np.float32), low=np.array(l, dtype=np.float32),close=np.array(c, dtype=np.float32), volume=np.array(v, dtype=np.float32))\n",
        "\n",
        "# Key: agent learns relative movement, rather than actual price values\n",
        "def prices_to_relative(prices):\n",
        "    \"\"\"\n",
        "    Convert prices to relative in respect to open price\n",
        "    :param ochl: tuple with open, close, high, low\n",
        "    :return: tuple with open, rel_close, rel_high, rel_low\n",
        "    \"\"\"\n",
        "    assert isinstance(prices, Prices)\n",
        "    rh = (prices.high - prices.open) / prices.open\n",
        "    rl = (prices.low - prices.open) / prices.open\n",
        "    rc = (prices.close - prices.open) / prices.open\n",
        "    return Prices(open=prices.open, high=rh, low=rl, close=rc, volume=prices.volume)\n",
        "\n",
        "def load_relative(csv_file):\n",
        "    return prices_to_relative(read_csv(csv_file))\n"
      ],
      "metadata": {
        "id": "p5FOgwwMctR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Action Space"
      ],
      "metadata": {
        "id": "8DFJQHhZzJ4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sets the actions trading agent can take when trading \n",
        "class Actions(enum.Enum):\n",
        "  Nothing = 0\n",
        "  Buy = 1\n",
        "  Close = 2"
      ],
      "metadata": {
        "id": "XHYjnWcAuOLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Trading Environment \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gzAKILhh7lG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of past trading days agent can observe\n",
        "DEFAULT_BARS_COUNT = 2\n",
        "# percentage of stock price trading agent pays broker on buying/selling SPY. By default, it's 0.1%.\n",
        "DEFAULT_COMMISSION_PERC = 0.1\n",
        "\n",
        "class StocksEnv(gym.Env):\n",
        "  # fields required by gym.Env\n",
        "  metadata = {'render.modes': ['human']}\n",
        "  spec = EnvSpec(\"SPYEnv-v0\")\n",
        "\n",
        "  # constructor of the environment\n",
        "  def __init__(self, prices, bars_count=DEFAULT_BARS_COUNT, commission=DEFAULT_COMMISSION_PERC,\n",
        "               reset_on_close=True, state_1d=False, random_ofs_on_reset=True,\n",
        "               reward_on_close=False, volumes=True):\n",
        "    # check to see stock prices is a dict data structure\n",
        "    assert isinstance(prices, dict)\n",
        "    self._prices = prices\n",
        "    \n",
        "    # important: creating the state object for the trading agent\n",
        "    if state_1d:\n",
        "      self._state = State1D(bars_count, commission, reset_on_close, reward_on_close=reward_on_close, volumes=volumes)\n",
        "    else:\n",
        "      self._state = State(bars_count, commission, reset_on_close, reward_on_close=reward_on_close, volumes=volumes)\n",
        "    \n",
        "    # creating discrete action space for trading agent\n",
        "    self.action_space = gym.spaces.Discrete(n=len(Actions))\n",
        "    \n",
        "    # creating observation space for training agent\n",
        "    # i.e. a (possibly unbounded) box in R^n. Specifically, a Box represents the \n",
        "    # Cartesian product of n closed intervals which in this case is (-inf, inf)\n",
        "    self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=self._state.shape, dtype=np.float32)\n",
        "    \n",
        "    # if true, on every reset of the environment, the random offset in the time series will be chosen. \n",
        "    # Otherwise,  it will start from the beginning of the data.\n",
        "    self.random_ofs_on_reset = random_ofs_on_reset\n",
        "    self.seed()\n",
        "\n",
        "  # important: creates the offset for time series data (i.e. not \n",
        "  # always starting at the beggining of the time series data)\n",
        "  def reset(self):\n",
        "    self._instrument = self.np_random.choice(list(self._prices.keys()))\n",
        "    prices = self._prices[self._instrument]\n",
        "    bars = self._state.bars_count\n",
        "    if self.random_ofs_on_reset:\n",
        "      offset = self.np_random.choice(prices.high.shape[0]-bars*10)+bars\n",
        "    else:\n",
        "      offset = bars\n",
        "    self._state.reset(prices, offset)\n",
        "    return self._state.encode()  \n",
        "\n",
        "  # important: executes the sequence of agent taking action, getting reward and\n",
        "  # then getting the next observation/state \n",
        "  def step(self, action_idx):\n",
        "    action = Actions(action_idx)\n",
        "    reward, done = self._state.step(action)\n",
        "    obs = self._state.encode()\n",
        "    info = {\"instrument\":self._instrument, \"offset\": self._state._offset}\n",
        "    return obs, reward, done, info\n",
        "\n",
        "  # methods required by gym.Env; future will implement the render method to view\n",
        "  # the observation space of agent when trading using a trading chart\n",
        "  def render(self, mode='human', close=False):\n",
        "    pass\n",
        "  def close(self):\n",
        "    pass\n",
        "\n",
        "  def seed(self, seed=None):\n",
        "    self.np_random, seed1 = seeding.np_random(seed)\n",
        "    seed2 = seeding.hash_seed(seed1+1) % 2**33\n",
        "    return [seed1, seed2]\n",
        "\n",
        "  # important: creates an instance of the  environment\n",
        "  @classmethod\n",
        "  def from_dir(cls, data_dir, **kwargs):\n",
        "    prices = {f: load_relative(f) for f in price_files(data_dir)}\n",
        "    return StocksEnv(prices, **kwargs)"
      ],
      "metadata": {
        "id": "uDHY94JWfUMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the State Space"
      ],
      "metadata": {
        "id": "Vei7smuIDjlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class State:\n",
        "  def __init__(self, bars_count, commission_perc, reset_on_close, reward_on_close=True, volumes=True):\n",
        "    # checking bars_count is an int\n",
        "    assert isinstance(bars_count, int)\n",
        "    # checking that bars_count is greater than zero\n",
        "    assert bars_count > 0\n",
        "    # checking commission is a float\n",
        "    assert isinstance(commission_perc, float)\n",
        "    # checking commission is greater than zero\n",
        "    assert commission_perc >= 0.0\n",
        "    # checking that reset_on_close and reward on close are bools\n",
        "    assert isinstance(reset_on_close, bool)\n",
        "    assert isinstance(reward_on_close, bool)\n",
        "    self.bars_count=bars_count\n",
        "    self.commission_perc = commission_perc\n",
        "    self.reset_on_close = reset_on_close\n",
        "    self.reward_on_close = reward_on_close\n",
        "    self.volumes = volumes\n",
        "  \n",
        "  # method that reset's the environment \n",
        "  def reset(self, prices, offset):\n",
        "    assert isinstance(prices, Prices)\n",
        "    assert offset >= self.bars_count-1\n",
        "    self.have_position = False\n",
        "    self.open_price = 0.0\n",
        "    self._prices = prices\n",
        "    self._offset = offset\n",
        "\n",
        "  # the shape of the state (i.e. 1D vector)\n",
        "  @property\n",
        "  def shape(self):\n",
        "    # the shape is the high, low, and closing prices of the current trading day\n",
        "    # (i.e. 3 or 4 if volume is used) times the num of bars\n",
        "    # (i.e. past prices agent can observe) plus the position flag \n",
        "    # (i.e. whether agent is holding onto the stock or not) and \n",
        "    # the relative profit agent has recieved since opening\n",
        "    if self.volumes:\n",
        "      return (4*self.bars_count+1+1, )\n",
        "    else:\n",
        "      return (3*self.bars_count+1+1, )\n",
        "  \n",
        "  # important: method that encodes the current state\n",
        "  def encode(self):\n",
        "    res = np.ndarray(shape=self.shape, dtype=np.float32)\n",
        "    shift = 0\n",
        "    for bar_idx in range(-self.bars_count+1, 1):\n",
        "      res[shift] = self._prices.high[self._offset + bar_idx]\n",
        "      shift += 1\n",
        "      res[shift] = self._prices.low[self._offset + bar_idx]\n",
        "      shift += 1\n",
        "      res[shift] = self._prices.close[self._offset + bar_idx]\n",
        "      shift += 1\n",
        "      if self.volumes:\n",
        "        res[shift] = self._prices.volume[self._offset + bar_idx]\n",
        "        shift += 1\n",
        "    res[shift] = float(self.have_position)\n",
        "    shift += 1\n",
        "    if not self.have_position:\n",
        "      res[shift] = 0.0\n",
        "    else:\n",
        "      res[shift] = (self._cur_close() - self.open_price) / self.open_price\n",
        "    return res\n",
        " \n",
        "  def _cur_close(self):\n",
        "    \"\"\"\n",
        "    Calculate real close price for the current bar\n",
        "    \"\"\"\n",
        "    open = self._prices.open[self._offset]\n",
        "    rel_close = self._prices.close[self._offset]\n",
        "    return open * (1.0 + rel_close)\n",
        "\n",
        "  # important: where agent takes the action (i.e. buying or Selling) based on past price/state, \n",
        "  # and returns reward for doing so and updates the price offset\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Perform one step in our price, adjust offset, check for the end of prices\n",
        "    and handle position change\n",
        "    :param action:\n",
        "    :return: reward, done\n",
        "    \"\"\"\n",
        "    assert isinstance(action, Actions)\n",
        "    reward = 0.0\n",
        "    done = False\n",
        "    close = self._cur_close()\n",
        "    if action == Actions.Buy and not self.have_position:\n",
        "      self.have_position = True\n",
        "      self.open_price = close\n",
        "      reward -= self.commission_perc\n",
        "    elif action == Actions.Close and self.have_position:\n",
        "      reward -= self.commission_perc\n",
        "      done |= self.reset_on_close\n",
        "      if self.reward_on_close:\n",
        "        reward += 100.0 * (close - self.open_price) / self.open_price\n",
        "      self.have_position = False\n",
        "      self.open_price = 0.0\n",
        "\n",
        "    self._offset += 1\n",
        "    prev_close = close\n",
        "    close = self._cur_close()\n",
        "    done |= self._offset >= self._prices.close.shape[0]-1\n",
        "    \n",
        "    if self.have_position and not self.reward_on_close:\n",
        "      reward += 100.0 * (close - prev_close) / prev_close\n",
        "      \n",
        "    return reward, done\n",
        "\n",
        "\n",
        "class State1D(State):\n",
        "    \"\"\"\n",
        "    State with shape suitable for 1D convolution\n",
        "    \"\"\"\n",
        "    @property\n",
        "    def shape(self):\n",
        "        if self.volumes:\n",
        "            return (6, self.bars_count)\n",
        "        else:\n",
        "            return (5, self.bars_count)\n",
        "\n",
        "    def encode(self):\n",
        "        res = np.zeros(shape=self.shape, dtype=np.float32)\n",
        "        ofs = self.bars_count-1\n",
        "        res[0] = self._prices.high[self._offset-ofs:self._offset+1]\n",
        "        res[1] = self._prices.low[self._offset-ofs:self._offset+1]\n",
        "        res[2] = self._prices.close[self._offset-ofs:self._offset+1]\n",
        "        if self.volumes:\n",
        "            res[3] = self._prices.volume[self._offset-ofs:self._offset+1]\n",
        "            dst = 4\n",
        "        else:\n",
        "            dst = 3\n",
        "        if self.have_position:\n",
        "            res[dst] = 1.0\n",
        "            res[dst+1] = (self._cur_close() - self.open_price) / self.open_price\n",
        "        return res"
      ],
      "metadata": {
        "id": "_cAFNoOgbJ7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Dueling DQN Model with 1D Convolutions \n"
      ],
      "metadata": {
        "id": "J2MAClP9fIV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNConv1DLarge(nn.Module):\n",
        "  def __init__(self, shape, actions_n):\n",
        "    super(DQNConv1DLarge, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv1d(shape[0], 32, 3),\n",
        "        nn.MaxPool1d(3, 2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(32, 32, 3),\n",
        "        nn.MaxPool1d(3, 2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(32, 32, 3),\n",
        "        nn.MaxPool1d(3, 2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(32, 32, 3),\n",
        "        nn.MaxPool1d(3, 2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(32, 32, 3),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(32, 32, 3),\n",
        "        nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    out_size = self._get_conv_out(shape)\n",
        "\n",
        "    self.fc_val = nn.Sequential(\n",
        "        nn.Linear(out_size, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    self.fc_adv = nn.Sequential(\n",
        "        nn.Linear(out_size, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, actions_n)\n",
        "        )\n",
        "\n",
        "  def _get_conv_out(self, shape):\n",
        "    o = self.conv(torch.zeros(1, *shape))\n",
        "    return int(np.prod(o.size()))\n",
        "\n",
        "  def forward(self, x):\n",
        "    conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "    val = self.fc_val(conv_out)\n",
        "    adv = self.fc_adv(conv_out)\n",
        "    return val + adv - adv.mean(dim=1, keepdim=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "PCJBg4JDmE43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Trading Agent"
      ],
      "metadata": {
        "id": "eN4fLLywnXk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import gym\n",
        "import ptan\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "BARS_COUNT = 2\n",
        "TARGET_NET_SYNC = 1000\n",
        "GAMMA = 0.99\n",
        "REPLAY_SIZE = 100000\n",
        "REPLAY_INITIAL = 10000\n",
        "REWARD_STEPS = 2\n",
        "LEARNING_RATE = 0.0001\n",
        "STATES_TO_EVALUATE = 1000\n",
        "EVAL_EVERY_STEP = 1000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_STOP = 0.1\n",
        "EPSILON_STEPS = 1000000\n",
        "CHECKPOINT_EVERY_STEP = 1000000\n",
        "VALIDATION_EVERY_STEP = 100000\n",
        "#------------------------------------------------------------------------#\n",
        "CUDA = True\n",
        "DEFAULT_STOCKS = \"/content/drive/MyDrive/Datasets/SPY/spy_past.csv\"\n",
        "DEFAULT_VAL_STOCKS = \"/content/drive/MyDrive/Datasets/SPY/spy_future.csv\"\n",
        "YEAR = None\n",
        "SAVE_PATH = \"saves\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
        "    saves_path = os.path.join(\"/content/\", SAVE_PATH)\n",
        "    os.makedirs(saves_path, exist_ok=True)\n",
        "\n",
        "    if YEAR is not None or os.path.isfile(DEFAULT_STOCKS):\n",
        "        if YEAR is not None:\n",
        "            stock_data = data.load_year_data(YEAR)\n",
        "        else:\n",
        "            stock_data = {\"SPY\": load_relative(DEFAULT_STOCKS)}\n",
        "        env = StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=True, volumes=True)\n",
        "        env_tst = StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=True)\n",
        "    elif os.path.isdir(DEFAULT_STOCKS):\n",
        "        # env = StocksEnv.from_dir(DEFAULT_STOCKS, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
        "        # env_tst = StocksEnv.from_dir(DEFAULT_STOCKS, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
        "    else:\n",
        "        raise RuntimeError(\"No data to train on\")\n",
        "    env = gym.wrappers.TimeLimit(env, max_episode_steps=1000)\n",
        "    \n",
        "    val_data = {\"SPY\": load_relative(DEFAULT_VAL_STOCKS)}\n",
        "    env_val = StocksEnv(val_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=True)\n",
        "\n",
        "    writer = SummaryWriter(comment=\"-simple-\" + \"run\")\n",
        "    net = DQNConv1DLarge(env.observation_space.shape[0], env.action_space.n).to(device)\n",
        "    tgt_net = ptan.agent.TargetNet(net)\n",
        "    selector = ptan.actions.EpsilonGreedyActionSelector(EPSILON_START)\n",
        "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
        "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, GAMMA, steps_count=REWARD_STEPS)\n",
        "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # main training loop\n",
        "    step_idx = 0\n",
        "    eval_states = None\n",
        "    best_mean_val = None\n",
        "\n",
        "    with RewardTracker(writer, np.inf, group_rewards=100) as reward_tracker:\n",
        "        while True:\n",
        "            step_idx += 1\n",
        "            buffer.populate(1)\n",
        "            selector.epsilon = max(EPSILON_STOP, EPSILON_START - step_idx / EPSILON_STEPS)\n",
        "\n",
        "            new_rewards = exp_source.pop_rewards_steps()\n",
        "            if new_rewards:\n",
        "                reward_tracker.reward(new_rewards[0], step_idx, selector.epsilon)\n",
        "\n",
        "            if len(buffer) < REPLAY_INITIAL:\n",
        "                continue\n",
        "\n",
        "            if eval_states is None:\n",
        "                print(\"Initial buffer populated, start training\")\n",
        "                eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
        "                eval_states = [np.array(transition.state, copy=False) for transition in eval_states]\n",
        "                eval_states = np.array(eval_states, copy=False)\n",
        "\n",
        "            if step_idx % EVAL_EVERY_STEP == 0:\n",
        "                mean_val = calc_values_of_states(eval_states, net, device=device)\n",
        "                writer.add_scalar(\"values_mean\", mean_val, step_idx)\n",
        "                if best_mean_val is None or best_mean_val < mean_val:\n",
        "                    if best_mean_val is not None:\n",
        "                        print(\"%d: Best mean value updated %.3f -> %.3f\" % (step_idx, best_mean_val, mean_val))\n",
        "                    best_mean_val = mean_val\n",
        "                    torch.save(net.state_dict(), os.path.join(saves_path, \"mean_val-%.3f.data\" % mean_val))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            batch = buffer.sample(BATCH_SIZE)\n",
        "            loss_v = calc_loss(batch, net, tgt_net.target_model, GAMMA ** REWARD_STEPS, device=device)\n",
        "            loss_v.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if step_idx % TARGET_NET_SYNC == 0:\n",
        "                tgt_net.sync()\n",
        "\n",
        "            if step_idx % CHECKPOINT_EVERY_STEP == 0:\n",
        "                idx = step_idx // CHECKPOINT_EVERY_STEP\n",
        "                torch.save(net.state_dict(), os.path.join(saves_path, \"checkpoint-%3d.data\" % idx))\n",
        "\n",
        "            if step_idx % VALIDATION_EVERY_STEP == 0:\n",
        "                # res = validation_run(env_tst, net, device=device)\n",
        "                # for key, val in res.items():\n",
        "                #     writer.add_scalar(key + \"_test\", val, step_idx)\n",
        "                res = validation_run(env_val, net, device=device)\n",
        "                for key, val in res.items():\n",
        "                    writer.add_scalar(key + \"_val\", val, step_idx)"
      ],
      "metadata": {
        "id": "VvCRry1ooHfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/"
      ],
      "metadata": {
        "id": "h7cdLOBVyV8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/runs.zip /content/runs/"
      ],
      "metadata": {
        "id": "1nlZzU4M3o8t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}