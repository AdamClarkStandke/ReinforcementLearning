{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpyTradingAgent",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1EXh-2mp-AwE3OsMXYMgFGNCtenUG_T67",
      "authorship_tag": "ABX9TyN4Re2YEHREJ1VvHC6LcEFB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aCStandke/ReinforcementLearning/blob/main/SpyTradingAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stocks Trading Using Reinforcment Learning\n",
        "\n",
        "\n",
        "This implementation comes straight from chapter 10 from the  book \n",
        "[Deep Reinforcement Learning Hands-On - Second Edition by Maxim Lapan](https://www.amazon.com/Deep-Reinforcement-Learning-Hands-optimization/dp/1838826998/ref=asc_df_1838826998/?tag=hyprod-20&linkCode=df0&hvadid=416741343328&hvpos=&hvnetw=g&hvrand=7234438034400691228&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9008183&hvtargid=pla-871456510229&psc=1&tag=&ref=&adgrpid=93867144477&hvpone=&hvptwo=&hvadid=416741343328&hvpos=&hvnetw=g&hvrand=7234438034400691228&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9008183&hvtargid=pla-871456510229)\n",
        "\n",
        "As stated in chapter 10: \n",
        "\n",
        "> Rather than learning new methods to solve toy reinforcement learning (RL) problems in this chapter, we will try to utilize our deep Q-network (DQN) knowledge to deal with the much more practical problem of financial trading. \n",
        "\n",
        "Namely, a RL agent has some observation of the market, and has to take an action to either buy, sell, or hold. If the agent buys before the price goes up, profit will be positive; otherwise, the agent will get a negative reward. The agent is tyring to obtain as much profit as possible in the trading environment. \n",
        "\n"
      ],
      "metadata": {
        "id": "iTCX_DxmyQLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ptan"
      ],
      "metadata": {
        "id": "i42g9KIlrVJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59504804-84ff-4982-98de-21dc6de5a4cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ptan\n",
            "  Downloading ptan-0.7.tar.gz (20 kB)\n",
            "Collecting torch==1.7.0\n",
            "  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.7 MB 3.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from ptan) (0.17.3)\n",
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.7/dist-packages (from ptan) (0.2.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ptan) (1.21.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from ptan) (4.6.0.66)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->ptan) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->ptan) (4.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py->ptan) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->ptan) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->ptan) (1.7.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->ptan) (1.5.0)\n",
            "Building wheels for collected packages: ptan\n",
            "  Building wheel for ptan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ptan: filename=ptan-0.7-py3-none-any.whl size=23505 sha256=c8de552d0cccfde12b797d6d25b1763c0075d052195555f7bd6ffb5c0d85d61c\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/72/3d/a3c47193fdb9efd08e3a54398af996b2989c68571813a71256\n",
            "Successfully built ptan\n",
            "Installing collected packages: dataclasses, torch, ptan\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed dataclasses-0.6 ptan-0.7 torch-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "ntC5suZYsSvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbf433b-b9e6-438d-8542-e2f2ecbc9d31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "import gym.spaces\n",
        "from gym.utils import seeding\n",
        "from gym.envs.registration import EnvSpec\n",
        "import enum\n",
        "import glob\n",
        "import os\n",
        "import collections\n",
        "import csv\n",
        "import sys\n",
        "import time"
      ],
      "metadata": {
        "id": "I7jj6N5k264p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# method for validation runs\n",
        "def validation_run(env, net, episodes=100, device=\"cpu\", epsilon=0.02, comission=0.1):\n",
        "    stats = {\n",
        "        'episode_reward': [],\n",
        "        'episode_steps': [],\n",
        "        'order_profits': [],\n",
        "        'order_steps': [],\n",
        "    }\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs = env.reset()\n",
        "\n",
        "        total_reward = 0.0\n",
        "        position = None\n",
        "        position_steps = None\n",
        "        episode_steps = 0\n",
        "\n",
        "        while True:\n",
        "            obs_v = torch.tensor([obs]).to(device)\n",
        "            out_v = net(obs_v)\n",
        "\n",
        "            action_idx = out_v.max(dim=1)[1].item()\n",
        "            if np.random.random() < epsilon:\n",
        "                action_idx = env.action_space.sample()\n",
        "            action = Actions(action_idx)\n",
        "\n",
        "            close_price = env._state._cur_close()\n",
        "\n",
        "            if action == Actions.Buy and position is None:\n",
        "                position = close_price\n",
        "                position_steps = 0\n",
        "            elif action == Actions.Close and position is not None:\n",
        "                profit = close_price - position - (close_price + position) * comission / 100\n",
        "                profit = 100.0 * profit / position\n",
        "                stats['order_profits'].append(profit)\n",
        "                stats['order_steps'].append(position_steps)\n",
        "                position = None\n",
        "                position_steps = None\n",
        "\n",
        "            obs, reward, done, _ = env.step(action_idx)\n",
        "            total_reward += reward\n",
        "            episode_steps += 1\n",
        "            if position_steps is not None:\n",
        "                position_steps += 1\n",
        "            if done:\n",
        "                if position is not None:\n",
        "                    profit = close_price - position - (close_price + position) * comission / 100\n",
        "                    profit = 100.0 * profit / position\n",
        "                    stats['order_profits'].append(profit)\n",
        "                    stats['order_steps'].append(position_steps)\n",
        "                break\n",
        "\n",
        "        stats['episode_reward'].append(total_reward)\n",
        "        stats['episode_steps'].append(episode_steps)\n",
        "\n",
        "    return { key: np.mean(vals) for key, vals in stats.items() }"
      ],
      "metadata": {
        "id": "Y0K7eALDp3RD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tracks the rewards of the trading agent\n",
        "class RewardTracker:\n",
        "    def __init__(self, writer, stop_reward, group_rewards=1):\n",
        "        self.writer = writer\n",
        "        self.stop_reward = stop_reward\n",
        "        self.reward_buf = []\n",
        "        self.steps_buf = []\n",
        "        self.group_rewards = group_rewards\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.ts = time.time()\n",
        "        self.ts_frame = 0\n",
        "        self.total_rewards = []\n",
        "        self.total_steps = []\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.writer.close()\n",
        "\n",
        "    def reward(self, reward_steps, frame, epsilon=None):\n",
        "        # gets the reward for num of steps agent took\n",
        "        reward, steps = reward_steps\n",
        "        # stores the reward in buffer\n",
        "        self.reward_buf.append(reward)\n",
        "        self.steps_buf.append(steps)\n",
        "        if len(self.reward_buf) < self.group_rewards:\n",
        "            return False\n",
        "        # calculates mean reward from buffer \n",
        "        reward = np.mean(self.reward_buf)\n",
        "        steps = np.mean(self.steps_buf)\n",
        "        self.reward_buf.clear()\n",
        "        self.steps_buf.clear()\n",
        "        # stores the total reward\n",
        "        self.total_rewards.append(reward)\n",
        "        self.total_steps.append(steps)\n",
        "        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n",
        "        self.ts_frame = frame\n",
        "        self.ts = time.time()\n",
        "        mean_reward = np.mean(self.total_rewards[-100:]) # \n",
        "        mean_steps = np.mean(self.total_steps[-100:])\n",
        "        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n",
        "        # print(\"%d: done %d games, mean reward %.3f, mean steps %.2f, speed %.2f f/s%s\" % (\n",
        "        #     frame, len(self.total_rewards)*self.group_rewards, mean_reward, mean_steps, speed, epsilon_str\n",
        "        # ))\n",
        "        print(\"mean reward %.3f, mean steps %.2f\"  % (mean_reward, mean_steps))\n",
        "        sys.stdout.flush()\n",
        "        if epsilon is not None:\n",
        "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
        "        self.writer.add_scalar(\"speed\", speed, frame)\n",
        "        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n",
        "        self.writer.add_scalar(\"reward\", reward, frame)\n",
        "        self.writer.add_scalar(\"steps_100\", mean_steps, frame)\n",
        "        self.writer.add_scalar(\"steps\", steps, frame)\n",
        "        if mean_reward > self.stop_reward:\n",
        "            print(\"Solved in %d frames!\" % frame)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "def calc_values_of_states(states, net, device=\"cpu\"):\n",
        "    mean_vals = []\n",
        "    for batch in np.array_split(states, 64):\n",
        "        states_v = torch.tensor(batch).to(device)\n",
        "        action_values_v = net(states_v)\n",
        "        best_action_values_v = action_values_v.max(1)[0]\n",
        "        mean_vals.append(best_action_values_v.mean().item())\n",
        "    return np.mean(mean_vals)\n",
        "\n",
        "\n",
        "def unpack_batch(batch):\n",
        "    states, actions, rewards, dones, last_states = [], [], [], [], []\n",
        "    for exp in batch:\n",
        "        state = np.array(exp.state, copy=False)\n",
        "        states.append(state)\n",
        "        actions.append(exp.action)\n",
        "        rewards.append(exp.reward)\n",
        "        dones.append(exp.last_state is None)\n",
        "        if exp.last_state is None:\n",
        "            last_states.append(state)       # the result will be masked anyway\n",
        "        else:\n",
        "            last_states.append(np.array(exp.last_state, copy=False))\n",
        "    return np.array(states, copy=False), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "           np.array(dones, dtype=np.uint8), np.array(last_states, copy=False)\n",
        "\n",
        "\n",
        "def calc_loss(batch, net, tgt_net, gamma, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, next_states = unpack_batch(batch)\n",
        "\n",
        "    states_v = torch.tensor(states).to(device)\n",
        "    next_states_v = torch.tensor(next_states).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.ByteTensor(dones).to(device)\n",
        "\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_actions = net(next_states_v).max(1)[1]\n",
        "    next_state_values = tgt_net(next_states_v).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_values[done_mask] = 0.0\n",
        "\n",
        "    expected_state_action_values = next_state_values.detach() * gamma + rewards_v\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n"
      ],
      "metadata": {
        "id": "4AkcT1EdL2sL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Price Data for Trading Environment\n",
        "\n",
        "The chapter uses Russian stock market prices from the period ranging from 2015-2016 for the technology company [Yandex](https://en.wikipedia.org/wiki/Yandex) for its reinforcment trading agent. It contained over 130,000 rows, where every row represented a single minute in time,and price movement during that minute was captured by five variables: open, high, low,close, and volume. \n",
        "\n",
        "Rather than use one stock, I decided to use a basket of stocks found in the [SPY ETF](https://www.etf.com/SPY#:~:text=SPY%20is%20the%20best%2Drecognized,US%20index%2C%20the%20S%26P%20500.). This would give a longer term trading horizon, rather than the trading horizon  provided by the Yandex data. The period ranged from 2005 to 2022. Each row represented a single trading day and price movement during the trading day was captured by five variables: open, high, low, close, and volume. "
      ],
      "metadata": {
        "id": "X5_4V4z3oX9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Prices = collections.namedtuple('Prices', field_names=['open', 'high', 'low', 'close', 'volume'])\n",
        "\n",
        "def read_csv(file_name, sep=',', filter_data=True, fix_open_prices=False):\n",
        "  print(\"Reading\", file_name)\n",
        "  with open(file_name, 'r') as fd:\n",
        "    reader = csv.reader(fd)\n",
        "    h = next(reader)\n",
        "    indices = [h.index(s) for s in ('Open', 'High', 'Low', 'Close', 'Volume')]\n",
        "    o, h, l, c, v = [], [], [], [], []\n",
        "    count_out = 0\n",
        "    count_filter = 0 \n",
        "    count_fixed = 0\n",
        "    prev_vals = None\n",
        "    for row in reader:\n",
        "      vals = list(map(float, [row[idx] for idx in indices])) \n",
        "      if filter_data and all(map(lambda v: abs(v-vals[0]) < 1e-8, vals[:-1])):\n",
        "        count_filter += 1\n",
        "        continue\n",
        "      \n",
        "      po, ph, pl, pc, pv = vals\n",
        "\n",
        "      count_out +=1\n",
        "      o.append(po)\n",
        "      c.append(pc)\n",
        "      h.append(ph)\n",
        "      l.append(pl)\n",
        "      v.append(pv)\n",
        "      prev_vals = vals\n",
        "  #print(\"Read done, got %d rows, %d filtered, %d open prices adjusted\" % (count_filter+count_out, count_filter, count_fixed))\n",
        "  return Prices(open=np.array(o, dtype=np.float32),high=np.array(h, dtype=np.float32), low=np.array(l, dtype=np.float32),close=np.array(c, dtype=np.float32), volume=np.array(v, dtype=np.float32))\n",
        "\n",
        "# Key: agent learns relative movement, rather than actual price values\n",
        "def prices_to_relative(prices):\n",
        "    \"\"\"\n",
        "    Convert prices to relative in respect to open price\n",
        "    :param ochl: tuple with open, close, high, low\n",
        "    :return: tuple with open, rel_close, rel_high, rel_low\n",
        "    \"\"\"\n",
        "    assert isinstance(prices, Prices)\n",
        "    rh = (prices.high - prices.open) / prices.open\n",
        "    rl = (prices.low - prices.open) / prices.open\n",
        "    rc = (prices.close - prices.open) / prices.open\n",
        "    return Prices(open=prices.open, high=rh, low=rl, close=rc, volume=prices.volume)\n",
        "\n",
        "def load_relative(csv_file):\n",
        "    return prices_to_relative(read_csv(csv_file))\n"
      ],
      "metadata": {
        "id": "p5FOgwwMctR3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Action Space"
      ],
      "metadata": {
        "id": "8DFJQHhZzJ4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sets the actions trading agent can take when trading \n",
        "class Actions(enum.Enum):\n",
        "  Nothing = 0\n",
        "  Buy = 1\n",
        "  Close = 2"
      ],
      "metadata": {
        "id": "XHYjnWcAuOLF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Trading Environment \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gzAKILhh7lG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Key!!! number of past trading days agent can observe, which for 1D \n",
        "# convolution on a 2D matrix is the column portion\n",
        "DEFAULT_BARS_COUNT = 2\n",
        "# percentage of stock price trading agent pays broker on buying/selling SPY. By default, it's 0.1%.\n",
        "DEFAULT_COMMISSION_PERC = 0.1\n",
        "\n",
        "class StocksEnv(gym.Env):\n",
        "  # fields required by gym.Env\n",
        "  metadata = {'render.modes': ['human']}\n",
        "  spec = EnvSpec(\"SPYEnv-v0\")\n",
        "\n",
        "  # constructor of the environment\n",
        "  def __init__(self, prices, bars_count=DEFAULT_BARS_COUNT, commission=DEFAULT_COMMISSION_PERC,\n",
        "               reset_on_close=True, state_1d=False, random_ofs_on_reset=True,\n",
        "               reward_on_close=False, volumes=True):\n",
        "    # check to see stock prices is a dict data structure\n",
        "    assert isinstance(prices, dict)\n",
        "    self._prices = prices\n",
        "    \n",
        "    # important: creating the state object for the trading agent\n",
        "    if state_1d:\n",
        "      self._state = State1D(bars_count, commission, reset_on_close, reward_on_close=reward_on_close, volumes=volumes)\n",
        "    else:\n",
        "      self._state = State(bars_count, commission, reset_on_close, reward_on_close=reward_on_close, volumes=volumes)\n",
        "    \n",
        "    # creating discrete action space for trading agent\n",
        "    self.action_space = gym.spaces.Discrete(n=len(Actions))\n",
        "    \n",
        "    # creating observation space for training agent\n",
        "    # i.e. a (possibly unbounded) box in R^n. Specifically, a Box represents the \n",
        "    # Cartesian product of n closed intervals which in this case is (-inf, inf)\n",
        "    self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=self._state.shape, dtype=np.float32)\n",
        "    \n",
        "    # if true, on every reset of the environment, the random offset in the time series will be chosen. \n",
        "    # Otherwise,  it will start from the beginning of the data.\n",
        "    self.random_ofs_on_reset = random_ofs_on_reset\n",
        "    self.seed()\n",
        "\n",
        "  # important: creates the offset for time series data (i.e. not \n",
        "  # always starting at the beggining of the time series data)\n",
        "  def reset(self):\n",
        "    self._instrument = self.np_random.choice(list(self._prices.keys()))\n",
        "    prices = self._prices[self._instrument]\n",
        "    bars = self._state.bars_count\n",
        "    if self.random_ofs_on_reset:\n",
        "      offset = self.np_random.choice(prices.high.shape[0]-bars*10)+bars\n",
        "    else:\n",
        "      offset = bars\n",
        "    self._state.reset(prices, offset)\n",
        "    return self._state.encode()  \n",
        "\n",
        "  # important: executes the sequence of agent taking action, getting reward and\n",
        "  # then getting the next observation/state \n",
        "  def step(self, action_idx):\n",
        "    action = Actions(action_idx)\n",
        "    reward, done = self._state.step(action)\n",
        "    obs = self._state.encode()\n",
        "    info = {\"instrument\":self._instrument, \"offset\": self._state._offset}\n",
        "    return obs, reward, done, info\n",
        "\n",
        "  # methods required by gym.Env; future will implement the render method to view\n",
        "  # the observation space of agent when trading using a trading chart\n",
        "  def render(self, mode='human', close=False):\n",
        "    pass\n",
        "  def close(self):\n",
        "    pass\n",
        "\n",
        "  def seed(self, seed=None):\n",
        "    self.np_random, seed1 = seeding.np_random(seed)\n",
        "    seed2 = seeding.hash_seed(seed1+1) % 2**33\n",
        "    return [seed1, seed2]\n",
        "\n",
        "  # important: creates an instance of the  environment\n",
        "  @classmethod\n",
        "  def from_dir(cls, data_dir, **kwargs):\n",
        "    prices = {f: load_relative(f) for f in price_files(data_dir)}\n",
        "    return StocksEnv(prices, **kwargs)"
      ],
      "metadata": {
        "id": "uDHY94JWfUMn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the State Space"
      ],
      "metadata": {
        "id": "Vei7smuIDjlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class State:\n",
        "  def __init__(self, bars_count, commission_perc, reset_on_close, reward_on_close=True, volumes=True):\n",
        "    # checking bars_count is an int\n",
        "    assert isinstance(bars_count, int)\n",
        "    # checking that bars_count is greater than zero\n",
        "    assert bars_count > 0\n",
        "    # checking commission is a float\n",
        "    assert isinstance(commission_perc, float)\n",
        "    # checking commission is greater than zero\n",
        "    assert commission_perc >= 0.0\n",
        "    # checking that reset_on_close and reward on close are bools\n",
        "    assert isinstance(reset_on_close, bool)\n",
        "    assert isinstance(reward_on_close, bool)\n",
        "    self.bars_count=bars_count\n",
        "    self.commission_perc = commission_perc\n",
        "    self.reset_on_close = reset_on_close\n",
        "    self.reward_on_close = reward_on_close\n",
        "    self.volumes = volumes\n",
        "  \n",
        "  # method that reset's the environment \n",
        "  def reset(self, prices, offset):\n",
        "    assert isinstance(prices, Prices)\n",
        "    assert offset >= self.bars_count-1\n",
        "    self.have_position = False\n",
        "    self.open_price = 0.0\n",
        "    self._prices = prices\n",
        "    self._offset = offset\n",
        "\n",
        "  # the shape of the state (i.e. 1D vector)\n",
        "  @property\n",
        "  def shape(self):\n",
        "    # the shape is the high, low, and closing prices of the current trading day\n",
        "    # (i.e. 3 or 4 if volume is used) times the num of bars\n",
        "    # (i.e. past prices agent can observe) plus the position flag \n",
        "    # (i.e. whether agent is holding onto the stock or not) and \n",
        "    # the relative profit agent has recieved since opening\n",
        "    if self.volumes:\n",
        "      return (4*self.bars_count+1+1, )\n",
        "    else:\n",
        "      return (3*self.bars_count+1+1, )\n",
        "  \n",
        "  # important: method that encodes the current state\n",
        "  def encode(self):\n",
        "    res = np.ndarray(shape=self.shape, dtype=np.float32)\n",
        "    shift = 0\n",
        "    for bar_idx in range(-self.bars_count+1, 1):\n",
        "      res[shift] = self._prices.high[self._offset + bar_idx]\n",
        "      shift += 1\n",
        "      res[shift] = self._prices.low[self._offset + bar_idx]\n",
        "      shift += 1\n",
        "      res[shift] = self._prices.close[self._offset + bar_idx]\n",
        "      shift += 1\n",
        "      if self.volumes:\n",
        "        res[shift] = self._prices.volume[self._offset + bar_idx]\n",
        "        shift += 1\n",
        "    res[shift] = float(self.have_position)\n",
        "    shift += 1\n",
        "    if not self.have_position:\n",
        "      res[shift] = 0.0\n",
        "    else:\n",
        "      res[shift] = (self._cur_close() - self.open_price) / self.open_price\n",
        "    return res\n",
        " \n",
        "  def _cur_close(self):\n",
        "    \"\"\"\n",
        "    Calculate real close price for the current bar\n",
        "    \"\"\"\n",
        "    open = self._prices.open[self._offset]\n",
        "    rel_close = self._prices.close[self._offset]\n",
        "    return open * (1.0 + rel_close)\n",
        "\n",
        "  # important: where agent takes the action (i.e. buying or Selling) based on past price/state, \n",
        "  # and returns reward for doing so and updates the price offset\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Perform one step in our price, adjust offset, check for the end of prices\n",
        "    and handle position change\n",
        "    :param action:\n",
        "    :return: reward, done\n",
        "    \"\"\"\n",
        "    assert isinstance(action, Actions)\n",
        "    reward = 0.0\n",
        "    done = False\n",
        "    close = self._cur_close()\n",
        "    if action == Actions.Buy and not self.have_position:\n",
        "      self.have_position = True\n",
        "      self.open_price = close\n",
        "      reward -= self.commission_perc\n",
        "    elif action == Actions.Close and self.have_position:\n",
        "      reward -= self.commission_perc\n",
        "      done |= self.reset_on_close\n",
        "      if self.reward_on_close:\n",
        "        reward += 100.0 * (close - self.open_price) / self.open_price\n",
        "      self.have_position = False\n",
        "      self.open_price = 0.0\n",
        "\n",
        "    self._offset += 1\n",
        "    prev_close = close\n",
        "    close = self._cur_close()\n",
        "    done |= self._offset >= self._prices.close.shape[0]-1\n",
        "    \n",
        "    if self.have_position and not self.reward_on_close:\n",
        "      reward += 100.0 * (close - prev_close) / prev_close\n",
        "      \n",
        "    return reward, done\n",
        "\n",
        "\n",
        "class State1D(State):\n",
        "    \"\"\"\n",
        "    State with shape suitable for 1D convolution\n",
        "    \"\"\"\n",
        "    @property\n",
        "    def shape(self):\n",
        "        if self.volumes:\n",
        "            return (6, self.bars_count)\n",
        "        else:\n",
        "            return (5, self.bars_count)\n",
        "\n",
        "    def encode(self):\n",
        "        res = np.zeros(shape=self.shape, dtype=np.float32)\n",
        "        ofs = self.bars_count-1\n",
        "        res[0] = self._prices.high[self._offset-ofs:self._offset+1]\n",
        "        res[1] = self._prices.low[self._offset-ofs:self._offset+1]\n",
        "        res[2] = self._prices.close[self._offset-ofs:self._offset+1]\n",
        "        if self.volumes:\n",
        "            res[3] = self._prices.volume[self._offset-ofs:self._offset+1]\n",
        "            dst = 4\n",
        "        else:\n",
        "            dst = 3\n",
        "        if self.have_position:\n",
        "            res[dst] = 1.0\n",
        "            res[dst+1] = (self._cur_close() - self.open_price) / self.open_price\n",
        "        return res"
      ],
      "metadata": {
        "id": "_cAFNoOgbJ7P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Dueling DQN Model with 1D Convolutions \n"
      ],
      "metadata": {
        "id": "J2MAClP9fIV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNConv1D(nn.Module):\n",
        "    def __init__(self, shape, actions_n):\n",
        "        super(DQNConv1D, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(shape[0], 128, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 128, 1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        out_size = self._get_conv_out(shape)\n",
        "\n",
        "        self.fc_val = nn.Sequential(\n",
        "            nn.Linear(out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "        self.fc_adv = nn.Sequential(\n",
        "            nn.Linear(out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, actions_n)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        val = self.fc_val(conv_out)\n",
        "        adv = self.fc_adv(conv_out)\n",
        "        return val + adv - adv.mean(dim=1, keepdim=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "PCJBg4JDmE43"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=True, volumes=True)\n",
        "env.observation_space.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FQgB1QuTfO8",
        "outputId": "0162388b-6ffd-44df-c4e3-fcdef2a92573"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Trading Agent"
      ],
      "metadata": {
        "id": "eN4fLLywnXk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import gym\n",
        "import ptan\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "BARS_COUNT = 2\n",
        "TARGET_NET_SYNC = 1000\n",
        "GAMMA = 0.99\n",
        "REPLAY_SIZE = 100000\n",
        "REPLAY_INITIAL = 10000\n",
        "REWARD_STEPS = 2\n",
        "LEARNING_RATE = 0.0001\n",
        "STATES_TO_EVALUATE = 1000\n",
        "EVAL_EVERY_STEP = 1000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_STOP = 0.1\n",
        "EPSILON_STEPS = 1000000\n",
        "CHECKPOINT_EVERY_STEP = 1000000\n",
        "VALIDATION_EVERY_STEP = 100000\n",
        "#------------------------------------------------------------------------#\n",
        "CUDA = True\n",
        "DEFAULT_STOCKS = \"/content/drive/MyDrive/Datasets/SPY/spy_past.csv\"\n",
        "DEFAULT_VAL_STOCKS = \"/content/drive/MyDrive/Datasets/SPY/spy_future.csv\"\n",
        "YEAR = None\n",
        "SAVE_PATH = \"saves\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
        "    saves_path = os.path.join(\"/content/\", SAVE_PATH)\n",
        "    os.makedirs(saves_path, exist_ok=True)\n",
        "\n",
        "    if YEAR is not None or os.path.isfile(DEFAULT_STOCKS):\n",
        "        if YEAR is not None:\n",
        "            stock_data = data.load_year_data(YEAR)\n",
        "        else:\n",
        "            stock_data = {\"SPY\": load_relative(DEFAULT_STOCKS)}\n",
        "        env = StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=True, volumes=True)\n",
        "        env_tst = StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=True)\n",
        "    elif os.path.isdir(DEFAULT_STOCKS):\n",
        "        env = StocksEnv.from_dir(DEFAULT_STOCKS, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
        "        env_tst = StocksEnv.from_dir(DEFAULT_STOCKS, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
        "    else:\n",
        "        raise RuntimeError(\"No data to train on\")\n",
        "    env = gym.wrappers.TimeLimit(env, max_episode_steps=1000)\n",
        "    \n",
        "    val_data = {\"SPY\": load_relative(DEFAULT_VAL_STOCKS)}\n",
        "    env_val = StocksEnv(val_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=True)\n",
        "\n",
        "    writer = SummaryWriter(comment=\"-simple-\" + \"run\")\n",
        "    net = DQNConv1D(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    tgt_net = ptan.agent.TargetNet(net)\n",
        "    selector = ptan.actions.EpsilonGreedyActionSelector(EPSILON_START)\n",
        "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
        "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, GAMMA, steps_count=REWARD_STEPS)\n",
        "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # main training loop\n",
        "    step_idx = 0\n",
        "    eval_states = None\n",
        "    best_mean_val = None\n",
        "\n",
        "    with RewardTracker(writer, np.inf, group_rewards=100) as reward_tracker:\n",
        "        while True:\n",
        "            step_idx += 1\n",
        "            buffer.populate(1)\n",
        "            selector.epsilon = max(EPSILON_STOP, EPSILON_START - step_idx / EPSILON_STEPS)\n",
        "\n",
        "            new_rewards = exp_source.pop_rewards_steps()\n",
        "            if new_rewards:\n",
        "                reward_tracker.reward(new_rewards[0], step_idx, selector.epsilon)\n",
        "\n",
        "            if len(buffer) < REPLAY_INITIAL:\n",
        "                continue\n",
        "\n",
        "            if eval_states is None:\n",
        "                print(\"Initial buffer populated, start training\")\n",
        "                eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
        "                eval_states = [np.array(transition.state, copy=False) for transition in eval_states]\n",
        "                eval_states = np.array(eval_states, copy=False)\n",
        "\n",
        "            if step_idx % EVAL_EVERY_STEP == 0:\n",
        "                mean_val = calc_values_of_states(eval_states, net, device=device)\n",
        "                writer.add_scalar(\"values_mean\", mean_val, step_idx)\n",
        "                if best_mean_val is None or best_mean_val < mean_val:\n",
        "                    if best_mean_val is not None:\n",
        "                        print(\"%d: Best mean value updated %.3f -> %.3f\" % (step_idx, best_mean_val, mean_val))\n",
        "                    best_mean_val = mean_val\n",
        "                    torch.save(net.state_dict(), os.path.join(saves_path, \"mean_val-%.3f.data\" % mean_val))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            batch = buffer.sample(BATCH_SIZE)\n",
        "            loss_v = calc_loss(batch, net, tgt_net.target_model, GAMMA ** REWARD_STEPS, device=device)\n",
        "            loss_v.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if step_idx % TARGET_NET_SYNC == 0:\n",
        "                tgt_net.sync()\n",
        "\n",
        "            if step_idx % CHECKPOINT_EVERY_STEP == 0:\n",
        "                idx = step_idx // CHECKPOINT_EVERY_STEP\n",
        "                torch.save(net.state_dict(), os.path.join(saves_path, \"checkpoint-%3d.data\" % idx))\n",
        "\n",
        "            if step_idx % VALIDATION_EVERY_STEP == 0:\n",
        "                # res = validation_run(env_tst, net, device=device)\n",
        "                # for key, val in res.items():\n",
        "                #     writer.add_scalar(key + \"_test\", val, step_idx)\n",
        "                res = validation_run(env_val, net, device=device)\n",
        "                for key, val in res.items():\n",
        "                    writer.add_scalar(key + \"_val\", val, step_idx)"
      ],
      "metadata": {
        "id": "VvCRry1ooHfz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "c51530a7-ae4f-4187-e7b1-62f212f45a10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7f8653fe0c47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mptan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ptan'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/"
      ],
      "metadata": {
        "id": "h7cdLOBVyV8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/runs.zip /content/runs/"
      ],
      "metadata": {
        "id": "1nlZzU4M3o8t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}